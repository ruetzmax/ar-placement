{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e3ef86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append('Depth-Anything-V2')\n",
    "from depth_anything_v2.dpt import DepthAnythingV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5406cd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "video_input_path = \"vid.mp4\"\n",
    "video_output_path = \"out.mp4\"\n",
    "checkpoint_path = \"depth_anything_v2_vits.pth\"\n",
    "resize_shortest = 518  # recommended by Depth Anything V2\n",
    "\n",
    "# --- Load model ---\n",
    "model = DepthAnythingV2(**{'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]})\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Open video ---\n",
    "cap = cv2.VideoCapture(video_input_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(video_output_path, fourcc, fps, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21d64735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1101 12:59:59.710000 65130 torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 11 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `DepthAnythingV2([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `DepthAnythingV2([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 11).\n",
      "Failed to convert the model to the target version 11 using the ONNX C API. The model was not modified\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/onnxscript/version_converter/__init__.py\", line 127, in call\n",
      "    converted_proto = _c_api_utils.call_onnx_api(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n",
      "    result = func(proto)\n",
      "             ^^^^^^^^^^^\n",
      "  File \"/home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/onnxscript/version_converter/__init__.py\", line 122, in _partial_convert_version\n",
      "    return onnx.version_converter.convert_version(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/onnx/version_converter.py\", line 39, in convert_version\n",
      "    converted_model_str = C.convert_version(model_str, target_version)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: /github/workspace/onnx/version_converter/BaseConverter.h:65: adapter_lookup: Assertion `false` failed: No Adapter To Version $17 for Resize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 5 of general pattern rewrite rules.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 18},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.9.0+cu128',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"x\"<FLOAT,[1,3,518,518]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"squeeze\"<FLOAT,[1,518,518]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"pretrained.patch_embed.proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.0.norm1.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.0.norm1.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.0.attn.proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.0.ls1.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.0.norm2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.0.norm2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.0.mlp.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.0.ls2.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.1.norm1.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.1.norm1.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.1.attn.proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.1.ls1.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.1.norm2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.1.norm2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.1.mlp.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.1.ls2.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.2.norm1.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.2.norm1.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.2.attn.proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.2.ls1.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.2.norm2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.2.norm2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.2.mlp.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.2.ls2.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.3.norm1.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.3.norm1.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.3.attn.proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.3.ls1.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.3.norm2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.3.norm2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.3.mlp.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.3.ls2.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.4.norm1.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.4.norm1.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.4.attn.proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.4.ls1.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.4.norm2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.4.norm2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.4.mlp.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.4.ls2.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.5.norm1.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.5.norm1.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.5.attn.proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.5.ls1.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.5.norm2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.5.norm2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.5.mlp.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.5.ls2.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.6.norm1.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.6.norm1.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.6.attn.proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.6.ls1.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.6.norm2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.6.norm2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.6.mlp.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.6.ls2.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.7.norm1.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.7.norm1.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.7.attn.proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.7.ls1.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.7.norm2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.7.norm2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.7.mlp.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.7.ls2.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.8.norm1.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.8.norm1.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.8.attn.proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.8.ls1.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.8.norm2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.8.norm2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.8.mlp.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.8.ls2.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.9.norm1.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.9.norm1.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.9.attn.proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.9.ls1.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.9.norm2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.9.norm2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.9.mlp.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.9.ls2.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.10.norm1.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.10.norm1.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.10.attn.proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.10.ls1.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.10.norm2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.10.norm2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.10.mlp.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.10.ls2.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.11.norm1.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.11.norm1.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.11.attn.proj.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.11.ls1.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.11.norm2.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.11.norm2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.11.mlp.fc2.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.11.ls2.gamma\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.norm.weight\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"pretrained.norm.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"depth_head.projects.0.bias\"<FLOAT,[48]>{TorchTensor(...)},\n",
       "                %\"depth_head.projects.1.bias\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"depth_head.projects.2.bias\"<FLOAT,[192]>{TorchTensor(...)},\n",
       "                %\"depth_head.projects.3.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"depth_head.resize_layers.0.bias\"<FLOAT,[48]>{TorchTensor(...)},\n",
       "                %\"depth_head.resize_layers.1.bias\"<FLOAT,[96]>{TorchTensor(...)},\n",
       "                %\"depth_head.resize_layers.3.bias\"<FLOAT,[384]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet1.out_conv.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet1.resConfUnit1.conv1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet1.resConfUnit1.conv2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet1.resConfUnit2.conv1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet1.resConfUnit2.conv2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet2.out_conv.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet2.resConfUnit1.conv1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet2.resConfUnit1.conv2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet2.resConfUnit2.conv1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet2.resConfUnit2.conv2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet3.out_conv.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet3.resConfUnit1.conv1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet3.resConfUnit1.conv2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet3.resConfUnit2.conv1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet3.resConfUnit2.conv2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet4.out_conv.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet4.resConfUnit2.conv1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet4.resConfUnit2.conv2.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.output_conv1.bias\"<FLOAT,[32]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.output_conv2.0.bias\"<FLOAT,[32]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.output_conv2.2.weight\"<FLOAT,[1,32,1,1]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.output_conv2.2.bias\"<FLOAT,[1]>{TorchTensor<FLOAT,[1]>(Parameter containing: tensor([0.0887], requires_grad=True), name='depth_head.scratch.output_conv2.2.bias')},\n",
       "                %\"pretrained.pos_embed\"<FLOAT,[1,1370,384]>{TorchTensor(...)},\n",
       "                %\"pretrained.patch_embed.proj.weight\"<FLOAT,[384,3,14,14]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.0.attn.qkv.bias\"<FLOAT,[1152]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.0.mlp.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.1.attn.qkv.bias\"<FLOAT,[1152]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.1.mlp.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.2.attn.qkv.bias\"<FLOAT,[1152]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.2.mlp.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.3.attn.qkv.bias\"<FLOAT,[1152]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.3.mlp.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.4.attn.qkv.bias\"<FLOAT,[1152]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.4.mlp.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.5.attn.qkv.bias\"<FLOAT,[1152]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.5.mlp.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.6.attn.qkv.bias\"<FLOAT,[1152]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.6.mlp.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.7.attn.qkv.bias\"<FLOAT,[1152]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.7.mlp.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.8.attn.qkv.bias\"<FLOAT,[1152]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.8.mlp.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.9.attn.qkv.bias\"<FLOAT,[1152]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.9.mlp.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.10.attn.qkv.bias\"<FLOAT,[1152]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.10.mlp.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.11.attn.qkv.bias\"<FLOAT,[1152]>{TorchTensor(...)},\n",
       "                %\"pretrained.blocks.11.mlp.fc1.bias\"<FLOAT,[1536]>{TorchTensor(...)},\n",
       "                %\"depth_head.projects.0.weight\"<FLOAT,[48,384,1,1]>{TorchTensor(...)},\n",
       "                %\"depth_head.projects.1.weight\"<FLOAT,[96,384,1,1]>{TorchTensor(...)},\n",
       "                %\"depth_head.projects.2.weight\"<FLOAT,[192,384,1,1]>{TorchTensor(...)},\n",
       "                %\"depth_head.projects.3.weight\"<FLOAT,[384,384,1,1]>{TorchTensor(...)},\n",
       "                %\"depth_head.resize_layers.0.weight\"<FLOAT,[48,48,4,4]>{TorchTensor(...)},\n",
       "                %\"depth_head.resize_layers.1.weight\"<FLOAT,[96,96,2,2]>{TorchTensor(...)},\n",
       "                %\"depth_head.resize_layers.3.weight\"<FLOAT,[384,384,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.layer1_rn.weight\"<FLOAT,[64,48,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.layer2_rn.weight\"<FLOAT,[64,96,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.layer3_rn.weight\"<FLOAT,[64,192,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.layer4_rn.weight\"<FLOAT,[64,384,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet1.out_conv.weight\"<FLOAT,[64,64,1,1]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet1.resConfUnit1.conv1.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet1.resConfUnit1.conv2.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet1.resConfUnit2.conv1.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet1.resConfUnit2.conv2.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet2.out_conv.weight\"<FLOAT,[64,64,1,1]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet2.resConfUnit1.conv1.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet2.resConfUnit1.conv2.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet2.resConfUnit2.conv1.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet2.resConfUnit2.conv2.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet3.out_conv.weight\"<FLOAT,[64,64,1,1]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet3.resConfUnit1.conv1.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet3.resConfUnit1.conv2.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet3.resConfUnit2.conv1.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet3.resConfUnit2.conv2.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet4.out_conv.weight\"<FLOAT,[64,64,1,1]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet4.resConfUnit2.conv1.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.refinenet4.resConfUnit2.conv2.weight\"<FLOAT,[64,64,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.output_conv1.weight\"<FLOAT,[32,64,3,3]>{TorchTensor(...)},\n",
       "                %\"depth_head.scratch.output_conv2.0.weight\"<FLOAT,[32,32,3,3]>{TorchTensor(...)},\n",
       "                %\"val_4\"<INT64,[3]>{Tensor<INT64,[3]>(array([   1,  384, 1369]), name='val_4')},\n",
       "                %\"expand\"<FLOAT,[1,1,384]>{Tensor(...)},\n",
       "                %\"val_12\"<FLOAT,[384,1152]>{Tensor(...)},\n",
       "                %\"val_20\"<INT64,[5]>{Tensor<INT64,[5]>(array([   1, 1370,    3,    6,   64]), name='val_20')},\n",
       "                %\"val_29\"<INT64,[3]>{Tensor<INT64,[3]>(array([   1, 1370,  384]), name='val_29')},\n",
       "                %\"val_30\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_34\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_43\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_47\"<FLOAT,[384,1152]>{Tensor(...)},\n",
       "                %\"val_61\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_65\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_74\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_78\"<FLOAT,[384,1152]>{Tensor(...)},\n",
       "                %\"val_92\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_96\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_105\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_109\"<FLOAT,[384,1152]>{Tensor(...)},\n",
       "                %\"val_123\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_127\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_136\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_140\"<FLOAT,[384,1152]>{Tensor(...)},\n",
       "                %\"val_154\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_158\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_167\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_171\"<FLOAT,[384,1152]>{Tensor(...)},\n",
       "                %\"val_185\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_189\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_198\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_202\"<FLOAT,[384,1152]>{Tensor(...)},\n",
       "                %\"val_216\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_220\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_229\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_233\"<FLOAT,[384,1152]>{Tensor(...)},\n",
       "                %\"val_247\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_251\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_260\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_264\"<FLOAT,[384,1152]>{Tensor(...)},\n",
       "                %\"val_278\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_282\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_291\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_295\"<FLOAT,[384,1152]>{Tensor(...)},\n",
       "                %\"val_309\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_313\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_322\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_326\"<FLOAT,[384,1152]>{Tensor(...)},\n",
       "                %\"val_340\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_344\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_353\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_357\"<FLOAT,[384,1152]>{Tensor(...)},\n",
       "                %\"val_371\"<FLOAT,[384,384]>{Tensor(...)},\n",
       "                %\"val_375\"<FLOAT,[384,1536]>{Tensor(...)},\n",
       "                %\"val_384\"<FLOAT,[1536,384]>{Tensor(...)},\n",
       "                %\"val_396\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_396')},\n",
       "                %\"val_400\"<INT64,[1]>{Tensor<INT64,[1]>(array([9223372036854775807]), name='val_400')},\n",
       "                %\"val_440\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1, 384,  37,  37]), name='val_440')},\n",
       "                %\"val_483\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 1, 64, 37, 37]), name='val_483')},\n",
       "                %\"val_487\"<INT64,[4]>{Tensor<INT64,[4]>(array([ 1, 64, 74, 74]), name='val_487')},\n",
       "                %\"val_491\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1,  64, 148, 148]), name='val_491')},\n",
       "                %\"val_496\"<INT64,[4]>{Tensor<INT64,[4]>(array([  1,  32, 518, 518]), name='val_496')},\n",
       "                %\"val_21\"<INT64,[]>{Tensor<INT64,[]>(array(0), name='val_21')},\n",
       "                %\"val_22\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.125, dtype=float32), name='val_22')},\n",
       "                %\"val_23\"<INT64,[]>{Tensor<INT64,[]>(array(1), name='val_23')},\n",
       "                %\"val_24\"<INT64,[]>{Tensor<INT64,[]>(array(2), name='val_24')},\n",
       "                %\"val_36\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1.4142135, dtype=float32), name='val_36')},\n",
       "                %\"val_39\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(1., dtype=float32), name='val_39')},\n",
       "                %\"val_41\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.5, dtype=float32), name='val_41')},\n",
       "                %\"val_492\"<FLOAT,[4]>{Tensor<FLOAT,[4]>(array([1., 1., 2., 2.], dtype=float32), name='val_492')}\n",
       "            ),\n",
       "        ) {\n",
       "              0 |  # node_conv2d\n",
       "                   %\"conv2d\"<FLOAT,[1,384,37,37]> ⬅️ ::Conv(%\"x\", %\"pretrained.patch_embed.proj.weight\"{...}, %\"pretrained.patch_embed.proj.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(14, 14), dilations=(1, 1)}\n",
       "              1 |  # node_view\n",
       "                   %\"view\"<FLOAT,[1,384,1369]> ⬅️ ::Reshape(%\"conv2d\", %\"val_4\"{[1, 384, 1369]}) {allowzero=1}\n",
       "              2 |  # node_transpose\n",
       "                   %\"transpose\"<FLOAT,[1,1369,384]> ⬅️ ::Transpose(%\"view\") {perm=(0, 2, 1)}\n",
       "              3 |  # node_cat\n",
       "                   %\"cat\"<FLOAT,[1,1370,384]> ⬅️ ::Concat(%\"expand\"{...}, %\"transpose\") {axis=1}\n",
       "              4 |  # node_add\n",
       "                   %\"add\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"cat\", %\"pretrained.pos_embed\"{...})\n",
       "              5 |  # node_layer_norm\n",
       "                   %\"layer_norm\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add\", %\"pretrained.blocks.0.norm1.weight\"{...}, %\"pretrained.blocks.0.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "              6 |  # node_MatMul_11\n",
       "                   %\"val_13\"<FLOAT,[1,1370,1152]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_12\"{...})\n",
       "              7 |  # node_linear\n",
       "                   %\"linear\"<FLOAT,[1,1370,1152]> ⬅️ ::Add(%\"val_13\", %\"pretrained.blocks.0.attn.qkv.bias\"{...})\n",
       "              8 |  # node_view_1\n",
       "                   %\"view_1\"<FLOAT,[1,1370,3,6,64]> ⬅️ ::Reshape(%\"linear\", %\"val_20\"{[1, 1370, 3, 6, 64]}) {allowzero=1}\n",
       "              9 |  # node_permute\n",
       "                   %\"permute\"<FLOAT,[3,1,6,1370,64]> ⬅️ ::Transpose(%\"view_1\") {perm=(2, 0, 3, 1, 4)}\n",
       "             10 |  # node_select\n",
       "                   %\"select\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute\", %\"val_21\"{0}) {axis=0}\n",
       "             11 |  # node_mul\n",
       "                   %\"mul\"<FLOAT,[1,6,1370,64]> ⬅️ ::Mul(%\"select\", %\"val_22\"{0.125})\n",
       "             12 |  # node_select_1\n",
       "                   %\"select_1\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute\", %\"val_23\"{1}) {axis=0}\n",
       "             13 |  # node_select_2\n",
       "                   %\"select_2\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute\", %\"val_24\"{2}) {axis=0}\n",
       "             14 |  # node_transpose_1\n",
       "                   %\"transpose_1\"<FLOAT,[1,6,64,1370]> ⬅️ ::Transpose(%\"select_1\") {perm=(0, 1, 3, 2)}\n",
       "             15 |  # node_matmul\n",
       "                   %\"matmul\"<FLOAT,[1,6,1370,1370]> ⬅️ ::MatMul(%\"mul\", %\"transpose_1\")\n",
       "             16 |  # node_softmax\n",
       "                   %\"softmax\"<FLOAT,[1,6,1370,1370]> ⬅️ ::Softmax(%\"matmul\") {axis=-1}\n",
       "             17 |  # node_matmul_1\n",
       "                   %\"matmul_1\"<FLOAT,[1,6,1370,64]> ⬅️ ::MatMul(%\"softmax\", %\"select_2\")\n",
       "             18 |  # node_transpose_2\n",
       "                   %\"transpose_2\"<FLOAT,[1,1370,6,64]> ⬅️ ::Transpose(%\"matmul_1\") {perm=(0, 2, 1, 3)}\n",
       "             19 |  # node__unsafe_view\n",
       "                   %\"_unsafe_view\"<FLOAT,[1,1370,384]> ⬅️ ::Reshape(%\"transpose_2\", %\"val_29\"{[1, 1370, 384]}) {allowzero=1}\n",
       "             20 |  # node_MatMul_29\n",
       "                   %\"val_31\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"_unsafe_view\", %\"val_30\"{...})\n",
       "             21 |  # node_linear_1\n",
       "                   %\"linear_1\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_31\", %\"pretrained.blocks.0.attn.proj.bias\"{...})\n",
       "             22 |  # node_mul_1\n",
       "                   %\"mul_1\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_1\", %\"pretrained.blocks.0.ls1.gamma\"{...})\n",
       "             23 |  # node_add_1\n",
       "                   %\"add_1\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add\", %\"mul_1\")\n",
       "             24 |  # node_layer_norm_1\n",
       "                   %\"layer_norm_1\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1\", %\"pretrained.blocks.0.norm2.weight\"{...}, %\"pretrained.blocks.0.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "             25 |  # node_MatMul_31\n",
       "                   %\"val_35\"<FLOAT,[1,1370,1536]> ⬅️ ::MatMul(%\"layer_norm_1\", %\"val_34\"{...})\n",
       "             26 |  # node_linear_2\n",
       "                   %\"linear_2\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_35\", %\"pretrained.blocks.0.mlp.fc1.bias\"{...})\n",
       "             27 |  # node_Div_33\n",
       "                   %\"val_37\"<FLOAT,[1,1370,1536]> ⬅️ ::Div(%\"linear_2\", %\"val_36\"{1.4142135381698608})\n",
       "             28 |  # node_Erf_34\n",
       "                   %\"val_38\"<FLOAT,[1,1370,1536]> ⬅️ ::Erf(%\"val_37\")\n",
       "             29 |  # node_Add_36\n",
       "                   %\"val_40\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_38\", %\"val_39\"{1.0})\n",
       "             30 |  # node_Mul_38\n",
       "                   %\"val_42\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"val_41\"{0.5}, %\"val_40\")\n",
       "             31 |  # node_gelu\n",
       "                   %\"gelu\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"linear_2\", %\"val_42\")\n",
       "             32 |  # node_MatMul_40\n",
       "                   %\"val_44\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"gelu\", %\"val_43\"{...})\n",
       "             33 |  # node_linear_3\n",
       "                   %\"linear_3\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_44\", %\"pretrained.blocks.0.mlp.fc2.bias\"{...})\n",
       "             34 |  # node_mul_2\n",
       "                   %\"mul_2\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_3\", %\"pretrained.blocks.0.ls2.gamma\"{...})\n",
       "             35 |  # node_add_2\n",
       "                   %\"add_2\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_1\", %\"mul_2\")\n",
       "             36 |  # node_layer_norm_2\n",
       "                   %\"layer_norm_2\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_2\", %\"pretrained.blocks.1.norm1.weight\"{...}, %\"pretrained.blocks.1.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "             37 |  # node_MatMul_42\n",
       "                   %\"val_48\"<FLOAT,[1,1370,1152]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_47\"{...})\n",
       "             38 |  # node_linear_4\n",
       "                   %\"linear_4\"<FLOAT,[1,1370,1152]> ⬅️ ::Add(%\"val_48\", %\"pretrained.blocks.1.attn.qkv.bias\"{...})\n",
       "             39 |  # node_view_2\n",
       "                   %\"view_2\"<FLOAT,[1,1370,3,6,64]> ⬅️ ::Reshape(%\"linear_4\", %\"val_20\"{[1, 1370, 3, 6, 64]}) {allowzero=1}\n",
       "             40 |  # node_permute_1\n",
       "                   %\"permute_1\"<FLOAT,[3,1,6,1370,64]> ⬅️ ::Transpose(%\"view_2\") {perm=(2, 0, 3, 1, 4)}\n",
       "             41 |  # node_select_3\n",
       "                   %\"select_3\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_1\", %\"val_21\"{0}) {axis=0}\n",
       "             42 |  # node_mul_3\n",
       "                   %\"mul_3\"<FLOAT,[1,6,1370,64]> ⬅️ ::Mul(%\"select_3\", %\"val_22\"{0.125})\n",
       "             43 |  # node_select_4\n",
       "                   %\"select_4\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_1\", %\"val_23\"{1}) {axis=0}\n",
       "             44 |  # node_select_5\n",
       "                   %\"select_5\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_1\", %\"val_24\"{2}) {axis=0}\n",
       "             45 |  # node_transpose_3\n",
       "                   %\"transpose_3\"<FLOAT,[1,6,64,1370]> ⬅️ ::Transpose(%\"select_4\") {perm=(0, 1, 3, 2)}\n",
       "             46 |  # node_matmul_2\n",
       "                   %\"matmul_2\"<FLOAT,[1,6,1370,1370]> ⬅️ ::MatMul(%\"mul_3\", %\"transpose_3\")\n",
       "             47 |  # node_softmax_1\n",
       "                   %\"softmax_1\"<FLOAT,[1,6,1370,1370]> ⬅️ ::Softmax(%\"matmul_2\") {axis=-1}\n",
       "             48 |  # node_matmul_3\n",
       "                   %\"matmul_3\"<FLOAT,[1,6,1370,64]> ⬅️ ::MatMul(%\"softmax_1\", %\"select_5\")\n",
       "             49 |  # node_transpose_4\n",
       "                   %\"transpose_4\"<FLOAT,[1,1370,6,64]> ⬅️ ::Transpose(%\"matmul_3\") {perm=(0, 2, 1, 3)}\n",
       "             50 |  # node__unsafe_view_1\n",
       "                   %\"_unsafe_view_1\"<FLOAT,[1,1370,384]> ⬅️ ::Reshape(%\"transpose_4\", %\"val_29\"{[1, 1370, 384]}) {allowzero=1}\n",
       "             51 |  # node_MatMul_56\n",
       "                   %\"val_62\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"_unsafe_view_1\", %\"val_61\"{...})\n",
       "             52 |  # node_linear_5\n",
       "                   %\"linear_5\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_62\", %\"pretrained.blocks.1.attn.proj.bias\"{...})\n",
       "             53 |  # node_mul_4\n",
       "                   %\"mul_4\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_5\", %\"pretrained.blocks.1.ls1.gamma\"{...})\n",
       "             54 |  # node_add_3\n",
       "                   %\"add_3\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_2\", %\"mul_4\")\n",
       "             55 |  # node_layer_norm_3\n",
       "                   %\"layer_norm_3\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_3\", %\"pretrained.blocks.1.norm2.weight\"{...}, %\"pretrained.blocks.1.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "             56 |  # node_MatMul_58\n",
       "                   %\"val_66\"<FLOAT,[1,1370,1536]> ⬅️ ::MatMul(%\"layer_norm_3\", %\"val_65\"{...})\n",
       "             57 |  # node_linear_6\n",
       "                   %\"linear_6\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_66\", %\"pretrained.blocks.1.mlp.fc1.bias\"{...})\n",
       "             58 |  # node_Div_60\n",
       "                   %\"val_68\"<FLOAT,[1,1370,1536]> ⬅️ ::Div(%\"linear_6\", %\"val_36\"{1.4142135381698608})\n",
       "             59 |  # node_Erf_61\n",
       "                   %\"val_69\"<FLOAT,[1,1370,1536]> ⬅️ ::Erf(%\"val_68\")\n",
       "             60 |  # node_Add_63\n",
       "                   %\"val_71\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_69\", %\"val_39\"{1.0})\n",
       "             61 |  # node_Mul_65\n",
       "                   %\"val_73\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"val_41\"{0.5}, %\"val_71\")\n",
       "             62 |  # node_gelu_1\n",
       "                   %\"gelu_1\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"linear_6\", %\"val_73\")\n",
       "             63 |  # node_MatMul_67\n",
       "                   %\"val_75\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"gelu_1\", %\"val_74\"{...})\n",
       "             64 |  # node_linear_7\n",
       "                   %\"linear_7\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_75\", %\"pretrained.blocks.1.mlp.fc2.bias\"{...})\n",
       "             65 |  # node_mul_5\n",
       "                   %\"mul_5\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_7\", %\"pretrained.blocks.1.ls2.gamma\"{...})\n",
       "             66 |  # node_add_4\n",
       "                   %\"add_4\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_3\", %\"mul_5\")\n",
       "             67 |  # node_layer_norm_4\n",
       "                   %\"layer_norm_4\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_4\", %\"pretrained.blocks.2.norm1.weight\"{...}, %\"pretrained.blocks.2.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "             68 |  # node_MatMul_69\n",
       "                   %\"val_79\"<FLOAT,[1,1370,1152]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_78\"{...})\n",
       "             69 |  # node_linear_8\n",
       "                   %\"linear_8\"<FLOAT,[1,1370,1152]> ⬅️ ::Add(%\"val_79\", %\"pretrained.blocks.2.attn.qkv.bias\"{...})\n",
       "             70 |  # node_view_3\n",
       "                   %\"view_3\"<FLOAT,[1,1370,3,6,64]> ⬅️ ::Reshape(%\"linear_8\", %\"val_20\"{[1, 1370, 3, 6, 64]}) {allowzero=1}\n",
       "             71 |  # node_permute_2\n",
       "                   %\"permute_2\"<FLOAT,[3,1,6,1370,64]> ⬅️ ::Transpose(%\"view_3\") {perm=(2, 0, 3, 1, 4)}\n",
       "             72 |  # node_select_6\n",
       "                   %\"select_6\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_2\", %\"val_21\"{0}) {axis=0}\n",
       "             73 |  # node_mul_6\n",
       "                   %\"mul_6\"<FLOAT,[1,6,1370,64]> ⬅️ ::Mul(%\"select_6\", %\"val_22\"{0.125})\n",
       "             74 |  # node_select_7\n",
       "                   %\"select_7\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_2\", %\"val_23\"{1}) {axis=0}\n",
       "             75 |  # node_select_8\n",
       "                   %\"select_8\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_2\", %\"val_24\"{2}) {axis=0}\n",
       "             76 |  # node_transpose_5\n",
       "                   %\"transpose_5\"<FLOAT,[1,6,64,1370]> ⬅️ ::Transpose(%\"select_7\") {perm=(0, 1, 3, 2)}\n",
       "             77 |  # node_matmul_4\n",
       "                   %\"matmul_4\"<FLOAT,[1,6,1370,1370]> ⬅️ ::MatMul(%\"mul_6\", %\"transpose_5\")\n",
       "             78 |  # node_softmax_2\n",
       "                   %\"softmax_2\"<FLOAT,[1,6,1370,1370]> ⬅️ ::Softmax(%\"matmul_4\") {axis=-1}\n",
       "             79 |  # node_matmul_5\n",
       "                   %\"matmul_5\"<FLOAT,[1,6,1370,64]> ⬅️ ::MatMul(%\"softmax_2\", %\"select_8\")\n",
       "             80 |  # node_transpose_6\n",
       "                   %\"transpose_6\"<FLOAT,[1,1370,6,64]> ⬅️ ::Transpose(%\"matmul_5\") {perm=(0, 2, 1, 3)}\n",
       "             81 |  # node__unsafe_view_2\n",
       "                   %\"_unsafe_view_2\"<FLOAT,[1,1370,384]> ⬅️ ::Reshape(%\"transpose_6\", %\"val_29\"{[1, 1370, 384]}) {allowzero=1}\n",
       "             82 |  # node_MatMul_83\n",
       "                   %\"val_93\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"_unsafe_view_2\", %\"val_92\"{...})\n",
       "             83 |  # node_linear_9\n",
       "                   %\"linear_9\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_93\", %\"pretrained.blocks.2.attn.proj.bias\"{...})\n",
       "             84 |  # node_mul_7\n",
       "                   %\"mul_7\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_9\", %\"pretrained.blocks.2.ls1.gamma\"{...})\n",
       "             85 |  # node_add_5\n",
       "                   %\"add_5\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_4\", %\"mul_7\")\n",
       "             86 |  # node_layer_norm_5\n",
       "                   %\"layer_norm_5\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_5\", %\"pretrained.blocks.2.norm2.weight\"{...}, %\"pretrained.blocks.2.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "             87 |  # node_MatMul_85\n",
       "                   %\"val_97\"<FLOAT,[1,1370,1536]> ⬅️ ::MatMul(%\"layer_norm_5\", %\"val_96\"{...})\n",
       "             88 |  # node_linear_10\n",
       "                   %\"linear_10\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_97\", %\"pretrained.blocks.2.mlp.fc1.bias\"{...})\n",
       "             89 |  # node_Div_87\n",
       "                   %\"val_99\"<FLOAT,[1,1370,1536]> ⬅️ ::Div(%\"linear_10\", %\"val_36\"{1.4142135381698608})\n",
       "             90 |  # node_Erf_88\n",
       "                   %\"val_100\"<FLOAT,[1,1370,1536]> ⬅️ ::Erf(%\"val_99\")\n",
       "             91 |  # node_Add_90\n",
       "                   %\"val_102\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_100\", %\"val_39\"{1.0})\n",
       "             92 |  # node_Mul_92\n",
       "                   %\"val_104\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"val_41\"{0.5}, %\"val_102\")\n",
       "             93 |  # node_gelu_2\n",
       "                   %\"gelu_2\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"linear_10\", %\"val_104\")\n",
       "             94 |  # node_MatMul_94\n",
       "                   %\"val_106\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"gelu_2\", %\"val_105\"{...})\n",
       "             95 |  # node_linear_11\n",
       "                   %\"linear_11\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_106\", %\"pretrained.blocks.2.mlp.fc2.bias\"{...})\n",
       "             96 |  # node_mul_8\n",
       "                   %\"mul_8\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_11\", %\"pretrained.blocks.2.ls2.gamma\"{...})\n",
       "             97 |  # node_add_6\n",
       "                   %\"add_6\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_5\", %\"mul_8\")\n",
       "             98 |  # node_layer_norm_6\n",
       "                   %\"layer_norm_6\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_6\", %\"pretrained.blocks.3.norm1.weight\"{...}, %\"pretrained.blocks.3.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "             99 |  # node_MatMul_96\n",
       "                   %\"val_110\"<FLOAT,[1,1370,1152]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_109\"{...})\n",
       "            100 |  # node_linear_12\n",
       "                   %\"linear_12\"<FLOAT,[1,1370,1152]> ⬅️ ::Add(%\"val_110\", %\"pretrained.blocks.3.attn.qkv.bias\"{...})\n",
       "            101 |  # node_view_4\n",
       "                   %\"view_4\"<FLOAT,[1,1370,3,6,64]> ⬅️ ::Reshape(%\"linear_12\", %\"val_20\"{[1, 1370, 3, 6, 64]}) {allowzero=1}\n",
       "            102 |  # node_permute_3\n",
       "                   %\"permute_3\"<FLOAT,[3,1,6,1370,64]> ⬅️ ::Transpose(%\"view_4\") {perm=(2, 0, 3, 1, 4)}\n",
       "            103 |  # node_select_9\n",
       "                   %\"select_9\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_3\", %\"val_21\"{0}) {axis=0}\n",
       "            104 |  # node_mul_9\n",
       "                   %\"mul_9\"<FLOAT,[1,6,1370,64]> ⬅️ ::Mul(%\"select_9\", %\"val_22\"{0.125})\n",
       "            105 |  # node_select_10\n",
       "                   %\"select_10\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_3\", %\"val_23\"{1}) {axis=0}\n",
       "            106 |  # node_select_11\n",
       "                   %\"select_11\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_3\", %\"val_24\"{2}) {axis=0}\n",
       "            107 |  # node_transpose_7\n",
       "                   %\"transpose_7\"<FLOAT,[1,6,64,1370]> ⬅️ ::Transpose(%\"select_10\") {perm=(0, 1, 3, 2)}\n",
       "            108 |  # node_matmul_6\n",
       "                   %\"matmul_6\"<FLOAT,[1,6,1370,1370]> ⬅️ ::MatMul(%\"mul_9\", %\"transpose_7\")\n",
       "            109 |  # node_softmax_3\n",
       "                   %\"softmax_3\"<FLOAT,[1,6,1370,1370]> ⬅️ ::Softmax(%\"matmul_6\") {axis=-1}\n",
       "            110 |  # node_matmul_7\n",
       "                   %\"matmul_7\"<FLOAT,[1,6,1370,64]> ⬅️ ::MatMul(%\"softmax_3\", %\"select_11\")\n",
       "            111 |  # node_transpose_8\n",
       "                   %\"transpose_8\"<FLOAT,[1,1370,6,64]> ⬅️ ::Transpose(%\"matmul_7\") {perm=(0, 2, 1, 3)}\n",
       "            112 |  # node__unsafe_view_3\n",
       "                   %\"_unsafe_view_3\"<FLOAT,[1,1370,384]> ⬅️ ::Reshape(%\"transpose_8\", %\"val_29\"{[1, 1370, 384]}) {allowzero=1}\n",
       "            113 |  # node_MatMul_110\n",
       "                   %\"val_124\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"_unsafe_view_3\", %\"val_123\"{...})\n",
       "            114 |  # node_linear_13\n",
       "                   %\"linear_13\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_124\", %\"pretrained.blocks.3.attn.proj.bias\"{...})\n",
       "            115 |  # node_mul_10\n",
       "                   %\"mul_10\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_13\", %\"pretrained.blocks.3.ls1.gamma\"{...})\n",
       "            116 |  # node_add_7\n",
       "                   %\"add_7\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_6\", %\"mul_10\")\n",
       "            117 |  # node_layer_norm_7\n",
       "                   %\"layer_norm_7\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_7\", %\"pretrained.blocks.3.norm2.weight\"{...}, %\"pretrained.blocks.3.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            118 |  # node_MatMul_112\n",
       "                   %\"val_128\"<FLOAT,[1,1370,1536]> ⬅️ ::MatMul(%\"layer_norm_7\", %\"val_127\"{...})\n",
       "            119 |  # node_linear_14\n",
       "                   %\"linear_14\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_128\", %\"pretrained.blocks.3.mlp.fc1.bias\"{...})\n",
       "            120 |  # node_Div_114\n",
       "                   %\"val_130\"<FLOAT,[1,1370,1536]> ⬅️ ::Div(%\"linear_14\", %\"val_36\"{1.4142135381698608})\n",
       "            121 |  # node_Erf_115\n",
       "                   %\"val_131\"<FLOAT,[1,1370,1536]> ⬅️ ::Erf(%\"val_130\")\n",
       "            122 |  # node_Add_117\n",
       "                   %\"val_133\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_131\", %\"val_39\"{1.0})\n",
       "            123 |  # node_Mul_119\n",
       "                   %\"val_135\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"val_41\"{0.5}, %\"val_133\")\n",
       "            124 |  # node_gelu_3\n",
       "                   %\"gelu_3\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"linear_14\", %\"val_135\")\n",
       "            125 |  # node_MatMul_121\n",
       "                   %\"val_137\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"gelu_3\", %\"val_136\"{...})\n",
       "            126 |  # node_linear_15\n",
       "                   %\"linear_15\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_137\", %\"pretrained.blocks.3.mlp.fc2.bias\"{...})\n",
       "            127 |  # node_mul_11\n",
       "                   %\"mul_11\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_15\", %\"pretrained.blocks.3.ls2.gamma\"{...})\n",
       "            128 |  # node_add_8\n",
       "                   %\"add_8\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_7\", %\"mul_11\")\n",
       "            129 |  # node_layer_norm_8\n",
       "                   %\"layer_norm_8\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_8\", %\"pretrained.blocks.4.norm1.weight\"{...}, %\"pretrained.blocks.4.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            130 |  # node_MatMul_123\n",
       "                   %\"val_141\"<FLOAT,[1,1370,1152]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_140\"{...})\n",
       "            131 |  # node_linear_16\n",
       "                   %\"linear_16\"<FLOAT,[1,1370,1152]> ⬅️ ::Add(%\"val_141\", %\"pretrained.blocks.4.attn.qkv.bias\"{...})\n",
       "            132 |  # node_view_5\n",
       "                   %\"view_5\"<FLOAT,[1,1370,3,6,64]> ⬅️ ::Reshape(%\"linear_16\", %\"val_20\"{[1, 1370, 3, 6, 64]}) {allowzero=1}\n",
       "            133 |  # node_permute_4\n",
       "                   %\"permute_4\"<FLOAT,[3,1,6,1370,64]> ⬅️ ::Transpose(%\"view_5\") {perm=(2, 0, 3, 1, 4)}\n",
       "            134 |  # node_select_12\n",
       "                   %\"select_12\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_4\", %\"val_21\"{0}) {axis=0}\n",
       "            135 |  # node_mul_12\n",
       "                   %\"mul_12\"<FLOAT,[1,6,1370,64]> ⬅️ ::Mul(%\"select_12\", %\"val_22\"{0.125})\n",
       "            136 |  # node_select_13\n",
       "                   %\"select_13\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_4\", %\"val_23\"{1}) {axis=0}\n",
       "            137 |  # node_select_14\n",
       "                   %\"select_14\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_4\", %\"val_24\"{2}) {axis=0}\n",
       "            138 |  # node_transpose_9\n",
       "                   %\"transpose_9\"<FLOAT,[1,6,64,1370]> ⬅️ ::Transpose(%\"select_13\") {perm=(0, 1, 3, 2)}\n",
       "            139 |  # node_matmul_8\n",
       "                   %\"matmul_8\"<FLOAT,[1,6,1370,1370]> ⬅️ ::MatMul(%\"mul_12\", %\"transpose_9\")\n",
       "            140 |  # node_softmax_4\n",
       "                   %\"softmax_4\"<FLOAT,[1,6,1370,1370]> ⬅️ ::Softmax(%\"matmul_8\") {axis=-1}\n",
       "            141 |  # node_matmul_9\n",
       "                   %\"matmul_9\"<FLOAT,[1,6,1370,64]> ⬅️ ::MatMul(%\"softmax_4\", %\"select_14\")\n",
       "            142 |  # node_transpose_10\n",
       "                   %\"transpose_10\"<FLOAT,[1,1370,6,64]> ⬅️ ::Transpose(%\"matmul_9\") {perm=(0, 2, 1, 3)}\n",
       "            143 |  # node__unsafe_view_4\n",
       "                   %\"_unsafe_view_4\"<FLOAT,[1,1370,384]> ⬅️ ::Reshape(%\"transpose_10\", %\"val_29\"{[1, 1370, 384]}) {allowzero=1}\n",
       "            144 |  # node_MatMul_137\n",
       "                   %\"val_155\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"_unsafe_view_4\", %\"val_154\"{...})\n",
       "            145 |  # node_linear_17\n",
       "                   %\"linear_17\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_155\", %\"pretrained.blocks.4.attn.proj.bias\"{...})\n",
       "            146 |  # node_mul_13\n",
       "                   %\"mul_13\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_17\", %\"pretrained.blocks.4.ls1.gamma\"{...})\n",
       "            147 |  # node_add_9\n",
       "                   %\"add_9\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_8\", %\"mul_13\")\n",
       "            148 |  # node_layer_norm_9\n",
       "                   %\"layer_norm_9\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_9\", %\"pretrained.blocks.4.norm2.weight\"{...}, %\"pretrained.blocks.4.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            149 |  # node_MatMul_139\n",
       "                   %\"val_159\"<FLOAT,[1,1370,1536]> ⬅️ ::MatMul(%\"layer_norm_9\", %\"val_158\"{...})\n",
       "            150 |  # node_linear_18\n",
       "                   %\"linear_18\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_159\", %\"pretrained.blocks.4.mlp.fc1.bias\"{...})\n",
       "            151 |  # node_Div_141\n",
       "                   %\"val_161\"<FLOAT,[1,1370,1536]> ⬅️ ::Div(%\"linear_18\", %\"val_36\"{1.4142135381698608})\n",
       "            152 |  # node_Erf_142\n",
       "                   %\"val_162\"<FLOAT,[1,1370,1536]> ⬅️ ::Erf(%\"val_161\")\n",
       "            153 |  # node_Add_144\n",
       "                   %\"val_164\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_162\", %\"val_39\"{1.0})\n",
       "            154 |  # node_Mul_146\n",
       "                   %\"val_166\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"val_41\"{0.5}, %\"val_164\")\n",
       "            155 |  # node_gelu_4\n",
       "                   %\"gelu_4\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"linear_18\", %\"val_166\")\n",
       "            156 |  # node_MatMul_148\n",
       "                   %\"val_168\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"gelu_4\", %\"val_167\"{...})\n",
       "            157 |  # node_linear_19\n",
       "                   %\"linear_19\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_168\", %\"pretrained.blocks.4.mlp.fc2.bias\"{...})\n",
       "            158 |  # node_mul_14\n",
       "                   %\"mul_14\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_19\", %\"pretrained.blocks.4.ls2.gamma\"{...})\n",
       "            159 |  # node_add_10\n",
       "                   %\"add_10\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_9\", %\"mul_14\")\n",
       "            160 |  # node_layer_norm_10\n",
       "                   %\"layer_norm_10\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_10\", %\"pretrained.blocks.5.norm1.weight\"{...}, %\"pretrained.blocks.5.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            161 |  # node_MatMul_150\n",
       "                   %\"val_172\"<FLOAT,[1,1370,1152]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_171\"{...})\n",
       "            162 |  # node_linear_20\n",
       "                   %\"linear_20\"<FLOAT,[1,1370,1152]> ⬅️ ::Add(%\"val_172\", %\"pretrained.blocks.5.attn.qkv.bias\"{...})\n",
       "            163 |  # node_view_6\n",
       "                   %\"view_6\"<FLOAT,[1,1370,3,6,64]> ⬅️ ::Reshape(%\"linear_20\", %\"val_20\"{[1, 1370, 3, 6, 64]}) {allowzero=1}\n",
       "            164 |  # node_permute_5\n",
       "                   %\"permute_5\"<FLOAT,[3,1,6,1370,64]> ⬅️ ::Transpose(%\"view_6\") {perm=(2, 0, 3, 1, 4)}\n",
       "            165 |  # node_select_15\n",
       "                   %\"select_15\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_5\", %\"val_21\"{0}) {axis=0}\n",
       "            166 |  # node_mul_15\n",
       "                   %\"mul_15\"<FLOAT,[1,6,1370,64]> ⬅️ ::Mul(%\"select_15\", %\"val_22\"{0.125})\n",
       "            167 |  # node_select_16\n",
       "                   %\"select_16\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_5\", %\"val_23\"{1}) {axis=0}\n",
       "            168 |  # node_select_17\n",
       "                   %\"select_17\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_5\", %\"val_24\"{2}) {axis=0}\n",
       "            169 |  # node_transpose_11\n",
       "                   %\"transpose_11\"<FLOAT,[1,6,64,1370]> ⬅️ ::Transpose(%\"select_16\") {perm=(0, 1, 3, 2)}\n",
       "            170 |  # node_matmul_10\n",
       "                   %\"matmul_10\"<FLOAT,[1,6,1370,1370]> ⬅️ ::MatMul(%\"mul_15\", %\"transpose_11\")\n",
       "            171 |  # node_softmax_5\n",
       "                   %\"softmax_5\"<FLOAT,[1,6,1370,1370]> ⬅️ ::Softmax(%\"matmul_10\") {axis=-1}\n",
       "            172 |  # node_matmul_11\n",
       "                   %\"matmul_11\"<FLOAT,[1,6,1370,64]> ⬅️ ::MatMul(%\"softmax_5\", %\"select_17\")\n",
       "            173 |  # node_transpose_12\n",
       "                   %\"transpose_12\"<FLOAT,[1,1370,6,64]> ⬅️ ::Transpose(%\"matmul_11\") {perm=(0, 2, 1, 3)}\n",
       "            174 |  # node__unsafe_view_5\n",
       "                   %\"_unsafe_view_5\"<FLOAT,[1,1370,384]> ⬅️ ::Reshape(%\"transpose_12\", %\"val_29\"{[1, 1370, 384]}) {allowzero=1}\n",
       "            175 |  # node_MatMul_164\n",
       "                   %\"val_186\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"_unsafe_view_5\", %\"val_185\"{...})\n",
       "            176 |  # node_linear_21\n",
       "                   %\"linear_21\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_186\", %\"pretrained.blocks.5.attn.proj.bias\"{...})\n",
       "            177 |  # node_mul_16\n",
       "                   %\"mul_16\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_21\", %\"pretrained.blocks.5.ls1.gamma\"{...})\n",
       "            178 |  # node_add_11\n",
       "                   %\"add_11\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_10\", %\"mul_16\")\n",
       "            179 |  # node_layer_norm_11\n",
       "                   %\"layer_norm_11\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_11\", %\"pretrained.blocks.5.norm2.weight\"{...}, %\"pretrained.blocks.5.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            180 |  # node_MatMul_166\n",
       "                   %\"val_190\"<FLOAT,[1,1370,1536]> ⬅️ ::MatMul(%\"layer_norm_11\", %\"val_189\"{...})\n",
       "            181 |  # node_linear_22\n",
       "                   %\"linear_22\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_190\", %\"pretrained.blocks.5.mlp.fc1.bias\"{...})\n",
       "            182 |  # node_Div_168\n",
       "                   %\"val_192\"<FLOAT,[1,1370,1536]> ⬅️ ::Div(%\"linear_22\", %\"val_36\"{1.4142135381698608})\n",
       "            183 |  # node_Erf_169\n",
       "                   %\"val_193\"<FLOAT,[1,1370,1536]> ⬅️ ::Erf(%\"val_192\")\n",
       "            184 |  # node_Add_171\n",
       "                   %\"val_195\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_193\", %\"val_39\"{1.0})\n",
       "            185 |  # node_Mul_173\n",
       "                   %\"val_197\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"val_41\"{0.5}, %\"val_195\")\n",
       "            186 |  # node_gelu_5\n",
       "                   %\"gelu_5\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"linear_22\", %\"val_197\")\n",
       "            187 |  # node_MatMul_175\n",
       "                   %\"val_199\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"gelu_5\", %\"val_198\"{...})\n",
       "            188 |  # node_linear_23\n",
       "                   %\"linear_23\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_199\", %\"pretrained.blocks.5.mlp.fc2.bias\"{...})\n",
       "            189 |  # node_mul_17\n",
       "                   %\"mul_17\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_23\", %\"pretrained.blocks.5.ls2.gamma\"{...})\n",
       "            190 |  # node_add_12\n",
       "                   %\"add_12\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_11\", %\"mul_17\")\n",
       "            191 |  # node_layer_norm_12\n",
       "                   %\"layer_norm_12\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_12\", %\"pretrained.blocks.6.norm1.weight\"{...}, %\"pretrained.blocks.6.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            192 |  # node_MatMul_177\n",
       "                   %\"val_203\"<FLOAT,[1,1370,1152]> ⬅️ ::MatMul(%\"layer_norm_12\", %\"val_202\"{...})\n",
       "            193 |  # node_linear_24\n",
       "                   %\"linear_24\"<FLOAT,[1,1370,1152]> ⬅️ ::Add(%\"val_203\", %\"pretrained.blocks.6.attn.qkv.bias\"{...})\n",
       "            194 |  # node_view_7\n",
       "                   %\"view_7\"<FLOAT,[1,1370,3,6,64]> ⬅️ ::Reshape(%\"linear_24\", %\"val_20\"{[1, 1370, 3, 6, 64]}) {allowzero=1}\n",
       "            195 |  # node_permute_6\n",
       "                   %\"permute_6\"<FLOAT,[3,1,6,1370,64]> ⬅️ ::Transpose(%\"view_7\") {perm=(2, 0, 3, 1, 4)}\n",
       "            196 |  # node_select_18\n",
       "                   %\"select_18\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_6\", %\"val_21\"{0}) {axis=0}\n",
       "            197 |  # node_mul_18\n",
       "                   %\"mul_18\"<FLOAT,[1,6,1370,64]> ⬅️ ::Mul(%\"select_18\", %\"val_22\"{0.125})\n",
       "            198 |  # node_select_19\n",
       "                   %\"select_19\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_6\", %\"val_23\"{1}) {axis=0}\n",
       "            199 |  # node_select_20\n",
       "                   %\"select_20\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_6\", %\"val_24\"{2}) {axis=0}\n",
       "            200 |  # node_transpose_13\n",
       "                   %\"transpose_13\"<FLOAT,[1,6,64,1370]> ⬅️ ::Transpose(%\"select_19\") {perm=(0, 1, 3, 2)}\n",
       "            201 |  # node_matmul_12\n",
       "                   %\"matmul_12\"<FLOAT,[1,6,1370,1370]> ⬅️ ::MatMul(%\"mul_18\", %\"transpose_13\")\n",
       "            202 |  # node_softmax_6\n",
       "                   %\"softmax_6\"<FLOAT,[1,6,1370,1370]> ⬅️ ::Softmax(%\"matmul_12\") {axis=-1}\n",
       "            203 |  # node_matmul_13\n",
       "                   %\"matmul_13\"<FLOAT,[1,6,1370,64]> ⬅️ ::MatMul(%\"softmax_6\", %\"select_20\")\n",
       "            204 |  # node_transpose_14\n",
       "                   %\"transpose_14\"<FLOAT,[1,1370,6,64]> ⬅️ ::Transpose(%\"matmul_13\") {perm=(0, 2, 1, 3)}\n",
       "            205 |  # node__unsafe_view_6\n",
       "                   %\"_unsafe_view_6\"<FLOAT,[1,1370,384]> ⬅️ ::Reshape(%\"transpose_14\", %\"val_29\"{[1, 1370, 384]}) {allowzero=1}\n",
       "            206 |  # node_MatMul_191\n",
       "                   %\"val_217\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"_unsafe_view_6\", %\"val_216\"{...})\n",
       "            207 |  # node_linear_25\n",
       "                   %\"linear_25\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_217\", %\"pretrained.blocks.6.attn.proj.bias\"{...})\n",
       "            208 |  # node_mul_19\n",
       "                   %\"mul_19\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_25\", %\"pretrained.blocks.6.ls1.gamma\"{...})\n",
       "            209 |  # node_add_13\n",
       "                   %\"add_13\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_12\", %\"mul_19\")\n",
       "            210 |  # node_layer_norm_13\n",
       "                   %\"layer_norm_13\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_13\", %\"pretrained.blocks.6.norm2.weight\"{...}, %\"pretrained.blocks.6.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            211 |  # node_MatMul_193\n",
       "                   %\"val_221\"<FLOAT,[1,1370,1536]> ⬅️ ::MatMul(%\"layer_norm_13\", %\"val_220\"{...})\n",
       "            212 |  # node_linear_26\n",
       "                   %\"linear_26\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_221\", %\"pretrained.blocks.6.mlp.fc1.bias\"{...})\n",
       "            213 |  # node_Div_195\n",
       "                   %\"val_223\"<FLOAT,[1,1370,1536]> ⬅️ ::Div(%\"linear_26\", %\"val_36\"{1.4142135381698608})\n",
       "            214 |  # node_Erf_196\n",
       "                   %\"val_224\"<FLOAT,[1,1370,1536]> ⬅️ ::Erf(%\"val_223\")\n",
       "            215 |  # node_Add_198\n",
       "                   %\"val_226\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_224\", %\"val_39\"{1.0})\n",
       "            216 |  # node_Mul_200\n",
       "                   %\"val_228\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"val_41\"{0.5}, %\"val_226\")\n",
       "            217 |  # node_gelu_6\n",
       "                   %\"gelu_6\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"linear_26\", %\"val_228\")\n",
       "            218 |  # node_MatMul_202\n",
       "                   %\"val_230\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"gelu_6\", %\"val_229\"{...})\n",
       "            219 |  # node_linear_27\n",
       "                   %\"linear_27\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_230\", %\"pretrained.blocks.6.mlp.fc2.bias\"{...})\n",
       "            220 |  # node_mul_20\n",
       "                   %\"mul_20\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_27\", %\"pretrained.blocks.6.ls2.gamma\"{...})\n",
       "            221 |  # node_add_14\n",
       "                   %\"add_14\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_13\", %\"mul_20\")\n",
       "            222 |  # node_layer_norm_14\n",
       "                   %\"layer_norm_14\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_14\", %\"pretrained.blocks.7.norm1.weight\"{...}, %\"pretrained.blocks.7.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            223 |  # node_MatMul_204\n",
       "                   %\"val_234\"<FLOAT,[1,1370,1152]> ⬅️ ::MatMul(%\"layer_norm_14\", %\"val_233\"{...})\n",
       "            224 |  # node_linear_28\n",
       "                   %\"linear_28\"<FLOAT,[1,1370,1152]> ⬅️ ::Add(%\"val_234\", %\"pretrained.blocks.7.attn.qkv.bias\"{...})\n",
       "            225 |  # node_view_8\n",
       "                   %\"view_8\"<FLOAT,[1,1370,3,6,64]> ⬅️ ::Reshape(%\"linear_28\", %\"val_20\"{[1, 1370, 3, 6, 64]}) {allowzero=1}\n",
       "            226 |  # node_permute_7\n",
       "                   %\"permute_7\"<FLOAT,[3,1,6,1370,64]> ⬅️ ::Transpose(%\"view_8\") {perm=(2, 0, 3, 1, 4)}\n",
       "            227 |  # node_select_21\n",
       "                   %\"select_21\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_7\", %\"val_21\"{0}) {axis=0}\n",
       "            228 |  # node_mul_21\n",
       "                   %\"mul_21\"<FLOAT,[1,6,1370,64]> ⬅️ ::Mul(%\"select_21\", %\"val_22\"{0.125})\n",
       "            229 |  # node_select_22\n",
       "                   %\"select_22\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_7\", %\"val_23\"{1}) {axis=0}\n",
       "            230 |  # node_select_23\n",
       "                   %\"select_23\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_7\", %\"val_24\"{2}) {axis=0}\n",
       "            231 |  # node_transpose_15\n",
       "                   %\"transpose_15\"<FLOAT,[1,6,64,1370]> ⬅️ ::Transpose(%\"select_22\") {perm=(0, 1, 3, 2)}\n",
       "            232 |  # node_matmul_14\n",
       "                   %\"matmul_14\"<FLOAT,[1,6,1370,1370]> ⬅️ ::MatMul(%\"mul_21\", %\"transpose_15\")\n",
       "            233 |  # node_softmax_7\n",
       "                   %\"softmax_7\"<FLOAT,[1,6,1370,1370]> ⬅️ ::Softmax(%\"matmul_14\") {axis=-1}\n",
       "            234 |  # node_matmul_15\n",
       "                   %\"matmul_15\"<FLOAT,[1,6,1370,64]> ⬅️ ::MatMul(%\"softmax_7\", %\"select_23\")\n",
       "            235 |  # node_transpose_16\n",
       "                   %\"transpose_16\"<FLOAT,[1,1370,6,64]> ⬅️ ::Transpose(%\"matmul_15\") {perm=(0, 2, 1, 3)}\n",
       "            236 |  # node__unsafe_view_7\n",
       "                   %\"_unsafe_view_7\"<FLOAT,[1,1370,384]> ⬅️ ::Reshape(%\"transpose_16\", %\"val_29\"{[1, 1370, 384]}) {allowzero=1}\n",
       "            237 |  # node_MatMul_218\n",
       "                   %\"val_248\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"_unsafe_view_7\", %\"val_247\"{...})\n",
       "            238 |  # node_linear_29\n",
       "                   %\"linear_29\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_248\", %\"pretrained.blocks.7.attn.proj.bias\"{...})\n",
       "            239 |  # node_mul_22\n",
       "                   %\"mul_22\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_29\", %\"pretrained.blocks.7.ls1.gamma\"{...})\n",
       "            240 |  # node_add_15\n",
       "                   %\"add_15\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_14\", %\"mul_22\")\n",
       "            241 |  # node_layer_norm_15\n",
       "                   %\"layer_norm_15\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_15\", %\"pretrained.blocks.7.norm2.weight\"{...}, %\"pretrained.blocks.7.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            242 |  # node_MatMul_220\n",
       "                   %\"val_252\"<FLOAT,[1,1370,1536]> ⬅️ ::MatMul(%\"layer_norm_15\", %\"val_251\"{...})\n",
       "            243 |  # node_linear_30\n",
       "                   %\"linear_30\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_252\", %\"pretrained.blocks.7.mlp.fc1.bias\"{...})\n",
       "            244 |  # node_Div_222\n",
       "                   %\"val_254\"<FLOAT,[1,1370,1536]> ⬅️ ::Div(%\"linear_30\", %\"val_36\"{1.4142135381698608})\n",
       "            245 |  # node_Erf_223\n",
       "                   %\"val_255\"<FLOAT,[1,1370,1536]> ⬅️ ::Erf(%\"val_254\")\n",
       "            246 |  # node_Add_225\n",
       "                   %\"val_257\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_255\", %\"val_39\"{1.0})\n",
       "            247 |  # node_Mul_227\n",
       "                   %\"val_259\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"val_41\"{0.5}, %\"val_257\")\n",
       "            248 |  # node_gelu_7\n",
       "                   %\"gelu_7\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"linear_30\", %\"val_259\")\n",
       "            249 |  # node_MatMul_229\n",
       "                   %\"val_261\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"gelu_7\", %\"val_260\"{...})\n",
       "            250 |  # node_linear_31\n",
       "                   %\"linear_31\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_261\", %\"pretrained.blocks.7.mlp.fc2.bias\"{...})\n",
       "            251 |  # node_mul_23\n",
       "                   %\"mul_23\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_31\", %\"pretrained.blocks.7.ls2.gamma\"{...})\n",
       "            252 |  # node_add_16\n",
       "                   %\"add_16\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_15\", %\"mul_23\")\n",
       "            253 |  # node_layer_norm_16\n",
       "                   %\"layer_norm_16\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_16\", %\"pretrained.blocks.8.norm1.weight\"{...}, %\"pretrained.blocks.8.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            254 |  # node_MatMul_231\n",
       "                   %\"val_265\"<FLOAT,[1,1370,1152]> ⬅️ ::MatMul(%\"layer_norm_16\", %\"val_264\"{...})\n",
       "            255 |  # node_linear_32\n",
       "                   %\"linear_32\"<FLOAT,[1,1370,1152]> ⬅️ ::Add(%\"val_265\", %\"pretrained.blocks.8.attn.qkv.bias\"{...})\n",
       "            256 |  # node_view_9\n",
       "                   %\"view_9\"<FLOAT,[1,1370,3,6,64]> ⬅️ ::Reshape(%\"linear_32\", %\"val_20\"{[1, 1370, 3, 6, 64]}) {allowzero=1}\n",
       "            257 |  # node_permute_8\n",
       "                   %\"permute_8\"<FLOAT,[3,1,6,1370,64]> ⬅️ ::Transpose(%\"view_9\") {perm=(2, 0, 3, 1, 4)}\n",
       "            258 |  # node_select_24\n",
       "                   %\"select_24\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_8\", %\"val_21\"{0}) {axis=0}\n",
       "            259 |  # node_mul_24\n",
       "                   %\"mul_24\"<FLOAT,[1,6,1370,64]> ⬅️ ::Mul(%\"select_24\", %\"val_22\"{0.125})\n",
       "            260 |  # node_select_25\n",
       "                   %\"select_25\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_8\", %\"val_23\"{1}) {axis=0}\n",
       "            261 |  # node_select_26\n",
       "                   %\"select_26\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_8\", %\"val_24\"{2}) {axis=0}\n",
       "            262 |  # node_transpose_17\n",
       "                   %\"transpose_17\"<FLOAT,[1,6,64,1370]> ⬅️ ::Transpose(%\"select_25\") {perm=(0, 1, 3, 2)}\n",
       "            263 |  # node_matmul_16\n",
       "                   %\"matmul_16\"<FLOAT,[1,6,1370,1370]> ⬅️ ::MatMul(%\"mul_24\", %\"transpose_17\")\n",
       "            264 |  # node_softmax_8\n",
       "                   %\"softmax_8\"<FLOAT,[1,6,1370,1370]> ⬅️ ::Softmax(%\"matmul_16\") {axis=-1}\n",
       "            265 |  # node_matmul_17\n",
       "                   %\"matmul_17\"<FLOAT,[1,6,1370,64]> ⬅️ ::MatMul(%\"softmax_8\", %\"select_26\")\n",
       "            266 |  # node_transpose_18\n",
       "                   %\"transpose_18\"<FLOAT,[1,1370,6,64]> ⬅️ ::Transpose(%\"matmul_17\") {perm=(0, 2, 1, 3)}\n",
       "            267 |  # node__unsafe_view_8\n",
       "                   %\"_unsafe_view_8\"<FLOAT,[1,1370,384]> ⬅️ ::Reshape(%\"transpose_18\", %\"val_29\"{[1, 1370, 384]}) {allowzero=1}\n",
       "            268 |  # node_MatMul_245\n",
       "                   %\"val_279\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"_unsafe_view_8\", %\"val_278\"{...})\n",
       "            269 |  # node_linear_33\n",
       "                   %\"linear_33\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_279\", %\"pretrained.blocks.8.attn.proj.bias\"{...})\n",
       "            270 |  # node_mul_25\n",
       "                   %\"mul_25\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_33\", %\"pretrained.blocks.8.ls1.gamma\"{...})\n",
       "            271 |  # node_add_17\n",
       "                   %\"add_17\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_16\", %\"mul_25\")\n",
       "            272 |  # node_layer_norm_17\n",
       "                   %\"layer_norm_17\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_17\", %\"pretrained.blocks.8.norm2.weight\"{...}, %\"pretrained.blocks.8.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            273 |  # node_MatMul_247\n",
       "                   %\"val_283\"<FLOAT,[1,1370,1536]> ⬅️ ::MatMul(%\"layer_norm_17\", %\"val_282\"{...})\n",
       "            274 |  # node_linear_34\n",
       "                   %\"linear_34\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_283\", %\"pretrained.blocks.8.mlp.fc1.bias\"{...})\n",
       "            275 |  # node_Div_249\n",
       "                   %\"val_285\"<FLOAT,[1,1370,1536]> ⬅️ ::Div(%\"linear_34\", %\"val_36\"{1.4142135381698608})\n",
       "            276 |  # node_Erf_250\n",
       "                   %\"val_286\"<FLOAT,[1,1370,1536]> ⬅️ ::Erf(%\"val_285\")\n",
       "            277 |  # node_Add_252\n",
       "                   %\"val_288\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_286\", %\"val_39\"{1.0})\n",
       "            278 |  # node_Mul_254\n",
       "                   %\"val_290\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"val_41\"{0.5}, %\"val_288\")\n",
       "            279 |  # node_gelu_8\n",
       "                   %\"gelu_8\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"linear_34\", %\"val_290\")\n",
       "            280 |  # node_MatMul_256\n",
       "                   %\"val_292\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"gelu_8\", %\"val_291\"{...})\n",
       "            281 |  # node_linear_35\n",
       "                   %\"linear_35\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_292\", %\"pretrained.blocks.8.mlp.fc2.bias\"{...})\n",
       "            282 |  # node_mul_26\n",
       "                   %\"mul_26\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_35\", %\"pretrained.blocks.8.ls2.gamma\"{...})\n",
       "            283 |  # node_add_18\n",
       "                   %\"add_18\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_17\", %\"mul_26\")\n",
       "            284 |  # node_layer_norm_18\n",
       "                   %\"layer_norm_18\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_18\", %\"pretrained.blocks.9.norm1.weight\"{...}, %\"pretrained.blocks.9.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            285 |  # node_MatMul_258\n",
       "                   %\"val_296\"<FLOAT,[1,1370,1152]> ⬅️ ::MatMul(%\"layer_norm_18\", %\"val_295\"{...})\n",
       "            286 |  # node_linear_36\n",
       "                   %\"linear_36\"<FLOAT,[1,1370,1152]> ⬅️ ::Add(%\"val_296\", %\"pretrained.blocks.9.attn.qkv.bias\"{...})\n",
       "            287 |  # node_view_10\n",
       "                   %\"view_10\"<FLOAT,[1,1370,3,6,64]> ⬅️ ::Reshape(%\"linear_36\", %\"val_20\"{[1, 1370, 3, 6, 64]}) {allowzero=1}\n",
       "            288 |  # node_permute_9\n",
       "                   %\"permute_9\"<FLOAT,[3,1,6,1370,64]> ⬅️ ::Transpose(%\"view_10\") {perm=(2, 0, 3, 1, 4)}\n",
       "            289 |  # node_select_27\n",
       "                   %\"select_27\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_9\", %\"val_21\"{0}) {axis=0}\n",
       "            290 |  # node_mul_27\n",
       "                   %\"mul_27\"<FLOAT,[1,6,1370,64]> ⬅️ ::Mul(%\"select_27\", %\"val_22\"{0.125})\n",
       "            291 |  # node_select_28\n",
       "                   %\"select_28\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_9\", %\"val_23\"{1}) {axis=0}\n",
       "            292 |  # node_select_29\n",
       "                   %\"select_29\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_9\", %\"val_24\"{2}) {axis=0}\n",
       "            293 |  # node_transpose_19\n",
       "                   %\"transpose_19\"<FLOAT,[1,6,64,1370]> ⬅️ ::Transpose(%\"select_28\") {perm=(0, 1, 3, 2)}\n",
       "            294 |  # node_matmul_18\n",
       "                   %\"matmul_18\"<FLOAT,[1,6,1370,1370]> ⬅️ ::MatMul(%\"mul_27\", %\"transpose_19\")\n",
       "            295 |  # node_softmax_9\n",
       "                   %\"softmax_9\"<FLOAT,[1,6,1370,1370]> ⬅️ ::Softmax(%\"matmul_18\") {axis=-1}\n",
       "            296 |  # node_matmul_19\n",
       "                   %\"matmul_19\"<FLOAT,[1,6,1370,64]> ⬅️ ::MatMul(%\"softmax_9\", %\"select_29\")\n",
       "            297 |  # node_transpose_20\n",
       "                   %\"transpose_20\"<FLOAT,[1,1370,6,64]> ⬅️ ::Transpose(%\"matmul_19\") {perm=(0, 2, 1, 3)}\n",
       "            298 |  # node__unsafe_view_9\n",
       "                   %\"_unsafe_view_9\"<FLOAT,[1,1370,384]> ⬅️ ::Reshape(%\"transpose_20\", %\"val_29\"{[1, 1370, 384]}) {allowzero=1}\n",
       "            299 |  # node_MatMul_272\n",
       "                   %\"val_310\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"_unsafe_view_9\", %\"val_309\"{...})\n",
       "            300 |  # node_linear_37\n",
       "                   %\"linear_37\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_310\", %\"pretrained.blocks.9.attn.proj.bias\"{...})\n",
       "            301 |  # node_mul_28\n",
       "                   %\"mul_28\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_37\", %\"pretrained.blocks.9.ls1.gamma\"{...})\n",
       "            302 |  # node_add_19\n",
       "                   %\"add_19\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_18\", %\"mul_28\")\n",
       "            303 |  # node_layer_norm_19\n",
       "                   %\"layer_norm_19\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_19\", %\"pretrained.blocks.9.norm2.weight\"{...}, %\"pretrained.blocks.9.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            304 |  # node_MatMul_274\n",
       "                   %\"val_314\"<FLOAT,[1,1370,1536]> ⬅️ ::MatMul(%\"layer_norm_19\", %\"val_313\"{...})\n",
       "            305 |  # node_linear_38\n",
       "                   %\"linear_38\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_314\", %\"pretrained.blocks.9.mlp.fc1.bias\"{...})\n",
       "            306 |  # node_Div_276\n",
       "                   %\"val_316\"<FLOAT,[1,1370,1536]> ⬅️ ::Div(%\"linear_38\", %\"val_36\"{1.4142135381698608})\n",
       "            307 |  # node_Erf_277\n",
       "                   %\"val_317\"<FLOAT,[1,1370,1536]> ⬅️ ::Erf(%\"val_316\")\n",
       "            308 |  # node_Add_279\n",
       "                   %\"val_319\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_317\", %\"val_39\"{1.0})\n",
       "            309 |  # node_Mul_281\n",
       "                   %\"val_321\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"val_41\"{0.5}, %\"val_319\")\n",
       "            310 |  # node_gelu_9\n",
       "                   %\"gelu_9\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"linear_38\", %\"val_321\")\n",
       "            311 |  # node_MatMul_283\n",
       "                   %\"val_323\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"gelu_9\", %\"val_322\"{...})\n",
       "            312 |  # node_linear_39\n",
       "                   %\"linear_39\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_323\", %\"pretrained.blocks.9.mlp.fc2.bias\"{...})\n",
       "            313 |  # node_mul_29\n",
       "                   %\"mul_29\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_39\", %\"pretrained.blocks.9.ls2.gamma\"{...})\n",
       "            314 |  # node_add_20\n",
       "                   %\"add_20\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_19\", %\"mul_29\")\n",
       "            315 |  # node_layer_norm_20\n",
       "                   %\"layer_norm_20\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_20\", %\"pretrained.blocks.10.norm1.weight\"{...}, %\"pretrained.blocks.10.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            316 |  # node_MatMul_285\n",
       "                   %\"val_327\"<FLOAT,[1,1370,1152]> ⬅️ ::MatMul(%\"layer_norm_20\", %\"val_326\"{...})\n",
       "            317 |  # node_linear_40\n",
       "                   %\"linear_40\"<FLOAT,[1,1370,1152]> ⬅️ ::Add(%\"val_327\", %\"pretrained.blocks.10.attn.qkv.bias\"{...})\n",
       "            318 |  # node_view_11\n",
       "                   %\"view_11\"<FLOAT,[1,1370,3,6,64]> ⬅️ ::Reshape(%\"linear_40\", %\"val_20\"{[1, 1370, 3, 6, 64]}) {allowzero=1}\n",
       "            319 |  # node_permute_10\n",
       "                   %\"permute_10\"<FLOAT,[3,1,6,1370,64]> ⬅️ ::Transpose(%\"view_11\") {perm=(2, 0, 3, 1, 4)}\n",
       "            320 |  # node_select_30\n",
       "                   %\"select_30\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_10\", %\"val_21\"{0}) {axis=0}\n",
       "            321 |  # node_mul_30\n",
       "                   %\"mul_30\"<FLOAT,[1,6,1370,64]> ⬅️ ::Mul(%\"select_30\", %\"val_22\"{0.125})\n",
       "            322 |  # node_select_31\n",
       "                   %\"select_31\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_10\", %\"val_23\"{1}) {axis=0}\n",
       "            323 |  # node_select_32\n",
       "                   %\"select_32\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_10\", %\"val_24\"{2}) {axis=0}\n",
       "            324 |  # node_transpose_21\n",
       "                   %\"transpose_21\"<FLOAT,[1,6,64,1370]> ⬅️ ::Transpose(%\"select_31\") {perm=(0, 1, 3, 2)}\n",
       "            325 |  # node_matmul_20\n",
       "                   %\"matmul_20\"<FLOAT,[1,6,1370,1370]> ⬅️ ::MatMul(%\"mul_30\", %\"transpose_21\")\n",
       "            326 |  # node_softmax_10\n",
       "                   %\"softmax_10\"<FLOAT,[1,6,1370,1370]> ⬅️ ::Softmax(%\"matmul_20\") {axis=-1}\n",
       "            327 |  # node_matmul_21\n",
       "                   %\"matmul_21\"<FLOAT,[1,6,1370,64]> ⬅️ ::MatMul(%\"softmax_10\", %\"select_32\")\n",
       "            328 |  # node_transpose_22\n",
       "                   %\"transpose_22\"<FLOAT,[1,1370,6,64]> ⬅️ ::Transpose(%\"matmul_21\") {perm=(0, 2, 1, 3)}\n",
       "            329 |  # node__unsafe_view_10\n",
       "                   %\"_unsafe_view_10\"<FLOAT,[1,1370,384]> ⬅️ ::Reshape(%\"transpose_22\", %\"val_29\"{[1, 1370, 384]}) {allowzero=1}\n",
       "            330 |  # node_MatMul_299\n",
       "                   %\"val_341\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"_unsafe_view_10\", %\"val_340\"{...})\n",
       "            331 |  # node_linear_41\n",
       "                   %\"linear_41\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_341\", %\"pretrained.blocks.10.attn.proj.bias\"{...})\n",
       "            332 |  # node_mul_31\n",
       "                   %\"mul_31\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_41\", %\"pretrained.blocks.10.ls1.gamma\"{...})\n",
       "            333 |  # node_add_21\n",
       "                   %\"add_21\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_20\", %\"mul_31\")\n",
       "            334 |  # node_layer_norm_21\n",
       "                   %\"layer_norm_21\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_21\", %\"pretrained.blocks.10.norm2.weight\"{...}, %\"pretrained.blocks.10.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            335 |  # node_MatMul_301\n",
       "                   %\"val_345\"<FLOAT,[1,1370,1536]> ⬅️ ::MatMul(%\"layer_norm_21\", %\"val_344\"{...})\n",
       "            336 |  # node_linear_42\n",
       "                   %\"linear_42\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_345\", %\"pretrained.blocks.10.mlp.fc1.bias\"{...})\n",
       "            337 |  # node_Div_303\n",
       "                   %\"val_347\"<FLOAT,[1,1370,1536]> ⬅️ ::Div(%\"linear_42\", %\"val_36\"{1.4142135381698608})\n",
       "            338 |  # node_Erf_304\n",
       "                   %\"val_348\"<FLOAT,[1,1370,1536]> ⬅️ ::Erf(%\"val_347\")\n",
       "            339 |  # node_Add_306\n",
       "                   %\"val_350\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_348\", %\"val_39\"{1.0})\n",
       "            340 |  # node_Mul_308\n",
       "                   %\"val_352\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"val_41\"{0.5}, %\"val_350\")\n",
       "            341 |  # node_gelu_10\n",
       "                   %\"gelu_10\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"linear_42\", %\"val_352\")\n",
       "            342 |  # node_MatMul_310\n",
       "                   %\"val_354\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"gelu_10\", %\"val_353\"{...})\n",
       "            343 |  # node_linear_43\n",
       "                   %\"linear_43\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_354\", %\"pretrained.blocks.10.mlp.fc2.bias\"{...})\n",
       "            344 |  # node_mul_32\n",
       "                   %\"mul_32\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_43\", %\"pretrained.blocks.10.ls2.gamma\"{...})\n",
       "            345 |  # node_add_22\n",
       "                   %\"add_22\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_21\", %\"mul_32\")\n",
       "            346 |  # node_layer_norm_22\n",
       "                   %\"layer_norm_22\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_22\", %\"pretrained.blocks.11.norm1.weight\"{...}, %\"pretrained.blocks.11.norm1.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            347 |  # node_MatMul_312\n",
       "                   %\"val_358\"<FLOAT,[1,1370,1152]> ⬅️ ::MatMul(%\"layer_norm_22\", %\"val_357\"{...})\n",
       "            348 |  # node_linear_44\n",
       "                   %\"linear_44\"<FLOAT,[1,1370,1152]> ⬅️ ::Add(%\"val_358\", %\"pretrained.blocks.11.attn.qkv.bias\"{...})\n",
       "            349 |  # node_view_12\n",
       "                   %\"view_12\"<FLOAT,[1,1370,3,6,64]> ⬅️ ::Reshape(%\"linear_44\", %\"val_20\"{[1, 1370, 3, 6, 64]}) {allowzero=1}\n",
       "            350 |  # node_permute_11\n",
       "                   %\"permute_11\"<FLOAT,[3,1,6,1370,64]> ⬅️ ::Transpose(%\"view_12\") {perm=(2, 0, 3, 1, 4)}\n",
       "            351 |  # node_select_33\n",
       "                   %\"select_33\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_11\", %\"val_21\"{0}) {axis=0}\n",
       "            352 |  # node_mul_33\n",
       "                   %\"mul_33\"<FLOAT,[1,6,1370,64]> ⬅️ ::Mul(%\"select_33\", %\"val_22\"{0.125})\n",
       "            353 |  # node_select_34\n",
       "                   %\"select_34\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_11\", %\"val_23\"{1}) {axis=0}\n",
       "            354 |  # node_select_35\n",
       "                   %\"select_35\"<FLOAT,[1,6,1370,64]> ⬅️ ::Gather(%\"permute_11\", %\"val_24\"{2}) {axis=0}\n",
       "            355 |  # node_transpose_23\n",
       "                   %\"transpose_23\"<FLOAT,[1,6,64,1370]> ⬅️ ::Transpose(%\"select_34\") {perm=(0, 1, 3, 2)}\n",
       "            356 |  # node_matmul_22\n",
       "                   %\"matmul_22\"<FLOAT,[1,6,1370,1370]> ⬅️ ::MatMul(%\"mul_33\", %\"transpose_23\")\n",
       "            357 |  # node_softmax_11\n",
       "                   %\"softmax_11\"<FLOAT,[1,6,1370,1370]> ⬅️ ::Softmax(%\"matmul_22\") {axis=-1}\n",
       "            358 |  # node_matmul_23\n",
       "                   %\"matmul_23\"<FLOAT,[1,6,1370,64]> ⬅️ ::MatMul(%\"softmax_11\", %\"select_35\")\n",
       "            359 |  # node_transpose_24\n",
       "                   %\"transpose_24\"<FLOAT,[1,1370,6,64]> ⬅️ ::Transpose(%\"matmul_23\") {perm=(0, 2, 1, 3)}\n",
       "            360 |  # node__unsafe_view_11\n",
       "                   %\"_unsafe_view_11\"<FLOAT,[1,1370,384]> ⬅️ ::Reshape(%\"transpose_24\", %\"val_29\"{[1, 1370, 384]}) {allowzero=1}\n",
       "            361 |  # node_MatMul_326\n",
       "                   %\"val_372\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"_unsafe_view_11\", %\"val_371\"{...})\n",
       "            362 |  # node_linear_45\n",
       "                   %\"linear_45\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_372\", %\"pretrained.blocks.11.attn.proj.bias\"{...})\n",
       "            363 |  # node_mul_34\n",
       "                   %\"mul_34\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_45\", %\"pretrained.blocks.11.ls1.gamma\"{...})\n",
       "            364 |  # node_add_23\n",
       "                   %\"add_23\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_22\", %\"mul_34\")\n",
       "            365 |  # node_layer_norm_23\n",
       "                   %\"layer_norm_23\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_23\", %\"pretrained.blocks.11.norm2.weight\"{...}, %\"pretrained.blocks.11.norm2.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            366 |  # node_MatMul_328\n",
       "                   %\"val_376\"<FLOAT,[1,1370,1536]> ⬅️ ::MatMul(%\"layer_norm_23\", %\"val_375\"{...})\n",
       "            367 |  # node_linear_46\n",
       "                   %\"linear_46\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_376\", %\"pretrained.blocks.11.mlp.fc1.bias\"{...})\n",
       "            368 |  # node_Div_330\n",
       "                   %\"val_378\"<FLOAT,[1,1370,1536]> ⬅️ ::Div(%\"linear_46\", %\"val_36\"{1.4142135381698608})\n",
       "            369 |  # node_Erf_331\n",
       "                   %\"val_379\"<FLOAT,[1,1370,1536]> ⬅️ ::Erf(%\"val_378\")\n",
       "            370 |  # node_Add_333\n",
       "                   %\"val_381\"<FLOAT,[1,1370,1536]> ⬅️ ::Add(%\"val_379\", %\"val_39\"{1.0})\n",
       "            371 |  # node_Mul_335\n",
       "                   %\"val_383\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"val_41\"{0.5}, %\"val_381\")\n",
       "            372 |  # node_gelu_11\n",
       "                   %\"gelu_11\"<FLOAT,[1,1370,1536]> ⬅️ ::Mul(%\"linear_46\", %\"val_383\")\n",
       "            373 |  # node_MatMul_337\n",
       "                   %\"val_385\"<FLOAT,[1,1370,384]> ⬅️ ::MatMul(%\"gelu_11\", %\"val_384\"{...})\n",
       "            374 |  # node_linear_47\n",
       "                   %\"linear_47\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"val_385\", %\"pretrained.blocks.11.mlp.fc2.bias\"{...})\n",
       "            375 |  # node_mul_35\n",
       "                   %\"mul_35\"<FLOAT,[1,1370,384]> ⬅️ ::Mul(%\"linear_47\", %\"pretrained.blocks.11.ls2.gamma\"{...})\n",
       "            376 |  # node_add_24\n",
       "                   %\"add_24\"<FLOAT,[1,1370,384]> ⬅️ ::Add(%\"add_23\", %\"mul_35\")\n",
       "            377 |  # node_layer_norm_24\n",
       "                   %\"layer_norm_24\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_6\", %\"pretrained.norm.weight\"{...}, %\"pretrained.norm.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            378 |  # node_layer_norm_25\n",
       "                   %\"layer_norm_25\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_12\", %\"pretrained.norm.weight\"{...}, %\"pretrained.norm.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            379 |  # node_layer_norm_26\n",
       "                   %\"layer_norm_26\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_18\", %\"pretrained.norm.weight\"{...}, %\"pretrained.norm.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            380 |  # node_layer_norm_27\n",
       "                   %\"layer_norm_27\"<FLOAT,[1,1370,384]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_24\", %\"pretrained.norm.weight\"{...}, %\"pretrained.norm.bias\"{...}) {stash_type=1, epsilon=1e-06, axis=-1}\n",
       "            381 |  # node_slice_1\n",
       "                   %\"slice_1\"<FLOAT,[1,1369,384]> ⬅️ ::Slice(%\"layer_norm_24\", %\"val_396\"{[1]}, %\"val_400\"{[9223372036854775807]}, %\"val_396\"{[1]}, %\"val_396\"{[1]})\n",
       "            382 |  # node_slice_2\n",
       "                   %\"slice_2\"<FLOAT,[1,1369,384]> ⬅️ ::Slice(%\"layer_norm_25\", %\"val_396\"{[1]}, %\"val_400\"{[9223372036854775807]}, %\"val_396\"{[1]}, %\"val_396\"{[1]})\n",
       "            383 |  # node_slice_3\n",
       "                   %\"slice_3\"<FLOAT,[1,1369,384]> ⬅️ ::Slice(%\"layer_norm_26\", %\"val_396\"{[1]}, %\"val_400\"{[9223372036854775807]}, %\"val_396\"{[1]}, %\"val_396\"{[1]})\n",
       "            384 |  # node_slice_4\n",
       "                   %\"slice_4\"<FLOAT,[1,1369,384]> ⬅️ ::Slice(%\"layer_norm_27\", %\"val_396\"{[1]}, %\"val_400\"{[9223372036854775807]}, %\"val_396\"{[1]}, %\"val_396\"{[1]})\n",
       "            385 |  # node_permute_12\n",
       "                   %\"permute_12\"<FLOAT,[1,384,1369]> ⬅️ ::Transpose(%\"slice_1\") {perm=(0, 2, 1)}\n",
       "            386 |  # node_view_13\n",
       "                   %\"view_13\"<FLOAT,[1,384,37,37]> ⬅️ ::Reshape(%\"permute_12\", %\"val_440\"{[1, 384, 37, 37]}) {allowzero=1}\n",
       "            387 |  # node_conv2d_1\n",
       "                   %\"conv2d_1\"<FLOAT,[1,48,37,37]> ⬅️ ::Conv(%\"view_13\", %\"depth_head.projects.0.weight\"{...}, %\"depth_head.projects.0.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            388 |  # node_convolution\n",
       "                   %\"convolution\"<FLOAT,[1,48,148,148]> ⬅️ ::ConvTranspose(%\"conv2d_1\", %\"depth_head.resize_layers.0.weight\"{...}, %\"depth_head.resize_layers.0.bias\"{...}) {group=1, auto_pad='NOTSET', strides=(4, 4), pads=(0, 0, 0, 0), output_padding=(0, 0), dilations=(1, 1)}\n",
       "            389 |  # node_permute_13\n",
       "                   %\"permute_13\"<FLOAT,[1,384,1369]> ⬅️ ::Transpose(%\"slice_2\") {perm=(0, 2, 1)}\n",
       "            390 |  # node_view_14\n",
       "                   %\"view_14\"<FLOAT,[1,384,37,37]> ⬅️ ::Reshape(%\"permute_13\", %\"val_440\"{[1, 384, 37, 37]}) {allowzero=1}\n",
       "            391 |  # node_conv2d_2\n",
       "                   %\"conv2d_2\"<FLOAT,[1,96,37,37]> ⬅️ ::Conv(%\"view_14\", %\"depth_head.projects.1.weight\"{...}, %\"depth_head.projects.1.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            392 |  # node_convolution_1\n",
       "                   %\"convolution_1\"<FLOAT,[1,96,74,74]> ⬅️ ::ConvTranspose(%\"conv2d_2\", %\"depth_head.resize_layers.1.weight\"{...}, %\"depth_head.resize_layers.1.bias\"{...}) {group=1, auto_pad='NOTSET', strides=(2, 2), pads=(0, 0, 0, 0), output_padding=(0, 0), dilations=(1, 1)}\n",
       "            393 |  # node_permute_14\n",
       "                   %\"permute_14\"<FLOAT,[1,384,1369]> ⬅️ ::Transpose(%\"slice_3\") {perm=(0, 2, 1)}\n",
       "            394 |  # node_view_15\n",
       "                   %\"view_15\"<FLOAT,[1,384,37,37]> ⬅️ ::Reshape(%\"permute_14\", %\"val_440\"{[1, 384, 37, 37]}) {allowzero=1}\n",
       "            395 |  # node_conv2d_3\n",
       "                   %\"conv2d_3\"<FLOAT,[1,192,37,37]> ⬅️ ::Conv(%\"view_15\", %\"depth_head.projects.2.weight\"{...}, %\"depth_head.projects.2.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            396 |  # node_permute_15\n",
       "                   %\"permute_15\"<FLOAT,[1,384,1369]> ⬅️ ::Transpose(%\"slice_4\") {perm=(0, 2, 1)}\n",
       "            397 |  # node_view_16\n",
       "                   %\"view_16\"<FLOAT,[1,384,37,37]> ⬅️ ::Reshape(%\"permute_15\", %\"val_440\"{[1, 384, 37, 37]}) {allowzero=1}\n",
       "            398 |  # node_conv2d_4\n",
       "                   %\"conv2d_4\"<FLOAT,[1,384,37,37]> ⬅️ ::Conv(%\"view_16\", %\"depth_head.projects.3.weight\"{...}, %\"depth_head.projects.3.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            399 |  # node_conv2d_5\n",
       "                   %\"conv2d_5\"<FLOAT,[1,384,19,19]> ⬅️ ::Conv(%\"conv2d_4\", %\"depth_head.resize_layers.3.weight\"{...}, %\"depth_head.resize_layers.3.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(2, 2), dilations=(1, 1)}\n",
       "            400 |  # node_Conv_474\n",
       "                   %\"conv2d_6\"<FLOAT,[1,64,148,148]> ⬅️ ::Conv(%\"convolution\", %\"depth_head.scratch.layer1_rn.weight\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            401 |  # node_Conv_475\n",
       "                   %\"conv2d_7\"<FLOAT,[1,64,74,74]> ⬅️ ::Conv(%\"convolution_1\", %\"depth_head.scratch.layer2_rn.weight\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            402 |  # node_Conv_476\n",
       "                   %\"conv2d_8\"<FLOAT,[1,64,37,37]> ⬅️ ::Conv(%\"conv2d_3\", %\"depth_head.scratch.layer3_rn.weight\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            403 |  # node_Conv_477\n",
       "                   %\"conv2d_9\"<FLOAT,[1,64,19,19]> ⬅️ ::Conv(%\"conv2d_5\", %\"depth_head.scratch.layer4_rn.weight\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            404 |  # node_relu\n",
       "                   %\"relu\"<FLOAT,[1,64,19,19]> ⬅️ ::Relu(%\"conv2d_9\")\n",
       "            405 |  # node_conv2d_10\n",
       "                   %\"conv2d_10\"<FLOAT,[1,64,19,19]> ⬅️ ::Conv(%\"relu\", %\"depth_head.scratch.refinenet4.resConfUnit2.conv1.weight\"{...}, %\"depth_head.scratch.refinenet4.resConfUnit2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            406 |  # node_relu_1\n",
       "                   %\"relu_1\"<FLOAT,[1,64,19,19]> ⬅️ ::Relu(%\"conv2d_10\")\n",
       "            407 |  # node_conv2d_11\n",
       "                   %\"conv2d_11\"<FLOAT,[1,64,19,19]> ⬅️ ::Conv(%\"relu_1\", %\"depth_head.scratch.refinenet4.resConfUnit2.conv2.weight\"{...}, %\"depth_head.scratch.refinenet4.resConfUnit2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            408 |  # node_add_25\n",
       "                   %\"add_25\"<FLOAT,[1,64,19,19]> ⬅️ ::Add(%\"conv2d_11\", %\"conv2d_9\")\n",
       "            409 |  # node_upsample_bilinear2d\n",
       "                   %\"upsample_bilinear2d\"<FLOAT,[1,64,37,37]> ⬅️ ::Resize(%\"add_25\", None, None, %\"val_483\"{[1, 64, 37, 37]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='align_corners', cubic_coeff_a=-0.75, mode='linear'}\n",
       "            410 |  # node_conv2d_12\n",
       "                   %\"conv2d_12\"<FLOAT,[1,64,37,37]> ⬅️ ::Conv(%\"upsample_bilinear2d\", %\"depth_head.scratch.refinenet4.out_conv.weight\"{...}, %\"depth_head.scratch.refinenet4.out_conv.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            411 |  # node_relu_2\n",
       "                   %\"relu_2\"<FLOAT,[1,64,37,37]> ⬅️ ::Relu(%\"conv2d_8\")\n",
       "            412 |  # node_conv2d_13\n",
       "                   %\"conv2d_13\"<FLOAT,[1,64,37,37]> ⬅️ ::Conv(%\"relu_2\", %\"depth_head.scratch.refinenet3.resConfUnit1.conv1.weight\"{...}, %\"depth_head.scratch.refinenet3.resConfUnit1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            413 |  # node_relu_3\n",
       "                   %\"relu_3\"<FLOAT,[1,64,37,37]> ⬅️ ::Relu(%\"conv2d_13\")\n",
       "            414 |  # node_conv2d_14\n",
       "                   %\"conv2d_14\"<FLOAT,[1,64,37,37]> ⬅️ ::Conv(%\"relu_3\", %\"depth_head.scratch.refinenet3.resConfUnit1.conv2.weight\"{...}, %\"depth_head.scratch.refinenet3.resConfUnit1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            415 |  # node_add_26\n",
       "                   %\"add_26\"<FLOAT,[1,64,37,37]> ⬅️ ::Add(%\"conv2d_14\", %\"conv2d_8\")\n",
       "            416 |  # node_add_27\n",
       "                   %\"add_27\"<FLOAT,[1,64,37,37]> ⬅️ ::Add(%\"conv2d_12\", %\"add_26\")\n",
       "            417 |  # node_relu_4\n",
       "                   %\"relu_4\"<FLOAT,[1,64,37,37]> ⬅️ ::Relu(%\"add_27\")\n",
       "            418 |  # node_conv2d_15\n",
       "                   %\"conv2d_15\"<FLOAT,[1,64,37,37]> ⬅️ ::Conv(%\"relu_4\", %\"depth_head.scratch.refinenet3.resConfUnit2.conv1.weight\"{...}, %\"depth_head.scratch.refinenet3.resConfUnit2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            419 |  # node_relu_5\n",
       "                   %\"relu_5\"<FLOAT,[1,64,37,37]> ⬅️ ::Relu(%\"conv2d_15\")\n",
       "            420 |  # node_conv2d_16\n",
       "                   %\"conv2d_16\"<FLOAT,[1,64,37,37]> ⬅️ ::Conv(%\"relu_5\", %\"depth_head.scratch.refinenet3.resConfUnit2.conv2.weight\"{...}, %\"depth_head.scratch.refinenet3.resConfUnit2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            421 |  # node_add_28\n",
       "                   %\"add_28\"<FLOAT,[1,64,37,37]> ⬅️ ::Add(%\"conv2d_16\", %\"add_27\")\n",
       "            422 |  # node_upsample_bilinear2d_1\n",
       "                   %\"upsample_bilinear2d_1\"<FLOAT,[1,64,74,74]> ⬅️ ::Resize(%\"add_28\", None, None, %\"val_487\"{[1, 64, 74, 74]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='align_corners', cubic_coeff_a=-0.75, mode='linear'}\n",
       "            423 |  # node_conv2d_17\n",
       "                   %\"conv2d_17\"<FLOAT,[1,64,74,74]> ⬅️ ::Conv(%\"upsample_bilinear2d_1\", %\"depth_head.scratch.refinenet3.out_conv.weight\"{...}, %\"depth_head.scratch.refinenet3.out_conv.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            424 |  # node_relu_6\n",
       "                   %\"relu_6\"<FLOAT,[1,64,74,74]> ⬅️ ::Relu(%\"conv2d_7\")\n",
       "            425 |  # node_conv2d_18\n",
       "                   %\"conv2d_18\"<FLOAT,[1,64,74,74]> ⬅️ ::Conv(%\"relu_6\", %\"depth_head.scratch.refinenet2.resConfUnit1.conv1.weight\"{...}, %\"depth_head.scratch.refinenet2.resConfUnit1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            426 |  # node_relu_7\n",
       "                   %\"relu_7\"<FLOAT,[1,64,74,74]> ⬅️ ::Relu(%\"conv2d_18\")\n",
       "            427 |  # node_conv2d_19\n",
       "                   %\"conv2d_19\"<FLOAT,[1,64,74,74]> ⬅️ ::Conv(%\"relu_7\", %\"depth_head.scratch.refinenet2.resConfUnit1.conv2.weight\"{...}, %\"depth_head.scratch.refinenet2.resConfUnit1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            428 |  # node_add_29\n",
       "                   %\"add_29\"<FLOAT,[1,64,74,74]> ⬅️ ::Add(%\"conv2d_19\", %\"conv2d_7\")\n",
       "            429 |  # node_add_30\n",
       "                   %\"add_30\"<FLOAT,[1,64,74,74]> ⬅️ ::Add(%\"conv2d_17\", %\"add_29\")\n",
       "            430 |  # node_relu_8\n",
       "                   %\"relu_8\"<FLOAT,[1,64,74,74]> ⬅️ ::Relu(%\"add_30\")\n",
       "            431 |  # node_conv2d_20\n",
       "                   %\"conv2d_20\"<FLOAT,[1,64,74,74]> ⬅️ ::Conv(%\"relu_8\", %\"depth_head.scratch.refinenet2.resConfUnit2.conv1.weight\"{...}, %\"depth_head.scratch.refinenet2.resConfUnit2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            432 |  # node_relu_9\n",
       "                   %\"relu_9\"<FLOAT,[1,64,74,74]> ⬅️ ::Relu(%\"conv2d_20\")\n",
       "            433 |  # node_conv2d_21\n",
       "                   %\"conv2d_21\"<FLOAT,[1,64,74,74]> ⬅️ ::Conv(%\"relu_9\", %\"depth_head.scratch.refinenet2.resConfUnit2.conv2.weight\"{...}, %\"depth_head.scratch.refinenet2.resConfUnit2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            434 |  # node_add_31\n",
       "                   %\"add_31\"<FLOAT,[1,64,74,74]> ⬅️ ::Add(%\"conv2d_21\", %\"add_30\")\n",
       "            435 |  # node_upsample_bilinear2d_2\n",
       "                   %\"upsample_bilinear2d_2\"<FLOAT,[1,64,148,148]> ⬅️ ::Resize(%\"add_31\", None, None, %\"val_491\"{[1, 64, 148, 148]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='align_corners', cubic_coeff_a=-0.75, mode='linear'}\n",
       "            436 |  # node_conv2d_22\n",
       "                   %\"conv2d_22\"<FLOAT,[1,64,148,148]> ⬅️ ::Conv(%\"upsample_bilinear2d_2\", %\"depth_head.scratch.refinenet2.out_conv.weight\"{...}, %\"depth_head.scratch.refinenet2.out_conv.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            437 |  # node_relu_10\n",
       "                   %\"relu_10\"<FLOAT,[1,64,148,148]> ⬅️ ::Relu(%\"conv2d_6\")\n",
       "            438 |  # node_conv2d_23\n",
       "                   %\"conv2d_23\"<FLOAT,[1,64,148,148]> ⬅️ ::Conv(%\"relu_10\", %\"depth_head.scratch.refinenet1.resConfUnit1.conv1.weight\"{...}, %\"depth_head.scratch.refinenet1.resConfUnit1.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            439 |  # node_relu_11\n",
       "                   %\"relu_11\"<FLOAT,[1,64,148,148]> ⬅️ ::Relu(%\"conv2d_23\")\n",
       "            440 |  # node_conv2d_24\n",
       "                   %\"conv2d_24\"<FLOAT,[1,64,148,148]> ⬅️ ::Conv(%\"relu_11\", %\"depth_head.scratch.refinenet1.resConfUnit1.conv2.weight\"{...}, %\"depth_head.scratch.refinenet1.resConfUnit1.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            441 |  # node_add_32\n",
       "                   %\"add_32\"<FLOAT,[1,64,148,148]> ⬅️ ::Add(%\"conv2d_24\", %\"conv2d_6\")\n",
       "            442 |  # node_add_33\n",
       "                   %\"add_33\"<FLOAT,[1,64,148,148]> ⬅️ ::Add(%\"conv2d_22\", %\"add_32\")\n",
       "            443 |  # node_relu_12\n",
       "                   %\"relu_12\"<FLOAT,[1,64,148,148]> ⬅️ ::Relu(%\"add_33\")\n",
       "            444 |  # node_conv2d_25\n",
       "                   %\"conv2d_25\"<FLOAT,[1,64,148,148]> ⬅️ ::Conv(%\"relu_12\", %\"depth_head.scratch.refinenet1.resConfUnit2.conv1.weight\"{...}, %\"depth_head.scratch.refinenet1.resConfUnit2.conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            445 |  # node_relu_13\n",
       "                   %\"relu_13\"<FLOAT,[1,64,148,148]> ⬅️ ::Relu(%\"conv2d_25\")\n",
       "            446 |  # node_conv2d_26\n",
       "                   %\"conv2d_26\"<FLOAT,[1,64,148,148]> ⬅️ ::Conv(%\"relu_13\", %\"depth_head.scratch.refinenet1.resConfUnit2.conv2.weight\"{...}, %\"depth_head.scratch.refinenet1.resConfUnit2.conv2.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            447 |  # node_add_34\n",
       "                   %\"add_34\"<FLOAT,[1,64,148,148]> ⬅️ ::Add(%\"conv2d_26\", %\"add_33\")\n",
       "            448 |  # node_upsample_bilinear2d_3\n",
       "                   %\"upsample_bilinear2d_3\"<FLOAT,[1,64,296,296]> ⬅️ ::Resize(%\"add_34\", None, %\"val_492\"{[1.0, 1.0, 2.0, 2.0]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='align_corners', cubic_coeff_a=-0.75, mode='linear'}\n",
       "            449 |  # node_conv2d_27\n",
       "                   %\"conv2d_27\"<FLOAT,[1,64,296,296]> ⬅️ ::Conv(%\"upsample_bilinear2d_3\", %\"depth_head.scratch.refinenet1.out_conv.weight\"{...}, %\"depth_head.scratch.refinenet1.out_conv.bias\"{...}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            450 |  # node_conv2d_28\n",
       "                   %\"conv2d_28\"<FLOAT,[1,32,296,296]> ⬅️ ::Conv(%\"conv2d_27\", %\"depth_head.scratch.output_conv1.weight\"{...}, %\"depth_head.scratch.output_conv1.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            451 |  # node_upsample_bilinear2d_4\n",
       "                   %\"upsample_bilinear2d_4\"<FLOAT,[1,32,518,518]> ⬅️ ::Resize(%\"conv2d_28\", None, None, %\"val_496\"{[1, 32, 518, 518]}) {keep_aspect_ratio_policy='stretch', antialias=0, extrapolation_value=0.0, exclude_outside=0, nearest_mode='floor', coordinate_transformation_mode='align_corners', cubic_coeff_a=-0.75, mode='linear'}\n",
       "            452 |  # node_conv2d_29\n",
       "                   %\"conv2d_29\"<FLOAT,[1,32,518,518]> ⬅️ ::Conv(%\"upsample_bilinear2d_4\", %\"depth_head.scratch.output_conv2.0.weight\"{...}, %\"depth_head.scratch.output_conv2.0.bias\"{...}) {group=1, pads=(1, 1, 1, 1), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            453 |  # node_relu_14\n",
       "                   %\"relu_14\"<FLOAT,[1,32,518,518]> ⬅️ ::Relu(%\"conv2d_29\")\n",
       "            454 |  # node_conv2d_30\n",
       "                   %\"conv2d_30\"<FLOAT,[1,1,518,518]> ⬅️ ::Conv(%\"relu_14\", %\"depth_head.scratch.output_conv2.2.weight\"{...}, %\"depth_head.scratch.output_conv2.2.bias\"{[0.08870424330234528]}) {group=1, pads=(0, 0, 0, 0), auto_pad='NOTSET', strides=(1, 1), dilations=(1, 1)}\n",
       "            455 |  # node_Relu_478\n",
       "                   %\"relu_16\"<FLOAT,[1,1,518,518]> ⬅️ ::Relu(%\"conv2d_30\")\n",
       "            456 |  # node_squeeze\n",
       "                   %\"squeeze\"<FLOAT,[1,518,518]> ⬅️ ::Squeeze(%\"relu_16\", %\"val_396\"{[1]})\n",
       "            return %\"squeeze\"<FLOAT,[1,518,518]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_pretrained_cls_token: \"f32[1, 1, 384]\", p_pretrained_pos_embed: \"f32[1, 1370, 384]\", p_pretrained_mask_token: \"f32[1, 384]\", p_pretrained_patch_embed_proj_weight: \"f32[384, 3, 14, 14]\", p_pretrained_patch_embed_proj_bias: \"f32[384]\", p_pretrained_blocks_0_norm1_weight: \"f32[384]\", p_pretrained_blocks_0_norm1_bias: \"f32[384]\", p_pretrained_blocks_0_attn_qkv_weight: \"f32[1152, 384]\", p_pretrained_blocks_0_attn_qkv_bias: \"f32[1152]\", p_pretrained_blocks_0_attn_proj_weight: \"f32[384, 384]\", p_pretrained_blocks_0_attn_proj_bias: \"f32[384]\", p_pretrained_blocks_0_ls1_gamma: \"f32[384]\", p_pretrained_blocks_0_norm2_weight: \"f32[384]\", p_pretrained_blocks_0_norm2_bias: \"f32[384]\", p_pretrained_blocks_0_mlp_fc1_weight: \"f32[1536, 384]\", p_pretrained_blocks_0_mlp_fc1_bias: \"f32[1536]\", p_pretrained_blocks_0_mlp_fc2_weight: \"f32[384, 1536]\", p_pretrained_blocks_0_mlp_fc2_bias: \"f32[384]\", p_pretrained_blocks_0_ls2_gamma: \"f32[384]\", p_pretrained_blocks_1_norm1_weight: \"f32[384]\", p_pretrained_blocks_1_norm1_bias: \"f32[384]\", p_pretrained_blocks_1_attn_qkv_weight: \"f32[1152, 384]\", p_pretrained_blocks_1_attn_qkv_bias: \"f32[1152]\", p_pretrained_blocks_1_attn_proj_weight: \"f32[384, 384]\", p_pretrained_blocks_1_attn_proj_bias: \"f32[384]\", p_pretrained_blocks_1_ls1_gamma: \"f32[384]\", p_pretrained_blocks_1_norm2_weight: \"f32[384]\", p_pretrained_blocks_1_norm2_bias: \"f32[384]\", p_pretrained_blocks_1_mlp_fc1_weight: \"f32[1536, 384]\", p_pretrained_blocks_1_mlp_fc1_bias: \"f32[1536]\", p_pretrained_blocks_1_mlp_fc2_weight: \"f32[384, 1536]\", p_pretrained_blocks_1_mlp_fc2_bias: \"f32[384]\", p_pretrained_blocks_1_ls2_gamma: \"f32[384]\", p_pretrained_blocks_2_norm1_weight: \"f32[384]\", p_pretrained_blocks_2_norm1_bias: \"f32[384]\", p_pretrained_blocks_2_attn_qkv_weight: \"f32[1152, 384]\", p_pretrained_blocks_2_attn_qkv_bias: \"f32[1152]\", p_pretrained_blocks_2_attn_proj_weight: \"f32[384, 384]\", p_pretrained_blocks_2_attn_proj_bias: \"f32[384]\", p_pretrained_blocks_2_ls1_gamma: \"f32[384]\", p_pretrained_blocks_2_norm2_weight: \"f32[384]\", p_pretrained_blocks_2_norm2_bias: \"f32[384]\", p_pretrained_blocks_2_mlp_fc1_weight: \"f32[1536, 384]\", p_pretrained_blocks_2_mlp_fc1_bias: \"f32[1536]\", p_pretrained_blocks_2_mlp_fc2_weight: \"f32[384, 1536]\", p_pretrained_blocks_2_mlp_fc2_bias: \"f32[384]\", p_pretrained_blocks_2_ls2_gamma: \"f32[384]\", p_pretrained_blocks_3_norm1_weight: \"f32[384]\", p_pretrained_blocks_3_norm1_bias: \"f32[384]\", p_pretrained_blocks_3_attn_qkv_weight: \"f32[1152, 384]\", p_pretrained_blocks_3_attn_qkv_bias: \"f32[1152]\", p_pretrained_blocks_3_attn_proj_weight: \"f32[384, 384]\", p_pretrained_blocks_3_attn_proj_bias: \"f32[384]\", p_pretrained_blocks_3_ls1_gamma: \"f32[384]\", p_pretrained_blocks_3_norm2_weight: \"f32[384]\", p_pretrained_blocks_3_norm2_bias: \"f32[384]\", p_pretrained_blocks_3_mlp_fc1_weight: \"f32[1536, 384]\", p_pretrained_blocks_3_mlp_fc1_bias: \"f32[1536]\", p_pretrained_blocks_3_mlp_fc2_weight: \"f32[384, 1536]\", p_pretrained_blocks_3_mlp_fc2_bias: \"f32[384]\", p_pretrained_blocks_3_ls2_gamma: \"f32[384]\", p_pretrained_blocks_4_norm1_weight: \"f32[384]\", p_pretrained_blocks_4_norm1_bias: \"f32[384]\", p_pretrained_blocks_4_attn_qkv_weight: \"f32[1152, 384]\", p_pretrained_blocks_4_attn_qkv_bias: \"f32[1152]\", p_pretrained_blocks_4_attn_proj_weight: \"f32[384, 384]\", p_pretrained_blocks_4_attn_proj_bias: \"f32[384]\", p_pretrained_blocks_4_ls1_gamma: \"f32[384]\", p_pretrained_blocks_4_norm2_weight: \"f32[384]\", p_pretrained_blocks_4_norm2_bias: \"f32[384]\", p_pretrained_blocks_4_mlp_fc1_weight: \"f32[1536, 384]\", p_pretrained_blocks_4_mlp_fc1_bias: \"f32[1536]\", p_pretrained_blocks_4_mlp_fc2_weight: \"f32[384, 1536]\", p_pretrained_blocks_4_mlp_fc2_bias: \"f32[384]\", p_pretrained_blocks_4_ls2_gamma: \"f32[384]\", p_pretrained_blocks_5_norm1_weight: \"f32[384]\", p_pretrained_blocks_5_norm1_bias: \"f32[384]\", p_pretrained_blocks_5_attn_qkv_weight: \"f32[1152, 384]\", p_pretrained_blocks_5_attn_qkv_bias: \"f32[1152]\", p_pretrained_blocks_5_attn_proj_weight: \"f32[384, 384]\", p_pretrained_blocks_5_attn_proj_bias: \"f32[384]\", p_pretrained_blocks_5_ls1_gamma: \"f32[384]\", p_pretrained_blocks_5_norm2_weight: \"f32[384]\", p_pretrained_blocks_5_norm2_bias: \"f32[384]\", p_pretrained_blocks_5_mlp_fc1_weight: \"f32[1536, 384]\", p_pretrained_blocks_5_mlp_fc1_bias: \"f32[1536]\", p_pretrained_blocks_5_mlp_fc2_weight: \"f32[384, 1536]\", p_pretrained_blocks_5_mlp_fc2_bias: \"f32[384]\", p_pretrained_blocks_5_ls2_gamma: \"f32[384]\", p_pretrained_blocks_6_norm1_weight: \"f32[384]\", p_pretrained_blocks_6_norm1_bias: \"f32[384]\", p_pretrained_blocks_6_attn_qkv_weight: \"f32[1152, 384]\", p_pretrained_blocks_6_attn_qkv_bias: \"f32[1152]\", p_pretrained_blocks_6_attn_proj_weight: \"f32[384, 384]\", p_pretrained_blocks_6_attn_proj_bias: \"f32[384]\", p_pretrained_blocks_6_ls1_gamma: \"f32[384]\", p_pretrained_blocks_6_norm2_weight: \"f32[384]\", p_pretrained_blocks_6_norm2_bias: \"f32[384]\", p_pretrained_blocks_6_mlp_fc1_weight: \"f32[1536, 384]\", p_pretrained_blocks_6_mlp_fc1_bias: \"f32[1536]\", p_pretrained_blocks_6_mlp_fc2_weight: \"f32[384, 1536]\", p_pretrained_blocks_6_mlp_fc2_bias: \"f32[384]\", p_pretrained_blocks_6_ls2_gamma: \"f32[384]\", p_pretrained_blocks_7_norm1_weight: \"f32[384]\", p_pretrained_blocks_7_norm1_bias: \"f32[384]\", p_pretrained_blocks_7_attn_qkv_weight: \"f32[1152, 384]\", p_pretrained_blocks_7_attn_qkv_bias: \"f32[1152]\", p_pretrained_blocks_7_attn_proj_weight: \"f32[384, 384]\", p_pretrained_blocks_7_attn_proj_bias: \"f32[384]\", p_pretrained_blocks_7_ls1_gamma: \"f32[384]\", p_pretrained_blocks_7_norm2_weight: \"f32[384]\", p_pretrained_blocks_7_norm2_bias: \"f32[384]\", p_pretrained_blocks_7_mlp_fc1_weight: \"f32[1536, 384]\", p_pretrained_blocks_7_mlp_fc1_bias: \"f32[1536]\", p_pretrained_blocks_7_mlp_fc2_weight: \"f32[384, 1536]\", p_pretrained_blocks_7_mlp_fc2_bias: \"f32[384]\", p_pretrained_blocks_7_ls2_gamma: \"f32[384]\", p_pretrained_blocks_8_norm1_weight: \"f32[384]\", p_pretrained_blocks_8_norm1_bias: \"f32[384]\", p_pretrained_blocks_8_attn_qkv_weight: \"f32[1152, 384]\", p_pretrained_blocks_8_attn_qkv_bias: \"f32[1152]\", p_pretrained_blocks_8_attn_proj_weight: \"f32[384, 384]\", p_pretrained_blocks_8_attn_proj_bias: \"f32[384]\", p_pretrained_blocks_8_ls1_gamma: \"f32[384]\", p_pretrained_blocks_8_norm2_weight: \"f32[384]\", p_pretrained_blocks_8_norm2_bias: \"f32[384]\", p_pretrained_blocks_8_mlp_fc1_weight: \"f32[1536, 384]\", p_pretrained_blocks_8_mlp_fc1_bias: \"f32[1536]\", p_pretrained_blocks_8_mlp_fc2_weight: \"f32[384, 1536]\", p_pretrained_blocks_8_mlp_fc2_bias: \"f32[384]\", p_pretrained_blocks_8_ls2_gamma: \"f32[384]\", p_pretrained_blocks_9_norm1_weight: \"f32[384]\", p_pretrained_blocks_9_norm1_bias: \"f32[384]\", p_pretrained_blocks_9_attn_qkv_weight: \"f32[1152, 384]\", p_pretrained_blocks_9_attn_qkv_bias: \"f32[1152]\", p_pretrained_blocks_9_attn_proj_weight: \"f32[384, 384]\", p_pretrained_blocks_9_attn_proj_bias: \"f32[384]\", p_pretrained_blocks_9_ls1_gamma: \"f32[384]\", p_pretrained_blocks_9_norm2_weight: \"f32[384]\", p_pretrained_blocks_9_norm2_bias: \"f32[384]\", p_pretrained_blocks_9_mlp_fc1_weight: \"f32[1536, 384]\", p_pretrained_blocks_9_mlp_fc1_bias: \"f32[1536]\", p_pretrained_blocks_9_mlp_fc2_weight: \"f32[384, 1536]\", p_pretrained_blocks_9_mlp_fc2_bias: \"f32[384]\", p_pretrained_blocks_9_ls2_gamma: \"f32[384]\", p_pretrained_blocks_10_norm1_weight: \"f32[384]\", p_pretrained_blocks_10_norm1_bias: \"f32[384]\", p_pretrained_blocks_10_attn_qkv_weight: \"f32[1152, 384]\", p_pretrained_blocks_10_attn_qkv_bias: \"f32[1152]\", p_pretrained_blocks_10_attn_proj_weight: \"f32[384, 384]\", p_pretrained_blocks_10_attn_proj_bias: \"f32[384]\", p_pretrained_blocks_10_ls1_gamma: \"f32[384]\", p_pretrained_blocks_10_norm2_weight: \"f32[384]\", p_pretrained_blocks_10_norm2_bias: \"f32[384]\", p_pretrained_blocks_10_mlp_fc1_weight: \"f32[1536, 384]\", p_pretrained_blocks_10_mlp_fc1_bias: \"f32[1536]\", p_pretrained_blocks_10_mlp_fc2_weight: \"f32[384, 1536]\", p_pretrained_blocks_10_mlp_fc2_bias: \"f32[384]\", p_pretrained_blocks_10_ls2_gamma: \"f32[384]\", p_pretrained_blocks_11_norm1_weight: \"f32[384]\", p_pretrained_blocks_11_norm1_bias: \"f32[384]\", p_pretrained_blocks_11_attn_qkv_weight: \"f32[1152, 384]\", p_pretrained_blocks_11_attn_qkv_bias: \"f32[1152]\", p_pretrained_blocks_11_attn_proj_weight: \"f32[384, 384]\", p_pretrained_blocks_11_attn_proj_bias: \"f32[384]\", p_pretrained_blocks_11_ls1_gamma: \"f32[384]\", p_pretrained_blocks_11_norm2_weight: \"f32[384]\", p_pretrained_blocks_11_norm2_bias: \"f32[384]\", p_pretrained_blocks_11_mlp_fc1_weight: \"f32[1536, 384]\", p_pretrained_blocks_11_mlp_fc1_bias: \"f32[1536]\", p_pretrained_blocks_11_mlp_fc2_weight: \"f32[384, 1536]\", p_pretrained_blocks_11_mlp_fc2_bias: \"f32[384]\", p_pretrained_blocks_11_ls2_gamma: \"f32[384]\", p_pretrained_norm_weight: \"f32[384]\", p_pretrained_norm_bias: \"f32[384]\", p_depth_head_projects_0_weight: \"f32[48, 384, 1, 1]\", p_depth_head_projects_0_bias: \"f32[48]\", p_depth_head_projects_1_weight: \"f32[96, 384, 1, 1]\", p_depth_head_projects_1_bias: \"f32[96]\", p_depth_head_projects_2_weight: \"f32[192, 384, 1, 1]\", p_depth_head_projects_2_bias: \"f32[192]\", p_depth_head_projects_3_weight: \"f32[384, 384, 1, 1]\", p_depth_head_projects_3_bias: \"f32[384]\", p_depth_head_resize_layers_0_weight: \"f32[48, 48, 4, 4]\", p_depth_head_resize_layers_0_bias: \"f32[48]\", p_depth_head_resize_layers_1_weight: \"f32[96, 96, 2, 2]\", p_depth_head_resize_layers_1_bias: \"f32[96]\", p_depth_head_resize_layers_3_weight: \"f32[384, 384, 3, 3]\", p_depth_head_resize_layers_3_bias: \"f32[384]\", p_depth_head_scratch_layer1_rn_weight: \"f32[64, 48, 3, 3]\", p_depth_head_scratch_layer2_rn_weight: \"f32[64, 96, 3, 3]\", p_depth_head_scratch_layer3_rn_weight: \"f32[64, 192, 3, 3]\", p_depth_head_scratch_layer4_rn_weight: \"f32[64, 384, 3, 3]\", p_depth_head_scratch_refinenet1_out_conv_weight: \"f32[64, 64, 1, 1]\", p_depth_head_scratch_refinenet1_out_conv_bias: \"f32[64]\", p_depth_head_scratch_refinenet1_resconfunit1_conv1_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet1_resconfunit1_conv1_bias: \"f32[64]\", p_depth_head_scratch_refinenet1_resconfunit1_conv2_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet1_resconfunit1_conv2_bias: \"f32[64]\", p_depth_head_scratch_refinenet1_resconfunit2_conv1_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet1_resconfunit2_conv1_bias: \"f32[64]\", p_depth_head_scratch_refinenet1_resconfunit2_conv2_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet1_resconfunit2_conv2_bias: \"f32[64]\", p_depth_head_scratch_refinenet2_out_conv_weight: \"f32[64, 64, 1, 1]\", p_depth_head_scratch_refinenet2_out_conv_bias: \"f32[64]\", p_depth_head_scratch_refinenet2_resconfunit1_conv1_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet2_resconfunit1_conv1_bias: \"f32[64]\", p_depth_head_scratch_refinenet2_resconfunit1_conv2_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet2_resconfunit1_conv2_bias: \"f32[64]\", p_depth_head_scratch_refinenet2_resconfunit2_conv1_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet2_resconfunit2_conv1_bias: \"f32[64]\", p_depth_head_scratch_refinenet2_resconfunit2_conv2_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet2_resconfunit2_conv2_bias: \"f32[64]\", p_depth_head_scratch_refinenet3_out_conv_weight: \"f32[64, 64, 1, 1]\", p_depth_head_scratch_refinenet3_out_conv_bias: \"f32[64]\", p_depth_head_scratch_refinenet3_resconfunit1_conv1_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet3_resconfunit1_conv1_bias: \"f32[64]\", p_depth_head_scratch_refinenet3_resconfunit1_conv2_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet3_resconfunit1_conv2_bias: \"f32[64]\", p_depth_head_scratch_refinenet3_resconfunit2_conv1_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet3_resconfunit2_conv1_bias: \"f32[64]\", p_depth_head_scratch_refinenet3_resconfunit2_conv2_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet3_resconfunit2_conv2_bias: \"f32[64]\", p_depth_head_scratch_refinenet4_out_conv_weight: \"f32[64, 64, 1, 1]\", p_depth_head_scratch_refinenet4_out_conv_bias: \"f32[64]\", p_depth_head_scratch_refinenet4_resconfunit1_conv1_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet4_resconfunit1_conv1_bias: \"f32[64]\", p_depth_head_scratch_refinenet4_resconfunit1_conv2_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet4_resconfunit1_conv2_bias: \"f32[64]\", p_depth_head_scratch_refinenet4_resconfunit2_conv1_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet4_resconfunit2_conv1_bias: \"f32[64]\", p_depth_head_scratch_refinenet4_resconfunit2_conv2_weight: \"f32[64, 64, 3, 3]\", p_depth_head_scratch_refinenet4_resconfunit2_conv2_bias: \"f32[64]\", p_depth_head_scratch_output_conv1_weight: \"f32[32, 64, 3, 3]\", p_depth_head_scratch_output_conv1_bias: \"f32[32]\", p_depth_head_scratch_output_conv2_0_weight: \"f32[32, 32, 3, 3]\", p_depth_head_scratch_output_conv2_0_bias: \"f32[32]\", p_depth_head_scratch_output_conv2_2_weight: \"f32[1, 32, 1, 1]\", p_depth_head_scratch_output_conv2_2_bias: \"f32[1]\", x: \"f32[1, 3, 518, 518]\"):\n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d: \"f32[1, 384, 37, 37]\" = torch.ops.aten.conv2d.default(x, p_pretrained_patch_embed_proj_weight, p_pretrained_patch_embed_proj_bias, [14, 14]);  x = p_pretrained_patch_embed_proj_weight = p_pretrained_patch_embed_proj_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/patch_embed.py:78 in forward, code: x = x.flatten(2).transpose(1, 2)  # B HW C\n",
       "                    view: \"f32[1, 384, 1369]\" = torch.ops.aten.view.default(conv2d, [1, 384, 1369]);  conv2d = None\n",
       "                    transpose: \"f32[1, 1369, 384]\" = torch.ops.aten.transpose.int(view, 1, 2);  view = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dpt.py:179 in forward, code: features = self.pretrained.get_intermediate_layers(x, self.intermediate_layer_idx[self.encoder], return_class_token=True)\n",
       "                    expand: \"f32[1, 1, 384]\" = torch.ops.aten.expand.default(p_pretrained_cls_token, [1, -1, -1]);  p_pretrained_cls_token = None\n",
       "                    cat: \"f32[1, 1370, 384]\" = torch.ops.aten.cat.default([expand, transpose], 1);  expand = transpose = None\n",
       "                    add: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(cat, p_pretrained_pos_embed);  cat = p_pretrained_pos_embed = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add, [384], p_pretrained_blocks_0_norm1_weight, p_pretrained_blocks_0_norm1_bias, 1e-06);  p_pretrained_blocks_0_norm1_weight = p_pretrained_blocks_0_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear: \"f32[1, 1370, 1152]\" = torch.ops.aten.linear.default(layer_norm, p_pretrained_blocks_0_attn_qkv_weight, p_pretrained_blocks_0_attn_qkv_bias);  layer_norm = p_pretrained_blocks_0_attn_qkv_weight = p_pretrained_blocks_0_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:51 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
       "                    view_1: \"f32[1, 1370, 3, 6, 64]\" = torch.ops.aten.view.default(linear, [1, 1370, 3, 6, 64]);  linear = None\n",
       "                    permute: \"f32[3, 1, 6, 1370, 64]\" = torch.ops.aten.permute.default(view_1, [2, 0, 3, 1, 4]);  view_1 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:53 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n",
       "                    select: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute, 0, 0)\n",
       "                    mul: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.mul.Tensor(select, 0.125);  select = None\n",
       "                    select_1: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute, 0, 1)\n",
       "                    select_2: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute, 0, 2);  permute = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:54 in forward, code: attn = q @ k.transpose(-2, -1)\n",
       "                    transpose_1: \"f32[1, 6, 64, 1370]\" = torch.ops.aten.transpose.int(select_1, -2, -1);  select_1 = None\n",
       "                    matmul: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.matmul.default(mul, transpose_1);  mul = transpose_1 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:56 in forward, code: attn = attn.softmax(dim=-1)\n",
       "                    softmax: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.softmax.int(matmul, -1);  matmul = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.clone.default(softmax);  softmax = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:59 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
       "                    matmul_1: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.matmul.default(clone, select_2);  clone = select_2 = None\n",
       "                    transpose_2: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.transpose.int(matmul_1, 1, 2);  matmul_1 = None\n",
       "                    clone_1: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.clone.default(transpose_2, memory_format = torch.contiguous_format);  transpose_2 = None\n",
       "                    _unsafe_view: \"f32[1, 1370, 384]\" = torch.ops.aten._unsafe_view.default(clone_1, [1, 1370, 384]);  clone_1 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_1: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(_unsafe_view, p_pretrained_blocks_0_attn_proj_weight, p_pretrained_blocks_0_attn_proj_bias);  _unsafe_view = p_pretrained_blocks_0_attn_proj_weight = p_pretrained_blocks_0_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_2: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_1);  linear_1 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_1: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_2, p_pretrained_blocks_0_ls1_gamma);  clone_2 = p_pretrained_blocks_0_ls1_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:105 in forward, code: x = x + attn_residual_func(x)\n",
       "                    add_1: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add, mul_1);  add = mul_1 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_1: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_1, [384], p_pretrained_blocks_0_norm2_weight, p_pretrained_blocks_0_norm2_bias, 1e-06);  p_pretrained_blocks_0_norm2_weight = p_pretrained_blocks_0_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_2: \"f32[1, 1370, 1536]\" = torch.ops.aten.linear.default(layer_norm_1, p_pretrained_blocks_0_mlp_fc1_weight, p_pretrained_blocks_0_mlp_fc1_bias);  layer_norm_1 = p_pretrained_blocks_0_mlp_fc1_weight = p_pretrained_blocks_0_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu: \"f32[1, 1370, 1536]\" = torch.ops.aten.gelu.default(linear_2);  linear_2 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_3: \"f32[1, 1370, 1536]\" = torch.ops.aten.clone.default(gelu);  gelu = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_3: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(clone_3, p_pretrained_blocks_0_mlp_fc2_weight, p_pretrained_blocks_0_mlp_fc2_bias);  clone_3 = p_pretrained_blocks_0_mlp_fc2_weight = p_pretrained_blocks_0_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_4: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_3);  linear_3 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_2: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_4, p_pretrained_blocks_0_ls2_gamma);  clone_4 = p_pretrained_blocks_0_ls2_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:106 in forward, code: x = x + ffn_residual_func(x)\n",
       "                    add_2: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_1, mul_2);  add_1 = mul_2 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_2: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_2, [384], p_pretrained_blocks_1_norm1_weight, p_pretrained_blocks_1_norm1_bias, 1e-06);  p_pretrained_blocks_1_norm1_weight = p_pretrained_blocks_1_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_4: \"f32[1, 1370, 1152]\" = torch.ops.aten.linear.default(layer_norm_2, p_pretrained_blocks_1_attn_qkv_weight, p_pretrained_blocks_1_attn_qkv_bias);  layer_norm_2 = p_pretrained_blocks_1_attn_qkv_weight = p_pretrained_blocks_1_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:51 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
       "                    view_2: \"f32[1, 1370, 3, 6, 64]\" = torch.ops.aten.view.default(linear_4, [1, 1370, 3, 6, 64]);  linear_4 = None\n",
       "                    permute_1: \"f32[3, 1, 6, 1370, 64]\" = torch.ops.aten.permute.default(view_2, [2, 0, 3, 1, 4]);  view_2 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:53 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n",
       "                    select_3: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_1, 0, 0)\n",
       "                    mul_3: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.mul.Tensor(select_3, 0.125);  select_3 = None\n",
       "                    select_4: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_1, 0, 1)\n",
       "                    select_5: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_1, 0, 2);  permute_1 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:54 in forward, code: attn = q @ k.transpose(-2, -1)\n",
       "                    transpose_3: \"f32[1, 6, 64, 1370]\" = torch.ops.aten.transpose.int(select_4, -2, -1);  select_4 = None\n",
       "                    matmul_2: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.matmul.default(mul_3, transpose_3);  mul_3 = transpose_3 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:56 in forward, code: attn = attn.softmax(dim=-1)\n",
       "                    softmax_1: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.softmax.int(matmul_2, -1);  matmul_2 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_5: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.clone.default(softmax_1);  softmax_1 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:59 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
       "                    matmul_3: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.matmul.default(clone_5, select_5);  clone_5 = select_5 = None\n",
       "                    transpose_4: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.transpose.int(matmul_3, 1, 2);  matmul_3 = None\n",
       "                    clone_6: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.clone.default(transpose_4, memory_format = torch.contiguous_format);  transpose_4 = None\n",
       "                    _unsafe_view_1: \"f32[1, 1370, 384]\" = torch.ops.aten._unsafe_view.default(clone_6, [1, 1370, 384]);  clone_6 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_5: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(_unsafe_view_1, p_pretrained_blocks_1_attn_proj_weight, p_pretrained_blocks_1_attn_proj_bias);  _unsafe_view_1 = p_pretrained_blocks_1_attn_proj_weight = p_pretrained_blocks_1_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_7: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_5);  linear_5 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_4: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_7, p_pretrained_blocks_1_ls1_gamma);  clone_7 = p_pretrained_blocks_1_ls1_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:105 in forward, code: x = x + attn_residual_func(x)\n",
       "                    add_3: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_2, mul_4);  add_2 = mul_4 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_3: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_3, [384], p_pretrained_blocks_1_norm2_weight, p_pretrained_blocks_1_norm2_bias, 1e-06);  p_pretrained_blocks_1_norm2_weight = p_pretrained_blocks_1_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_6: \"f32[1, 1370, 1536]\" = torch.ops.aten.linear.default(layer_norm_3, p_pretrained_blocks_1_mlp_fc1_weight, p_pretrained_blocks_1_mlp_fc1_bias);  layer_norm_3 = p_pretrained_blocks_1_mlp_fc1_weight = p_pretrained_blocks_1_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_1: \"f32[1, 1370, 1536]\" = torch.ops.aten.gelu.default(linear_6);  linear_6 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_8: \"f32[1, 1370, 1536]\" = torch.ops.aten.clone.default(gelu_1);  gelu_1 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_7: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(clone_8, p_pretrained_blocks_1_mlp_fc2_weight, p_pretrained_blocks_1_mlp_fc2_bias);  clone_8 = p_pretrained_blocks_1_mlp_fc2_weight = p_pretrained_blocks_1_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_9: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_7);  linear_7 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_5: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_9, p_pretrained_blocks_1_ls2_gamma);  clone_9 = p_pretrained_blocks_1_ls2_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:106 in forward, code: x = x + ffn_residual_func(x)\n",
       "                    add_4: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_3, mul_5);  add_3 = mul_5 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_4: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_4, [384], p_pretrained_blocks_2_norm1_weight, p_pretrained_blocks_2_norm1_bias, 1e-06);  p_pretrained_blocks_2_norm1_weight = p_pretrained_blocks_2_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_8: \"f32[1, 1370, 1152]\" = torch.ops.aten.linear.default(layer_norm_4, p_pretrained_blocks_2_attn_qkv_weight, p_pretrained_blocks_2_attn_qkv_bias);  layer_norm_4 = p_pretrained_blocks_2_attn_qkv_weight = p_pretrained_blocks_2_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:51 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
       "                    view_3: \"f32[1, 1370, 3, 6, 64]\" = torch.ops.aten.view.default(linear_8, [1, 1370, 3, 6, 64]);  linear_8 = None\n",
       "                    permute_2: \"f32[3, 1, 6, 1370, 64]\" = torch.ops.aten.permute.default(view_3, [2, 0, 3, 1, 4]);  view_3 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:53 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n",
       "                    select_6: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_2, 0, 0)\n",
       "                    mul_6: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.mul.Tensor(select_6, 0.125);  select_6 = None\n",
       "                    select_7: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_2, 0, 1)\n",
       "                    select_8: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_2, 0, 2);  permute_2 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:54 in forward, code: attn = q @ k.transpose(-2, -1)\n",
       "                    transpose_5: \"f32[1, 6, 64, 1370]\" = torch.ops.aten.transpose.int(select_7, -2, -1);  select_7 = None\n",
       "                    matmul_4: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.matmul.default(mul_6, transpose_5);  mul_6 = transpose_5 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:56 in forward, code: attn = attn.softmax(dim=-1)\n",
       "                    softmax_2: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.softmax.int(matmul_4, -1);  matmul_4 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_10: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.clone.default(softmax_2);  softmax_2 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:59 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
       "                    matmul_5: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.matmul.default(clone_10, select_8);  clone_10 = select_8 = None\n",
       "                    transpose_6: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.transpose.int(matmul_5, 1, 2);  matmul_5 = None\n",
       "                    clone_11: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.clone.default(transpose_6, memory_format = torch.contiguous_format);  transpose_6 = None\n",
       "                    _unsafe_view_2: \"f32[1, 1370, 384]\" = torch.ops.aten._unsafe_view.default(clone_11, [1, 1370, 384]);  clone_11 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_9: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(_unsafe_view_2, p_pretrained_blocks_2_attn_proj_weight, p_pretrained_blocks_2_attn_proj_bias);  _unsafe_view_2 = p_pretrained_blocks_2_attn_proj_weight = p_pretrained_blocks_2_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_12: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_9);  linear_9 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_7: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_12, p_pretrained_blocks_2_ls1_gamma);  clone_12 = p_pretrained_blocks_2_ls1_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:105 in forward, code: x = x + attn_residual_func(x)\n",
       "                    add_5: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_4, mul_7);  add_4 = mul_7 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_5: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_5, [384], p_pretrained_blocks_2_norm2_weight, p_pretrained_blocks_2_norm2_bias, 1e-06);  p_pretrained_blocks_2_norm2_weight = p_pretrained_blocks_2_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_10: \"f32[1, 1370, 1536]\" = torch.ops.aten.linear.default(layer_norm_5, p_pretrained_blocks_2_mlp_fc1_weight, p_pretrained_blocks_2_mlp_fc1_bias);  layer_norm_5 = p_pretrained_blocks_2_mlp_fc1_weight = p_pretrained_blocks_2_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_2: \"f32[1, 1370, 1536]\" = torch.ops.aten.gelu.default(linear_10);  linear_10 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_13: \"f32[1, 1370, 1536]\" = torch.ops.aten.clone.default(gelu_2);  gelu_2 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_11: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(clone_13, p_pretrained_blocks_2_mlp_fc2_weight, p_pretrained_blocks_2_mlp_fc2_bias);  clone_13 = p_pretrained_blocks_2_mlp_fc2_weight = p_pretrained_blocks_2_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_14: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_11);  linear_11 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_8: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_14, p_pretrained_blocks_2_ls2_gamma);  clone_14 = p_pretrained_blocks_2_ls2_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:106 in forward, code: x = x + ffn_residual_func(x)\n",
       "                    add_6: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_5, mul_8);  add_5 = mul_8 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_6: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_6, [384], p_pretrained_blocks_3_norm1_weight, p_pretrained_blocks_3_norm1_bias, 1e-06);  p_pretrained_blocks_3_norm1_weight = p_pretrained_blocks_3_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_12: \"f32[1, 1370, 1152]\" = torch.ops.aten.linear.default(layer_norm_6, p_pretrained_blocks_3_attn_qkv_weight, p_pretrained_blocks_3_attn_qkv_bias);  layer_norm_6 = p_pretrained_blocks_3_attn_qkv_weight = p_pretrained_blocks_3_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:51 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
       "                    view_4: \"f32[1, 1370, 3, 6, 64]\" = torch.ops.aten.view.default(linear_12, [1, 1370, 3, 6, 64]);  linear_12 = None\n",
       "                    permute_3: \"f32[3, 1, 6, 1370, 64]\" = torch.ops.aten.permute.default(view_4, [2, 0, 3, 1, 4]);  view_4 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:53 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n",
       "                    select_9: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_3, 0, 0)\n",
       "                    mul_9: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.mul.Tensor(select_9, 0.125);  select_9 = None\n",
       "                    select_10: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_3, 0, 1)\n",
       "                    select_11: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_3, 0, 2);  permute_3 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:54 in forward, code: attn = q @ k.transpose(-2, -1)\n",
       "                    transpose_7: \"f32[1, 6, 64, 1370]\" = torch.ops.aten.transpose.int(select_10, -2, -1);  select_10 = None\n",
       "                    matmul_6: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.matmul.default(mul_9, transpose_7);  mul_9 = transpose_7 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:56 in forward, code: attn = attn.softmax(dim=-1)\n",
       "                    softmax_3: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.softmax.int(matmul_6, -1);  matmul_6 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_15: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.clone.default(softmax_3);  softmax_3 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:59 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
       "                    matmul_7: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.matmul.default(clone_15, select_11);  clone_15 = select_11 = None\n",
       "                    transpose_8: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.transpose.int(matmul_7, 1, 2);  matmul_7 = None\n",
       "                    clone_16: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.clone.default(transpose_8, memory_format = torch.contiguous_format);  transpose_8 = None\n",
       "                    _unsafe_view_3: \"f32[1, 1370, 384]\" = torch.ops.aten._unsafe_view.default(clone_16, [1, 1370, 384]);  clone_16 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_13: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(_unsafe_view_3, p_pretrained_blocks_3_attn_proj_weight, p_pretrained_blocks_3_attn_proj_bias);  _unsafe_view_3 = p_pretrained_blocks_3_attn_proj_weight = p_pretrained_blocks_3_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_17: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_13);  linear_13 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_10: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_17, p_pretrained_blocks_3_ls1_gamma);  clone_17 = p_pretrained_blocks_3_ls1_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:105 in forward, code: x = x + attn_residual_func(x)\n",
       "                    add_7: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_6, mul_10);  mul_10 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_7: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_7, [384], p_pretrained_blocks_3_norm2_weight, p_pretrained_blocks_3_norm2_bias, 1e-06);  p_pretrained_blocks_3_norm2_weight = p_pretrained_blocks_3_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_14: \"f32[1, 1370, 1536]\" = torch.ops.aten.linear.default(layer_norm_7, p_pretrained_blocks_3_mlp_fc1_weight, p_pretrained_blocks_3_mlp_fc1_bias);  layer_norm_7 = p_pretrained_blocks_3_mlp_fc1_weight = p_pretrained_blocks_3_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_3: \"f32[1, 1370, 1536]\" = torch.ops.aten.gelu.default(linear_14);  linear_14 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_18: \"f32[1, 1370, 1536]\" = torch.ops.aten.clone.default(gelu_3);  gelu_3 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_15: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(clone_18, p_pretrained_blocks_3_mlp_fc2_weight, p_pretrained_blocks_3_mlp_fc2_bias);  clone_18 = p_pretrained_blocks_3_mlp_fc2_weight = p_pretrained_blocks_3_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_19: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_15);  linear_15 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_11: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_19, p_pretrained_blocks_3_ls2_gamma);  clone_19 = p_pretrained_blocks_3_ls2_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:106 in forward, code: x = x + ffn_residual_func(x)\n",
       "                    add_8: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_7, mul_11);  add_7 = mul_11 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_8: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_8, [384], p_pretrained_blocks_4_norm1_weight, p_pretrained_blocks_4_norm1_bias, 1e-06);  p_pretrained_blocks_4_norm1_weight = p_pretrained_blocks_4_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_16: \"f32[1, 1370, 1152]\" = torch.ops.aten.linear.default(layer_norm_8, p_pretrained_blocks_4_attn_qkv_weight, p_pretrained_blocks_4_attn_qkv_bias);  layer_norm_8 = p_pretrained_blocks_4_attn_qkv_weight = p_pretrained_blocks_4_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:51 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
       "                    view_5: \"f32[1, 1370, 3, 6, 64]\" = torch.ops.aten.view.default(linear_16, [1, 1370, 3, 6, 64]);  linear_16 = None\n",
       "                    permute_4: \"f32[3, 1, 6, 1370, 64]\" = torch.ops.aten.permute.default(view_5, [2, 0, 3, 1, 4]);  view_5 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:53 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n",
       "                    select_12: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_4, 0, 0)\n",
       "                    mul_12: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.mul.Tensor(select_12, 0.125);  select_12 = None\n",
       "                    select_13: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_4, 0, 1)\n",
       "                    select_14: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_4, 0, 2);  permute_4 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:54 in forward, code: attn = q @ k.transpose(-2, -1)\n",
       "                    transpose_9: \"f32[1, 6, 64, 1370]\" = torch.ops.aten.transpose.int(select_13, -2, -1);  select_13 = None\n",
       "                    matmul_8: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.matmul.default(mul_12, transpose_9);  mul_12 = transpose_9 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:56 in forward, code: attn = attn.softmax(dim=-1)\n",
       "                    softmax_4: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.softmax.int(matmul_8, -1);  matmul_8 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_20: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.clone.default(softmax_4);  softmax_4 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:59 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
       "                    matmul_9: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.matmul.default(clone_20, select_14);  clone_20 = select_14 = None\n",
       "                    transpose_10: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.transpose.int(matmul_9, 1, 2);  matmul_9 = None\n",
       "                    clone_21: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.clone.default(transpose_10, memory_format = torch.contiguous_format);  transpose_10 = None\n",
       "                    _unsafe_view_4: \"f32[1, 1370, 384]\" = torch.ops.aten._unsafe_view.default(clone_21, [1, 1370, 384]);  clone_21 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_17: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(_unsafe_view_4, p_pretrained_blocks_4_attn_proj_weight, p_pretrained_blocks_4_attn_proj_bias);  _unsafe_view_4 = p_pretrained_blocks_4_attn_proj_weight = p_pretrained_blocks_4_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_22: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_17);  linear_17 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_13: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_22, p_pretrained_blocks_4_ls1_gamma);  clone_22 = p_pretrained_blocks_4_ls1_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:105 in forward, code: x = x + attn_residual_func(x)\n",
       "                    add_9: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_8, mul_13);  add_8 = mul_13 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_9: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_9, [384], p_pretrained_blocks_4_norm2_weight, p_pretrained_blocks_4_norm2_bias, 1e-06);  p_pretrained_blocks_4_norm2_weight = p_pretrained_blocks_4_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_18: \"f32[1, 1370, 1536]\" = torch.ops.aten.linear.default(layer_norm_9, p_pretrained_blocks_4_mlp_fc1_weight, p_pretrained_blocks_4_mlp_fc1_bias);  layer_norm_9 = p_pretrained_blocks_4_mlp_fc1_weight = p_pretrained_blocks_4_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_4: \"f32[1, 1370, 1536]\" = torch.ops.aten.gelu.default(linear_18);  linear_18 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_23: \"f32[1, 1370, 1536]\" = torch.ops.aten.clone.default(gelu_4);  gelu_4 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_19: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(clone_23, p_pretrained_blocks_4_mlp_fc2_weight, p_pretrained_blocks_4_mlp_fc2_bias);  clone_23 = p_pretrained_blocks_4_mlp_fc2_weight = p_pretrained_blocks_4_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_24: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_19);  linear_19 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_14: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_24, p_pretrained_blocks_4_ls2_gamma);  clone_24 = p_pretrained_blocks_4_ls2_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:106 in forward, code: x = x + ffn_residual_func(x)\n",
       "                    add_10: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_9, mul_14);  add_9 = mul_14 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_10: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_10, [384], p_pretrained_blocks_5_norm1_weight, p_pretrained_blocks_5_norm1_bias, 1e-06);  p_pretrained_blocks_5_norm1_weight = p_pretrained_blocks_5_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_20: \"f32[1, 1370, 1152]\" = torch.ops.aten.linear.default(layer_norm_10, p_pretrained_blocks_5_attn_qkv_weight, p_pretrained_blocks_5_attn_qkv_bias);  layer_norm_10 = p_pretrained_blocks_5_attn_qkv_weight = p_pretrained_blocks_5_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:51 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
       "                    view_6: \"f32[1, 1370, 3, 6, 64]\" = torch.ops.aten.view.default(linear_20, [1, 1370, 3, 6, 64]);  linear_20 = None\n",
       "                    permute_5: \"f32[3, 1, 6, 1370, 64]\" = torch.ops.aten.permute.default(view_6, [2, 0, 3, 1, 4]);  view_6 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:53 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n",
       "                    select_15: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_5, 0, 0)\n",
       "                    mul_15: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.mul.Tensor(select_15, 0.125);  select_15 = None\n",
       "                    select_16: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_5, 0, 1)\n",
       "                    select_17: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_5, 0, 2);  permute_5 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:54 in forward, code: attn = q @ k.transpose(-2, -1)\n",
       "                    transpose_11: \"f32[1, 6, 64, 1370]\" = torch.ops.aten.transpose.int(select_16, -2, -1);  select_16 = None\n",
       "                    matmul_10: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.matmul.default(mul_15, transpose_11);  mul_15 = transpose_11 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:56 in forward, code: attn = attn.softmax(dim=-1)\n",
       "                    softmax_5: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.softmax.int(matmul_10, -1);  matmul_10 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_25: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.clone.default(softmax_5);  softmax_5 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:59 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
       "                    matmul_11: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.matmul.default(clone_25, select_17);  clone_25 = select_17 = None\n",
       "                    transpose_12: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.transpose.int(matmul_11, 1, 2);  matmul_11 = None\n",
       "                    clone_26: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.clone.default(transpose_12, memory_format = torch.contiguous_format);  transpose_12 = None\n",
       "                    _unsafe_view_5: \"f32[1, 1370, 384]\" = torch.ops.aten._unsafe_view.default(clone_26, [1, 1370, 384]);  clone_26 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_21: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(_unsafe_view_5, p_pretrained_blocks_5_attn_proj_weight, p_pretrained_blocks_5_attn_proj_bias);  _unsafe_view_5 = p_pretrained_blocks_5_attn_proj_weight = p_pretrained_blocks_5_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_27: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_21);  linear_21 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_16: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_27, p_pretrained_blocks_5_ls1_gamma);  clone_27 = p_pretrained_blocks_5_ls1_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:105 in forward, code: x = x + attn_residual_func(x)\n",
       "                    add_11: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_10, mul_16);  add_10 = mul_16 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_11: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_11, [384], p_pretrained_blocks_5_norm2_weight, p_pretrained_blocks_5_norm2_bias, 1e-06);  p_pretrained_blocks_5_norm2_weight = p_pretrained_blocks_5_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_22: \"f32[1, 1370, 1536]\" = torch.ops.aten.linear.default(layer_norm_11, p_pretrained_blocks_5_mlp_fc1_weight, p_pretrained_blocks_5_mlp_fc1_bias);  layer_norm_11 = p_pretrained_blocks_5_mlp_fc1_weight = p_pretrained_blocks_5_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_5: \"f32[1, 1370, 1536]\" = torch.ops.aten.gelu.default(linear_22);  linear_22 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_28: \"f32[1, 1370, 1536]\" = torch.ops.aten.clone.default(gelu_5);  gelu_5 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_23: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(clone_28, p_pretrained_blocks_5_mlp_fc2_weight, p_pretrained_blocks_5_mlp_fc2_bias);  clone_28 = p_pretrained_blocks_5_mlp_fc2_weight = p_pretrained_blocks_5_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_29: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_23);  linear_23 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_17: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_29, p_pretrained_blocks_5_ls2_gamma);  clone_29 = p_pretrained_blocks_5_ls2_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:106 in forward, code: x = x + ffn_residual_func(x)\n",
       "                    add_12: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_11, mul_17);  add_11 = mul_17 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_12: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_12, [384], p_pretrained_blocks_6_norm1_weight, p_pretrained_blocks_6_norm1_bias, 1e-06);  p_pretrained_blocks_6_norm1_weight = p_pretrained_blocks_6_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_24: \"f32[1, 1370, 1152]\" = torch.ops.aten.linear.default(layer_norm_12, p_pretrained_blocks_6_attn_qkv_weight, p_pretrained_blocks_6_attn_qkv_bias);  layer_norm_12 = p_pretrained_blocks_6_attn_qkv_weight = p_pretrained_blocks_6_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:51 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
       "                    view_7: \"f32[1, 1370, 3, 6, 64]\" = torch.ops.aten.view.default(linear_24, [1, 1370, 3, 6, 64]);  linear_24 = None\n",
       "                    permute_6: \"f32[3, 1, 6, 1370, 64]\" = torch.ops.aten.permute.default(view_7, [2, 0, 3, 1, 4]);  view_7 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:53 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n",
       "                    select_18: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_6, 0, 0)\n",
       "                    mul_18: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.mul.Tensor(select_18, 0.125);  select_18 = None\n",
       "                    select_19: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_6, 0, 1)\n",
       "                    select_20: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_6, 0, 2);  permute_6 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:54 in forward, code: attn = q @ k.transpose(-2, -1)\n",
       "                    transpose_13: \"f32[1, 6, 64, 1370]\" = torch.ops.aten.transpose.int(select_19, -2, -1);  select_19 = None\n",
       "                    matmul_12: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.matmul.default(mul_18, transpose_13);  mul_18 = transpose_13 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:56 in forward, code: attn = attn.softmax(dim=-1)\n",
       "                    softmax_6: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.softmax.int(matmul_12, -1);  matmul_12 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_30: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.clone.default(softmax_6);  softmax_6 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:59 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
       "                    matmul_13: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.matmul.default(clone_30, select_20);  clone_30 = select_20 = None\n",
       "                    transpose_14: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.transpose.int(matmul_13, 1, 2);  matmul_13 = None\n",
       "                    clone_31: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.clone.default(transpose_14, memory_format = torch.contiguous_format);  transpose_14 = None\n",
       "                    _unsafe_view_6: \"f32[1, 1370, 384]\" = torch.ops.aten._unsafe_view.default(clone_31, [1, 1370, 384]);  clone_31 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_25: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(_unsafe_view_6, p_pretrained_blocks_6_attn_proj_weight, p_pretrained_blocks_6_attn_proj_bias);  _unsafe_view_6 = p_pretrained_blocks_6_attn_proj_weight = p_pretrained_blocks_6_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_32: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_25);  linear_25 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_19: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_32, p_pretrained_blocks_6_ls1_gamma);  clone_32 = p_pretrained_blocks_6_ls1_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:105 in forward, code: x = x + attn_residual_func(x)\n",
       "                    add_13: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_12, mul_19);  mul_19 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_13: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_13, [384], p_pretrained_blocks_6_norm2_weight, p_pretrained_blocks_6_norm2_bias, 1e-06);  p_pretrained_blocks_6_norm2_weight = p_pretrained_blocks_6_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_26: \"f32[1, 1370, 1536]\" = torch.ops.aten.linear.default(layer_norm_13, p_pretrained_blocks_6_mlp_fc1_weight, p_pretrained_blocks_6_mlp_fc1_bias);  layer_norm_13 = p_pretrained_blocks_6_mlp_fc1_weight = p_pretrained_blocks_6_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_6: \"f32[1, 1370, 1536]\" = torch.ops.aten.gelu.default(linear_26);  linear_26 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_33: \"f32[1, 1370, 1536]\" = torch.ops.aten.clone.default(gelu_6);  gelu_6 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_27: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(clone_33, p_pretrained_blocks_6_mlp_fc2_weight, p_pretrained_blocks_6_mlp_fc2_bias);  clone_33 = p_pretrained_blocks_6_mlp_fc2_weight = p_pretrained_blocks_6_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_34: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_27);  linear_27 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_20: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_34, p_pretrained_blocks_6_ls2_gamma);  clone_34 = p_pretrained_blocks_6_ls2_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:106 in forward, code: x = x + ffn_residual_func(x)\n",
       "                    add_14: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_13, mul_20);  add_13 = mul_20 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_14: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_14, [384], p_pretrained_blocks_7_norm1_weight, p_pretrained_blocks_7_norm1_bias, 1e-06);  p_pretrained_blocks_7_norm1_weight = p_pretrained_blocks_7_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_28: \"f32[1, 1370, 1152]\" = torch.ops.aten.linear.default(layer_norm_14, p_pretrained_blocks_7_attn_qkv_weight, p_pretrained_blocks_7_attn_qkv_bias);  layer_norm_14 = p_pretrained_blocks_7_attn_qkv_weight = p_pretrained_blocks_7_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:51 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
       "                    view_8: \"f32[1, 1370, 3, 6, 64]\" = torch.ops.aten.view.default(linear_28, [1, 1370, 3, 6, 64]);  linear_28 = None\n",
       "                    permute_7: \"f32[3, 1, 6, 1370, 64]\" = torch.ops.aten.permute.default(view_8, [2, 0, 3, 1, 4]);  view_8 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:53 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n",
       "                    select_21: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_7, 0, 0)\n",
       "                    mul_21: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.mul.Tensor(select_21, 0.125);  select_21 = None\n",
       "                    select_22: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_7, 0, 1)\n",
       "                    select_23: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_7, 0, 2);  permute_7 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:54 in forward, code: attn = q @ k.transpose(-2, -1)\n",
       "                    transpose_15: \"f32[1, 6, 64, 1370]\" = torch.ops.aten.transpose.int(select_22, -2, -1);  select_22 = None\n",
       "                    matmul_14: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.matmul.default(mul_21, transpose_15);  mul_21 = transpose_15 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:56 in forward, code: attn = attn.softmax(dim=-1)\n",
       "                    softmax_7: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.softmax.int(matmul_14, -1);  matmul_14 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_35: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.clone.default(softmax_7);  softmax_7 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:59 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
       "                    matmul_15: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.matmul.default(clone_35, select_23);  clone_35 = select_23 = None\n",
       "                    transpose_16: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.transpose.int(matmul_15, 1, 2);  matmul_15 = None\n",
       "                    clone_36: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.clone.default(transpose_16, memory_format = torch.contiguous_format);  transpose_16 = None\n",
       "                    _unsafe_view_7: \"f32[1, 1370, 384]\" = torch.ops.aten._unsafe_view.default(clone_36, [1, 1370, 384]);  clone_36 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_29: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(_unsafe_view_7, p_pretrained_blocks_7_attn_proj_weight, p_pretrained_blocks_7_attn_proj_bias);  _unsafe_view_7 = p_pretrained_blocks_7_attn_proj_weight = p_pretrained_blocks_7_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_37: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_29);  linear_29 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_22: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_37, p_pretrained_blocks_7_ls1_gamma);  clone_37 = p_pretrained_blocks_7_ls1_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:105 in forward, code: x = x + attn_residual_func(x)\n",
       "                    add_15: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_14, mul_22);  add_14 = mul_22 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_15: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_15, [384], p_pretrained_blocks_7_norm2_weight, p_pretrained_blocks_7_norm2_bias, 1e-06);  p_pretrained_blocks_7_norm2_weight = p_pretrained_blocks_7_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_30: \"f32[1, 1370, 1536]\" = torch.ops.aten.linear.default(layer_norm_15, p_pretrained_blocks_7_mlp_fc1_weight, p_pretrained_blocks_7_mlp_fc1_bias);  layer_norm_15 = p_pretrained_blocks_7_mlp_fc1_weight = p_pretrained_blocks_7_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_7: \"f32[1, 1370, 1536]\" = torch.ops.aten.gelu.default(linear_30);  linear_30 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_38: \"f32[1, 1370, 1536]\" = torch.ops.aten.clone.default(gelu_7);  gelu_7 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_31: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(clone_38, p_pretrained_blocks_7_mlp_fc2_weight, p_pretrained_blocks_7_mlp_fc2_bias);  clone_38 = p_pretrained_blocks_7_mlp_fc2_weight = p_pretrained_blocks_7_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_39: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_31);  linear_31 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_23: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_39, p_pretrained_blocks_7_ls2_gamma);  clone_39 = p_pretrained_blocks_7_ls2_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:106 in forward, code: x = x + ffn_residual_func(x)\n",
       "                    add_16: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_15, mul_23);  add_15 = mul_23 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_16: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_16, [384], p_pretrained_blocks_8_norm1_weight, p_pretrained_blocks_8_norm1_bias, 1e-06);  p_pretrained_blocks_8_norm1_weight = p_pretrained_blocks_8_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_32: \"f32[1, 1370, 1152]\" = torch.ops.aten.linear.default(layer_norm_16, p_pretrained_blocks_8_attn_qkv_weight, p_pretrained_blocks_8_attn_qkv_bias);  layer_norm_16 = p_pretrained_blocks_8_attn_qkv_weight = p_pretrained_blocks_8_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:51 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
       "                    view_9: \"f32[1, 1370, 3, 6, 64]\" = torch.ops.aten.view.default(linear_32, [1, 1370, 3, 6, 64]);  linear_32 = None\n",
       "                    permute_8: \"f32[3, 1, 6, 1370, 64]\" = torch.ops.aten.permute.default(view_9, [2, 0, 3, 1, 4]);  view_9 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:53 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n",
       "                    select_24: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_8, 0, 0)\n",
       "                    mul_24: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.mul.Tensor(select_24, 0.125);  select_24 = None\n",
       "                    select_25: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_8, 0, 1)\n",
       "                    select_26: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_8, 0, 2);  permute_8 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:54 in forward, code: attn = q @ k.transpose(-2, -1)\n",
       "                    transpose_17: \"f32[1, 6, 64, 1370]\" = torch.ops.aten.transpose.int(select_25, -2, -1);  select_25 = None\n",
       "                    matmul_16: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.matmul.default(mul_24, transpose_17);  mul_24 = transpose_17 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:56 in forward, code: attn = attn.softmax(dim=-1)\n",
       "                    softmax_8: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.softmax.int(matmul_16, -1);  matmul_16 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_40: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.clone.default(softmax_8);  softmax_8 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:59 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
       "                    matmul_17: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.matmul.default(clone_40, select_26);  clone_40 = select_26 = None\n",
       "                    transpose_18: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.transpose.int(matmul_17, 1, 2);  matmul_17 = None\n",
       "                    clone_41: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.clone.default(transpose_18, memory_format = torch.contiguous_format);  transpose_18 = None\n",
       "                    _unsafe_view_8: \"f32[1, 1370, 384]\" = torch.ops.aten._unsafe_view.default(clone_41, [1, 1370, 384]);  clone_41 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_33: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(_unsafe_view_8, p_pretrained_blocks_8_attn_proj_weight, p_pretrained_blocks_8_attn_proj_bias);  _unsafe_view_8 = p_pretrained_blocks_8_attn_proj_weight = p_pretrained_blocks_8_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_42: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_33);  linear_33 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_25: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_42, p_pretrained_blocks_8_ls1_gamma);  clone_42 = p_pretrained_blocks_8_ls1_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:105 in forward, code: x = x + attn_residual_func(x)\n",
       "                    add_17: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_16, mul_25);  add_16 = mul_25 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_17: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_17, [384], p_pretrained_blocks_8_norm2_weight, p_pretrained_blocks_8_norm2_bias, 1e-06);  p_pretrained_blocks_8_norm2_weight = p_pretrained_blocks_8_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_34: \"f32[1, 1370, 1536]\" = torch.ops.aten.linear.default(layer_norm_17, p_pretrained_blocks_8_mlp_fc1_weight, p_pretrained_blocks_8_mlp_fc1_bias);  layer_norm_17 = p_pretrained_blocks_8_mlp_fc1_weight = p_pretrained_blocks_8_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_8: \"f32[1, 1370, 1536]\" = torch.ops.aten.gelu.default(linear_34);  linear_34 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_43: \"f32[1, 1370, 1536]\" = torch.ops.aten.clone.default(gelu_8);  gelu_8 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_35: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(clone_43, p_pretrained_blocks_8_mlp_fc2_weight, p_pretrained_blocks_8_mlp_fc2_bias);  clone_43 = p_pretrained_blocks_8_mlp_fc2_weight = p_pretrained_blocks_8_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_44: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_35);  linear_35 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_26: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_44, p_pretrained_blocks_8_ls2_gamma);  clone_44 = p_pretrained_blocks_8_ls2_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:106 in forward, code: x = x + ffn_residual_func(x)\n",
       "                    add_18: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_17, mul_26);  add_17 = mul_26 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_18: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_18, [384], p_pretrained_blocks_9_norm1_weight, p_pretrained_blocks_9_norm1_bias, 1e-06);  p_pretrained_blocks_9_norm1_weight = p_pretrained_blocks_9_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_36: \"f32[1, 1370, 1152]\" = torch.ops.aten.linear.default(layer_norm_18, p_pretrained_blocks_9_attn_qkv_weight, p_pretrained_blocks_9_attn_qkv_bias);  layer_norm_18 = p_pretrained_blocks_9_attn_qkv_weight = p_pretrained_blocks_9_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:51 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
       "                    view_10: \"f32[1, 1370, 3, 6, 64]\" = torch.ops.aten.view.default(linear_36, [1, 1370, 3, 6, 64]);  linear_36 = None\n",
       "                    permute_9: \"f32[3, 1, 6, 1370, 64]\" = torch.ops.aten.permute.default(view_10, [2, 0, 3, 1, 4]);  view_10 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:53 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n",
       "                    select_27: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_9, 0, 0)\n",
       "                    mul_27: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.mul.Tensor(select_27, 0.125);  select_27 = None\n",
       "                    select_28: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_9, 0, 1)\n",
       "                    select_29: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_9, 0, 2);  permute_9 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:54 in forward, code: attn = q @ k.transpose(-2, -1)\n",
       "                    transpose_19: \"f32[1, 6, 64, 1370]\" = torch.ops.aten.transpose.int(select_28, -2, -1);  select_28 = None\n",
       "                    matmul_18: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.matmul.default(mul_27, transpose_19);  mul_27 = transpose_19 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:56 in forward, code: attn = attn.softmax(dim=-1)\n",
       "                    softmax_9: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.softmax.int(matmul_18, -1);  matmul_18 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_45: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.clone.default(softmax_9);  softmax_9 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:59 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
       "                    matmul_19: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.matmul.default(clone_45, select_29);  clone_45 = select_29 = None\n",
       "                    transpose_20: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.transpose.int(matmul_19, 1, 2);  matmul_19 = None\n",
       "                    clone_46: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.clone.default(transpose_20, memory_format = torch.contiguous_format);  transpose_20 = None\n",
       "                    _unsafe_view_9: \"f32[1, 1370, 384]\" = torch.ops.aten._unsafe_view.default(clone_46, [1, 1370, 384]);  clone_46 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_37: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(_unsafe_view_9, p_pretrained_blocks_9_attn_proj_weight, p_pretrained_blocks_9_attn_proj_bias);  _unsafe_view_9 = p_pretrained_blocks_9_attn_proj_weight = p_pretrained_blocks_9_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_47: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_37);  linear_37 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_28: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_47, p_pretrained_blocks_9_ls1_gamma);  clone_47 = p_pretrained_blocks_9_ls1_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:105 in forward, code: x = x + attn_residual_func(x)\n",
       "                    add_19: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_18, mul_28);  mul_28 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_19: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_19, [384], p_pretrained_blocks_9_norm2_weight, p_pretrained_blocks_9_norm2_bias, 1e-06);  p_pretrained_blocks_9_norm2_weight = p_pretrained_blocks_9_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_38: \"f32[1, 1370, 1536]\" = torch.ops.aten.linear.default(layer_norm_19, p_pretrained_blocks_9_mlp_fc1_weight, p_pretrained_blocks_9_mlp_fc1_bias);  layer_norm_19 = p_pretrained_blocks_9_mlp_fc1_weight = p_pretrained_blocks_9_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_9: \"f32[1, 1370, 1536]\" = torch.ops.aten.gelu.default(linear_38);  linear_38 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_48: \"f32[1, 1370, 1536]\" = torch.ops.aten.clone.default(gelu_9);  gelu_9 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_39: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(clone_48, p_pretrained_blocks_9_mlp_fc2_weight, p_pretrained_blocks_9_mlp_fc2_bias);  clone_48 = p_pretrained_blocks_9_mlp_fc2_weight = p_pretrained_blocks_9_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_49: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_39);  linear_39 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_29: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_49, p_pretrained_blocks_9_ls2_gamma);  clone_49 = p_pretrained_blocks_9_ls2_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:106 in forward, code: x = x + ffn_residual_func(x)\n",
       "                    add_20: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_19, mul_29);  add_19 = mul_29 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_20: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_20, [384], p_pretrained_blocks_10_norm1_weight, p_pretrained_blocks_10_norm1_bias, 1e-06);  p_pretrained_blocks_10_norm1_weight = p_pretrained_blocks_10_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_40: \"f32[1, 1370, 1152]\" = torch.ops.aten.linear.default(layer_norm_20, p_pretrained_blocks_10_attn_qkv_weight, p_pretrained_blocks_10_attn_qkv_bias);  layer_norm_20 = p_pretrained_blocks_10_attn_qkv_weight = p_pretrained_blocks_10_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:51 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
       "                    view_11: \"f32[1, 1370, 3, 6, 64]\" = torch.ops.aten.view.default(linear_40, [1, 1370, 3, 6, 64]);  linear_40 = None\n",
       "                    permute_10: \"f32[3, 1, 6, 1370, 64]\" = torch.ops.aten.permute.default(view_11, [2, 0, 3, 1, 4]);  view_11 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:53 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n",
       "                    select_30: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_10, 0, 0)\n",
       "                    mul_30: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.mul.Tensor(select_30, 0.125);  select_30 = None\n",
       "                    select_31: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_10, 0, 1)\n",
       "                    select_32: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_10, 0, 2);  permute_10 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:54 in forward, code: attn = q @ k.transpose(-2, -1)\n",
       "                    transpose_21: \"f32[1, 6, 64, 1370]\" = torch.ops.aten.transpose.int(select_31, -2, -1);  select_31 = None\n",
       "                    matmul_20: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.matmul.default(mul_30, transpose_21);  mul_30 = transpose_21 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:56 in forward, code: attn = attn.softmax(dim=-1)\n",
       "                    softmax_10: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.softmax.int(matmul_20, -1);  matmul_20 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_50: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.clone.default(softmax_10);  softmax_10 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:59 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
       "                    matmul_21: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.matmul.default(clone_50, select_32);  clone_50 = select_32 = None\n",
       "                    transpose_22: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.transpose.int(matmul_21, 1, 2);  matmul_21 = None\n",
       "                    clone_51: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.clone.default(transpose_22, memory_format = torch.contiguous_format);  transpose_22 = None\n",
       "                    _unsafe_view_10: \"f32[1, 1370, 384]\" = torch.ops.aten._unsafe_view.default(clone_51, [1, 1370, 384]);  clone_51 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_41: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(_unsafe_view_10, p_pretrained_blocks_10_attn_proj_weight, p_pretrained_blocks_10_attn_proj_bias);  _unsafe_view_10 = p_pretrained_blocks_10_attn_proj_weight = p_pretrained_blocks_10_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_52: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_41);  linear_41 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_31: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_52, p_pretrained_blocks_10_ls1_gamma);  clone_52 = p_pretrained_blocks_10_ls1_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:105 in forward, code: x = x + attn_residual_func(x)\n",
       "                    add_21: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_20, mul_31);  add_20 = mul_31 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_21: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_21, [384], p_pretrained_blocks_10_norm2_weight, p_pretrained_blocks_10_norm2_bias, 1e-06);  p_pretrained_blocks_10_norm2_weight = p_pretrained_blocks_10_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_42: \"f32[1, 1370, 1536]\" = torch.ops.aten.linear.default(layer_norm_21, p_pretrained_blocks_10_mlp_fc1_weight, p_pretrained_blocks_10_mlp_fc1_bias);  layer_norm_21 = p_pretrained_blocks_10_mlp_fc1_weight = p_pretrained_blocks_10_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_10: \"f32[1, 1370, 1536]\" = torch.ops.aten.gelu.default(linear_42);  linear_42 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_53: \"f32[1, 1370, 1536]\" = torch.ops.aten.clone.default(gelu_10);  gelu_10 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_43: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(clone_53, p_pretrained_blocks_10_mlp_fc2_weight, p_pretrained_blocks_10_mlp_fc2_bias);  clone_53 = p_pretrained_blocks_10_mlp_fc2_weight = p_pretrained_blocks_10_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_54: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_43);  linear_43 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_32: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_54, p_pretrained_blocks_10_ls2_gamma);  clone_54 = p_pretrained_blocks_10_ls2_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:106 in forward, code: x = x + ffn_residual_func(x)\n",
       "                    add_22: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_21, mul_32);  add_21 = mul_32 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_22: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_22, [384], p_pretrained_blocks_11_norm1_weight, p_pretrained_blocks_11_norm1_bias, 1e-06);  p_pretrained_blocks_11_norm1_weight = p_pretrained_blocks_11_norm1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_44: \"f32[1, 1370, 1152]\" = torch.ops.aten.linear.default(layer_norm_22, p_pretrained_blocks_11_attn_qkv_weight, p_pretrained_blocks_11_attn_qkv_bias);  layer_norm_22 = p_pretrained_blocks_11_attn_qkv_weight = p_pretrained_blocks_11_attn_qkv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:51 in forward, code: qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
       "                    view_12: \"f32[1, 1370, 3, 6, 64]\" = torch.ops.aten.view.default(linear_44, [1, 1370, 3, 6, 64]);  linear_44 = None\n",
       "                    permute_11: \"f32[3, 1, 6, 1370, 64]\" = torch.ops.aten.permute.default(view_12, [2, 0, 3, 1, 4]);  view_12 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:53 in forward, code: q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n",
       "                    select_33: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_11, 0, 0)\n",
       "                    mul_33: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.mul.Tensor(select_33, 0.125);  select_33 = None\n",
       "                    select_34: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_11, 0, 1)\n",
       "                    select_35: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.select.int(permute_11, 0, 2);  permute_11 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:54 in forward, code: attn = q @ k.transpose(-2, -1)\n",
       "                    transpose_23: \"f32[1, 6, 64, 1370]\" = torch.ops.aten.transpose.int(select_34, -2, -1);  select_34 = None\n",
       "                    matmul_22: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.matmul.default(mul_33, transpose_23);  mul_33 = transpose_23 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:56 in forward, code: attn = attn.softmax(dim=-1)\n",
       "                    softmax_11: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.softmax.int(matmul_22, -1);  matmul_22 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_55: \"f32[1, 6, 1370, 1370]\" = torch.ops.aten.clone.default(softmax_11);  softmax_11 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/attention.py:59 in forward, code: x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
       "                    matmul_23: \"f32[1, 6, 1370, 64]\" = torch.ops.aten.matmul.default(clone_55, select_35);  clone_55 = select_35 = None\n",
       "                    transpose_24: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.transpose.int(matmul_23, 1, 2);  matmul_23 = None\n",
       "                    clone_56: \"f32[1, 1370, 6, 64]\" = torch.ops.aten.clone.default(transpose_24, memory_format = torch.contiguous_format);  transpose_24 = None\n",
       "                    _unsafe_view_11: \"f32[1, 1370, 384]\" = torch.ops.aten._unsafe_view.default(clone_56, [1, 1370, 384]);  clone_56 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_45: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(_unsafe_view_11, p_pretrained_blocks_11_attn_proj_weight, p_pretrained_blocks_11_attn_proj_bias);  _unsafe_view_11 = p_pretrained_blocks_11_attn_proj_weight = p_pretrained_blocks_11_attn_proj_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_57: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_45);  linear_45 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_34: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_57, p_pretrained_blocks_11_ls1_gamma);  clone_57 = p_pretrained_blocks_11_ls1_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:105 in forward, code: x = x + attn_residual_func(x)\n",
       "                    add_23: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_22, mul_34);  add_22 = mul_34 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_23: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_23, [384], p_pretrained_blocks_11_norm2_weight, p_pretrained_blocks_11_norm2_bias, 1e-06);  p_pretrained_blocks_11_norm2_weight = p_pretrained_blocks_11_norm2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_46: \"f32[1, 1370, 1536]\" = torch.ops.aten.linear.default(layer_norm_23, p_pretrained_blocks_11_mlp_fc1_weight, p_pretrained_blocks_11_mlp_fc1_bias);  layer_norm_23 = p_pretrained_blocks_11_mlp_fc1_weight = p_pretrained_blocks_11_mlp_fc1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:816 in forward, code: return F.gelu(input, approximate=self.approximate)\n",
       "                    gelu_11: \"f32[1, 1370, 1536]\" = torch.ops.aten.gelu.default(linear_46);  linear_46 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_58: \"f32[1, 1370, 1536]\" = torch.ops.aten.clone.default(gelu_11);  gelu_11 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_47: \"f32[1, 1370, 384]\" = torch.ops.aten.linear.default(clone_58, p_pretrained_blocks_11_mlp_fc2_weight, p_pretrained_blocks_11_mlp_fc2_bias);  clone_58 = p_pretrained_blocks_11_mlp_fc2_weight = p_pretrained_blocks_11_mlp_fc2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_59: \"f32[1, 1370, 384]\" = torch.ops.aten.clone.default(linear_47);  linear_47 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/layer_scale.py:28 in forward, code: return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
       "                    mul_35: \"f32[1, 1370, 384]\" = torch.ops.aten.mul.Tensor(clone_59, p_pretrained_blocks_11_ls2_gamma);  clone_59 = p_pretrained_blocks_11_ls2_gamma = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dinov2_layers/block.py:106 in forward, code: x = x + ffn_residual_func(x)\n",
       "                    add_24: \"f32[1, 1370, 384]\" = torch.ops.aten.add.Tensor(add_23, mul_35);  add_23 = mul_35 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_24: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_6, [384], p_pretrained_norm_weight, p_pretrained_norm_bias, 1e-06);  add_6 = None\n",
       "                    layer_norm_25: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_12, [384], p_pretrained_norm_weight, p_pretrained_norm_bias, 1e-06);  add_12 = None\n",
       "                    layer_norm_26: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_18, [384], p_pretrained_norm_weight, p_pretrained_norm_bias, 1e-06);  add_18 = None\n",
       "                    layer_norm_27: \"f32[1, 1370, 384]\" = torch.ops.aten.layer_norm.default(add_24, [384], p_pretrained_norm_weight, p_pretrained_norm_bias, 1e-06);  add_24 = p_pretrained_norm_weight = p_pretrained_norm_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dpt.py:179 in forward, code: features = self.pretrained.get_intermediate_layers(x, self.intermediate_layer_idx[self.encoder], return_class_token=True)\n",
       "                    slice_1: \"f32[1, 1369, 384]\" = torch.ops.aten.slice.Tensor(layer_norm_24, 1, 1, 9223372036854775807);  layer_norm_24 = None\n",
       "                    slice_2: \"f32[1, 1369, 384]\" = torch.ops.aten.slice.Tensor(layer_norm_25, 1, 1, 9223372036854775807);  layer_norm_25 = None\n",
       "                    slice_3: \"f32[1, 1369, 384]\" = torch.ops.aten.slice.Tensor(layer_norm_26, 1, 1, 9223372036854775807);  layer_norm_26 = None\n",
       "                    slice_4: \"f32[1, 1369, 384]\" = torch.ops.aten.slice.Tensor(layer_norm_27, 1, 1, 9223372036854775807);  layer_norm_27 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dpt.py:127 in forward, code: x = x.permute(0, 2, 1).reshape((x.shape[0], x.shape[-1], patch_h, patch_w))\n",
       "                    permute_12: \"f32[1, 384, 1369]\" = torch.ops.aten.permute.default(slice_1, [0, 2, 1]);  slice_1 = None\n",
       "                    view_13: \"f32[1, 384, 37, 37]\" = torch.ops.aten.view.default(permute_12, [1, 384, 37, 37]);  permute_12 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_1: \"f32[1, 48, 37, 37]\" = torch.ops.aten.conv2d.default(view_13, p_depth_head_projects_0_weight, p_depth_head_projects_0_bias);  view_13 = p_depth_head_projects_0_weight = p_depth_head_projects_0_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:1161 in forward, code: return F.conv_transpose2d(\n",
       "                    convolution: \"f32[1, 48, 148, 148]\" = torch.ops.aten.convolution.default(conv2d_1, p_depth_head_resize_layers_0_weight, p_depth_head_resize_layers_0_bias, [4, 4], [0, 0], [1, 1], True, [0, 0], 1);  conv2d_1 = p_depth_head_resize_layers_0_weight = p_depth_head_resize_layers_0_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dpt.py:127 in forward, code: x = x.permute(0, 2, 1).reshape((x.shape[0], x.shape[-1], patch_h, patch_w))\n",
       "                    permute_13: \"f32[1, 384, 1369]\" = torch.ops.aten.permute.default(slice_2, [0, 2, 1]);  slice_2 = None\n",
       "                    view_14: \"f32[1, 384, 37, 37]\" = torch.ops.aten.view.default(permute_13, [1, 384, 37, 37]);  permute_13 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_2: \"f32[1, 96, 37, 37]\" = torch.ops.aten.conv2d.default(view_14, p_depth_head_projects_1_weight, p_depth_head_projects_1_bias);  view_14 = p_depth_head_projects_1_weight = p_depth_head_projects_1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:1161 in forward, code: return F.conv_transpose2d(\n",
       "                    convolution_1: \"f32[1, 96, 74, 74]\" = torch.ops.aten.convolution.default(conv2d_2, p_depth_head_resize_layers_1_weight, p_depth_head_resize_layers_1_bias, [2, 2], [0, 0], [1, 1], True, [0, 0], 1);  conv2d_2 = p_depth_head_resize_layers_1_weight = p_depth_head_resize_layers_1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dpt.py:127 in forward, code: x = x.permute(0, 2, 1).reshape((x.shape[0], x.shape[-1], patch_h, patch_w))\n",
       "                    permute_14: \"f32[1, 384, 1369]\" = torch.ops.aten.permute.default(slice_3, [0, 2, 1]);  slice_3 = None\n",
       "                    view_15: \"f32[1, 384, 37, 37]\" = torch.ops.aten.view.default(permute_14, [1, 384, 37, 37]);  permute_14 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_3: \"f32[1, 192, 37, 37]\" = torch.ops.aten.conv2d.default(view_15, p_depth_head_projects_2_weight, p_depth_head_projects_2_bias);  view_15 = p_depth_head_projects_2_weight = p_depth_head_projects_2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dpt.py:127 in forward, code: x = x.permute(0, 2, 1).reshape((x.shape[0], x.shape[-1], patch_h, patch_w))\n",
       "                    permute_15: \"f32[1, 384, 1369]\" = torch.ops.aten.permute.default(slice_4, [0, 2, 1]);  slice_4 = None\n",
       "                    view_16: \"f32[1, 384, 37, 37]\" = torch.ops.aten.view.default(permute_15, [1, 384, 37, 37]);  permute_15 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_4: \"f32[1, 384, 37, 37]\" = torch.ops.aten.conv2d.default(view_16, p_depth_head_projects_3_weight, p_depth_head_projects_3_bias);  view_16 = p_depth_head_projects_3_weight = p_depth_head_projects_3_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_5: \"f32[1, 384, 19, 19]\" = torch.ops.aten.conv2d.default(conv2d_4, p_depth_head_resize_layers_3_weight, p_depth_head_resize_layers_3_bias, [2, 2], [1, 1]);  conv2d_4 = p_depth_head_resize_layers_3_weight = p_depth_head_resize_layers_3_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_6: \"f32[1, 64, 148, 148]\" = torch.ops.aten.conv2d.default(convolution, p_depth_head_scratch_layer1_rn_weight, None, [1, 1], [1, 1]);  convolution = p_depth_head_scratch_layer1_rn_weight = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_7: \"f32[1, 64, 74, 74]\" = torch.ops.aten.conv2d.default(convolution_1, p_depth_head_scratch_layer2_rn_weight, None, [1, 1], [1, 1]);  convolution_1 = p_depth_head_scratch_layer2_rn_weight = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_8: \"f32[1, 64, 37, 37]\" = torch.ops.aten.conv2d.default(conv2d_3, p_depth_head_scratch_layer3_rn_weight, None, [1, 1], [1, 1]);  conv2d_3 = p_depth_head_scratch_layer3_rn_weight = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_9: \"f32[1, 64, 19, 19]\" = torch.ops.aten.conv2d.default(conv2d_5, p_depth_head_scratch_layer4_rn_weight, None, [1, 1], [1, 1]);  conv2d_5 = p_depth_head_scratch_layer4_rn_weight = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu: \"f32[1, 64, 19, 19]\" = torch.ops.aten.relu.default(conv2d_9)\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_10: \"f32[1, 64, 19, 19]\" = torch.ops.aten.conv2d.default(relu, p_depth_head_scratch_refinenet4_resconfunit2_conv1_weight, p_depth_head_scratch_refinenet4_resconfunit2_conv1_bias, [1, 1], [1, 1]);  relu = p_depth_head_scratch_refinenet4_resconfunit2_conv1_weight = p_depth_head_scratch_refinenet4_resconfunit2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_1: \"f32[1, 64, 19, 19]\" = torch.ops.aten.relu.default(conv2d_10);  conv2d_10 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_11: \"f32[1, 64, 19, 19]\" = torch.ops.aten.conv2d.default(relu_1, p_depth_head_scratch_refinenet4_resconfunit2_conv2_weight, p_depth_head_scratch_refinenet4_resconfunit2_conv2_bias, [1, 1], [1, 1]);  relu_1 = p_depth_head_scratch_refinenet4_resconfunit2_conv2_weight = p_depth_head_scratch_refinenet4_resconfunit2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/util/blocks.py:80 in forward, code: return self.skip_add.add(out, x)\n",
       "                    add_25: \"f32[1, 64, 19, 19]\" = torch.ops.aten.add.Tensor(conv2d_11, conv2d_9);  conv2d_11 = conv2d_9 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/util/blocks.py:144 in forward, code: output = nn.functional.interpolate(output, **modifier, mode=\"bilinear\", align_corners=self.align_corners)\n",
       "                    upsample_bilinear2d: \"f32[1, 64, 37, 37]\" = torch.ops.aten.upsample_bilinear2d.vec(add_25, [37, 37], True, None);  add_25 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_12: \"f32[1, 64, 37, 37]\" = torch.ops.aten.conv2d.default(upsample_bilinear2d, p_depth_head_scratch_refinenet4_out_conv_weight, p_depth_head_scratch_refinenet4_out_conv_bias);  upsample_bilinear2d = p_depth_head_scratch_refinenet4_out_conv_weight = p_depth_head_scratch_refinenet4_out_conv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_2: \"f32[1, 64, 37, 37]\" = torch.ops.aten.relu.default(conv2d_8)\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_13: \"f32[1, 64, 37, 37]\" = torch.ops.aten.conv2d.default(relu_2, p_depth_head_scratch_refinenet3_resconfunit1_conv1_weight, p_depth_head_scratch_refinenet3_resconfunit1_conv1_bias, [1, 1], [1, 1]);  relu_2 = p_depth_head_scratch_refinenet3_resconfunit1_conv1_weight = p_depth_head_scratch_refinenet3_resconfunit1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_3: \"f32[1, 64, 37, 37]\" = torch.ops.aten.relu.default(conv2d_13);  conv2d_13 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_14: \"f32[1, 64, 37, 37]\" = torch.ops.aten.conv2d.default(relu_3, p_depth_head_scratch_refinenet3_resconfunit1_conv2_weight, p_depth_head_scratch_refinenet3_resconfunit1_conv2_bias, [1, 1], [1, 1]);  relu_3 = p_depth_head_scratch_refinenet3_resconfunit1_conv2_weight = p_depth_head_scratch_refinenet3_resconfunit1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/util/blocks.py:80 in forward, code: return self.skip_add.add(out, x)\n",
       "                    add_26: \"f32[1, 64, 37, 37]\" = torch.ops.aten.add.Tensor(conv2d_14, conv2d_8);  conv2d_14 = conv2d_8 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/util/blocks.py:133 in forward, code: output = self.skip_add.add(output, res)\n",
       "                    add_27: \"f32[1, 64, 37, 37]\" = torch.ops.aten.add.Tensor(conv2d_12, add_26);  conv2d_12 = add_26 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_4: \"f32[1, 64, 37, 37]\" = torch.ops.aten.relu.default(add_27)\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_15: \"f32[1, 64, 37, 37]\" = torch.ops.aten.conv2d.default(relu_4, p_depth_head_scratch_refinenet3_resconfunit2_conv1_weight, p_depth_head_scratch_refinenet3_resconfunit2_conv1_bias, [1, 1], [1, 1]);  relu_4 = p_depth_head_scratch_refinenet3_resconfunit2_conv1_weight = p_depth_head_scratch_refinenet3_resconfunit2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_5: \"f32[1, 64, 37, 37]\" = torch.ops.aten.relu.default(conv2d_15);  conv2d_15 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_16: \"f32[1, 64, 37, 37]\" = torch.ops.aten.conv2d.default(relu_5, p_depth_head_scratch_refinenet3_resconfunit2_conv2_weight, p_depth_head_scratch_refinenet3_resconfunit2_conv2_bias, [1, 1], [1, 1]);  relu_5 = p_depth_head_scratch_refinenet3_resconfunit2_conv2_weight = p_depth_head_scratch_refinenet3_resconfunit2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/util/blocks.py:80 in forward, code: return self.skip_add.add(out, x)\n",
       "                    add_28: \"f32[1, 64, 37, 37]\" = torch.ops.aten.add.Tensor(conv2d_16, add_27);  conv2d_16 = add_27 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/util/blocks.py:144 in forward, code: output = nn.functional.interpolate(output, **modifier, mode=\"bilinear\", align_corners=self.align_corners)\n",
       "                    upsample_bilinear2d_1: \"f32[1, 64, 74, 74]\" = torch.ops.aten.upsample_bilinear2d.vec(add_28, [74, 74], True, None);  add_28 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_17: \"f32[1, 64, 74, 74]\" = torch.ops.aten.conv2d.default(upsample_bilinear2d_1, p_depth_head_scratch_refinenet3_out_conv_weight, p_depth_head_scratch_refinenet3_out_conv_bias);  upsample_bilinear2d_1 = p_depth_head_scratch_refinenet3_out_conv_weight = p_depth_head_scratch_refinenet3_out_conv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_6: \"f32[1, 64, 74, 74]\" = torch.ops.aten.relu.default(conv2d_7)\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_18: \"f32[1, 64, 74, 74]\" = torch.ops.aten.conv2d.default(relu_6, p_depth_head_scratch_refinenet2_resconfunit1_conv1_weight, p_depth_head_scratch_refinenet2_resconfunit1_conv1_bias, [1, 1], [1, 1]);  relu_6 = p_depth_head_scratch_refinenet2_resconfunit1_conv1_weight = p_depth_head_scratch_refinenet2_resconfunit1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_7: \"f32[1, 64, 74, 74]\" = torch.ops.aten.relu.default(conv2d_18);  conv2d_18 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_19: \"f32[1, 64, 74, 74]\" = torch.ops.aten.conv2d.default(relu_7, p_depth_head_scratch_refinenet2_resconfunit1_conv2_weight, p_depth_head_scratch_refinenet2_resconfunit1_conv2_bias, [1, 1], [1, 1]);  relu_7 = p_depth_head_scratch_refinenet2_resconfunit1_conv2_weight = p_depth_head_scratch_refinenet2_resconfunit1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/util/blocks.py:80 in forward, code: return self.skip_add.add(out, x)\n",
       "                    add_29: \"f32[1, 64, 74, 74]\" = torch.ops.aten.add.Tensor(conv2d_19, conv2d_7);  conv2d_19 = conv2d_7 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/util/blocks.py:133 in forward, code: output = self.skip_add.add(output, res)\n",
       "                    add_30: \"f32[1, 64, 74, 74]\" = torch.ops.aten.add.Tensor(conv2d_17, add_29);  conv2d_17 = add_29 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_8: \"f32[1, 64, 74, 74]\" = torch.ops.aten.relu.default(add_30)\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_20: \"f32[1, 64, 74, 74]\" = torch.ops.aten.conv2d.default(relu_8, p_depth_head_scratch_refinenet2_resconfunit2_conv1_weight, p_depth_head_scratch_refinenet2_resconfunit2_conv1_bias, [1, 1], [1, 1]);  relu_8 = p_depth_head_scratch_refinenet2_resconfunit2_conv1_weight = p_depth_head_scratch_refinenet2_resconfunit2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_9: \"f32[1, 64, 74, 74]\" = torch.ops.aten.relu.default(conv2d_20);  conv2d_20 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_21: \"f32[1, 64, 74, 74]\" = torch.ops.aten.conv2d.default(relu_9, p_depth_head_scratch_refinenet2_resconfunit2_conv2_weight, p_depth_head_scratch_refinenet2_resconfunit2_conv2_bias, [1, 1], [1, 1]);  relu_9 = p_depth_head_scratch_refinenet2_resconfunit2_conv2_weight = p_depth_head_scratch_refinenet2_resconfunit2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/util/blocks.py:80 in forward, code: return self.skip_add.add(out, x)\n",
       "                    add_31: \"f32[1, 64, 74, 74]\" = torch.ops.aten.add.Tensor(conv2d_21, add_30);  conv2d_21 = add_30 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/util/blocks.py:144 in forward, code: output = nn.functional.interpolate(output, **modifier, mode=\"bilinear\", align_corners=self.align_corners)\n",
       "                    upsample_bilinear2d_2: \"f32[1, 64, 148, 148]\" = torch.ops.aten.upsample_bilinear2d.vec(add_31, [148, 148], True, None);  add_31 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_22: \"f32[1, 64, 148, 148]\" = torch.ops.aten.conv2d.default(upsample_bilinear2d_2, p_depth_head_scratch_refinenet2_out_conv_weight, p_depth_head_scratch_refinenet2_out_conv_bias);  upsample_bilinear2d_2 = p_depth_head_scratch_refinenet2_out_conv_weight = p_depth_head_scratch_refinenet2_out_conv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_10: \"f32[1, 64, 148, 148]\" = torch.ops.aten.relu.default(conv2d_6)\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_23: \"f32[1, 64, 148, 148]\" = torch.ops.aten.conv2d.default(relu_10, p_depth_head_scratch_refinenet1_resconfunit1_conv1_weight, p_depth_head_scratch_refinenet1_resconfunit1_conv1_bias, [1, 1], [1, 1]);  relu_10 = p_depth_head_scratch_refinenet1_resconfunit1_conv1_weight = p_depth_head_scratch_refinenet1_resconfunit1_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_11: \"f32[1, 64, 148, 148]\" = torch.ops.aten.relu.default(conv2d_23);  conv2d_23 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_24: \"f32[1, 64, 148, 148]\" = torch.ops.aten.conv2d.default(relu_11, p_depth_head_scratch_refinenet1_resconfunit1_conv2_weight, p_depth_head_scratch_refinenet1_resconfunit1_conv2_bias, [1, 1], [1, 1]);  relu_11 = p_depth_head_scratch_refinenet1_resconfunit1_conv2_weight = p_depth_head_scratch_refinenet1_resconfunit1_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/util/blocks.py:80 in forward, code: return self.skip_add.add(out, x)\n",
       "                    add_32: \"f32[1, 64, 148, 148]\" = torch.ops.aten.add.Tensor(conv2d_24, conv2d_6);  conv2d_24 = conv2d_6 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/util/blocks.py:133 in forward, code: output = self.skip_add.add(output, res)\n",
       "                    add_33: \"f32[1, 64, 148, 148]\" = torch.ops.aten.add.Tensor(conv2d_22, add_32);  conv2d_22 = add_32 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_12: \"f32[1, 64, 148, 148]\" = torch.ops.aten.relu.default(add_33)\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_25: \"f32[1, 64, 148, 148]\" = torch.ops.aten.conv2d.default(relu_12, p_depth_head_scratch_refinenet1_resconfunit2_conv1_weight, p_depth_head_scratch_refinenet1_resconfunit2_conv1_bias, [1, 1], [1, 1]);  relu_12 = p_depth_head_scratch_refinenet1_resconfunit2_conv1_weight = p_depth_head_scratch_refinenet1_resconfunit2_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_13: \"f32[1, 64, 148, 148]\" = torch.ops.aten.relu.default(conv2d_25);  conv2d_25 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_26: \"f32[1, 64, 148, 148]\" = torch.ops.aten.conv2d.default(relu_13, p_depth_head_scratch_refinenet1_resconfunit2_conv2_weight, p_depth_head_scratch_refinenet1_resconfunit2_conv2_bias, [1, 1], [1, 1]);  relu_13 = p_depth_head_scratch_refinenet1_resconfunit2_conv2_weight = p_depth_head_scratch_refinenet1_resconfunit2_conv2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/util/blocks.py:80 in forward, code: return self.skip_add.add(out, x)\n",
       "                    add_34: \"f32[1, 64, 148, 148]\" = torch.ops.aten.add.Tensor(conv2d_26, add_33);  conv2d_26 = add_33 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/util/blocks.py:144 in forward, code: output = nn.functional.interpolate(output, **modifier, mode=\"bilinear\", align_corners=self.align_corners)\n",
       "                    upsample_bilinear2d_3: \"f32[1, 64, 296, 296]\" = torch.ops.aten.upsample_bilinear2d.vec(add_34, None, True, [2.0, 2.0]);  add_34 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_27: \"f32[1, 64, 296, 296]\" = torch.ops.aten.conv2d.default(upsample_bilinear2d_3, p_depth_head_scratch_refinenet1_out_conv_weight, p_depth_head_scratch_refinenet1_out_conv_bias);  upsample_bilinear2d_3 = p_depth_head_scratch_refinenet1_out_conv_weight = p_depth_head_scratch_refinenet1_out_conv_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_28: \"f32[1, 32, 296, 296]\" = torch.ops.aten.conv2d.default(conv2d_27, p_depth_head_scratch_output_conv1_weight, p_depth_head_scratch_output_conv1_bias, [1, 1], [1, 1]);  conv2d_27 = p_depth_head_scratch_output_conv1_weight = p_depth_head_scratch_output_conv1_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dpt.py:147 in forward, code: out = F.interpolate(out, (int(patch_h * 14), int(patch_w * 14)), mode=\"bilinear\", align_corners=True)\n",
       "                    upsample_bilinear2d_4: \"f32[1, 32, 518, 518]\" = torch.ops.aten.upsample_bilinear2d.vec(conv2d_28, [518, 518], True, None);  conv2d_28 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_29: \"f32[1, 32, 518, 518]\" = torch.ops.aten.conv2d.default(upsample_bilinear2d_4, p_depth_head_scratch_output_conv2_0_weight, p_depth_head_scratch_output_conv2_0_bias, [1, 1], [1, 1]);  upsample_bilinear2d_4 = p_depth_head_scratch_output_conv2_0_weight = p_depth_head_scratch_output_conv2_0_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_14: \"f32[1, 32, 518, 518]\" = torch.ops.aten.relu.default(conv2d_29);  conv2d_29 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_30: \"f32[1, 1, 518, 518]\" = torch.ops.aten.conv2d.default(relu_14, p_depth_head_scratch_output_conv2_2_weight, p_depth_head_scratch_output_conv2_2_bias);  relu_14 = p_depth_head_scratch_output_conv2_2_weight = p_depth_head_scratch_output_conv2_2_bias = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_15: \"f32[1, 1, 518, 518]\" = torch.ops.aten.relu.default(conv2d_30);  conv2d_30 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dpt.py:182 in forward, code: depth = F.relu(depth)\n",
       "                    relu_16: \"f32[1, 1, 518, 518]\" = torch.ops.aten.relu.default(relu_15);  relu_15 = None\n",
       "            \n",
       "                     # File: /home/max/Documents/VisualComputing/exercises/project/ar-placement/depth_tracking/Depth-Anything-V2/depth_anything_v2/dpt.py:184 in forward, code: return depth.squeeze(1)\n",
       "                    squeeze: \"f32[1, 518, 518]\" = torch.ops.aten.squeeze.dim(relu_16, 1);  relu_16 = None\n",
       "                    return (squeeze,)\n",
       "            \n",
       "        Graph signature: \n",
       "            # inputs\n",
       "            p_pretrained_cls_token: PARAMETER target='pretrained.cls_token'\n",
       "            p_pretrained_pos_embed: PARAMETER target='pretrained.pos_embed'\n",
       "            p_pretrained_mask_token: PARAMETER target='pretrained.mask_token'\n",
       "            p_pretrained_patch_embed_proj_weight: PARAMETER target='pretrained.patch_embed.proj.weight'\n",
       "            p_pretrained_patch_embed_proj_bias: PARAMETER target='pretrained.patch_embed.proj.bias'\n",
       "            p_pretrained_blocks_0_norm1_weight: PARAMETER target='pretrained.blocks.0.norm1.weight'\n",
       "            p_pretrained_blocks_0_norm1_bias: PARAMETER target='pretrained.blocks.0.norm1.bias'\n",
       "            p_pretrained_blocks_0_attn_qkv_weight: PARAMETER target='pretrained.blocks.0.attn.qkv.weight'\n",
       "            p_pretrained_blocks_0_attn_qkv_bias: PARAMETER target='pretrained.blocks.0.attn.qkv.bias'\n",
       "            p_pretrained_blocks_0_attn_proj_weight: PARAMETER target='pretrained.blocks.0.attn.proj.weight'\n",
       "            p_pretrained_blocks_0_attn_proj_bias: PARAMETER target='pretrained.blocks.0.attn.proj.bias'\n",
       "            p_pretrained_blocks_0_ls1_gamma: PARAMETER target='pretrained.blocks.0.ls1.gamma'\n",
       "            p_pretrained_blocks_0_norm2_weight: PARAMETER target='pretrained.blocks.0.norm2.weight'\n",
       "            p_pretrained_blocks_0_norm2_bias: PARAMETER target='pretrained.blocks.0.norm2.bias'\n",
       "            p_pretrained_blocks_0_mlp_fc1_weight: PARAMETER target='pretrained.blocks.0.mlp.fc1.weight'\n",
       "            p_pretrained_blocks_0_mlp_fc1_bias: PARAMETER target='pretrained.blocks.0.mlp.fc1.bias'\n",
       "            p_pretrained_blocks_0_mlp_fc2_weight: PARAMETER target='pretrained.blocks.0.mlp.fc2.weight'\n",
       "            p_pretrained_blocks_0_mlp_fc2_bias: PARAMETER target='pretrained.blocks.0.mlp.fc2.bias'\n",
       "            p_pretrained_blocks_0_ls2_gamma: PARAMETER target='pretrained.blocks.0.ls2.gamma'\n",
       "            p_pretrained_blocks_1_norm1_weight: PARAMETER target='pretrained.blocks.1.norm1.weight'\n",
       "            p_pretrained_blocks_1_norm1_bias: PARAMETER target='pretrained.blocks.1.norm1.bias'\n",
       "            p_pretrained_blocks_1_attn_qkv_weight: PARAMETER target='pretrained.blocks.1.attn.qkv.weight'\n",
       "            p_pretrained_blocks_1_attn_qkv_bias: PARAMETER target='pretrained.blocks.1.attn.qkv.bias'\n",
       "            p_pretrained_blocks_1_attn_proj_weight: PARAMETER target='pretrained.blocks.1.attn.proj.weight'\n",
       "            p_pretrained_blocks_1_attn_proj_bias: PARAMETER target='pretrained.blocks.1.attn.proj.bias'\n",
       "            p_pretrained_blocks_1_ls1_gamma: PARAMETER target='pretrained.blocks.1.ls1.gamma'\n",
       "            p_pretrained_blocks_1_norm2_weight: PARAMETER target='pretrained.blocks.1.norm2.weight'\n",
       "            p_pretrained_blocks_1_norm2_bias: PARAMETER target='pretrained.blocks.1.norm2.bias'\n",
       "            p_pretrained_blocks_1_mlp_fc1_weight: PARAMETER target='pretrained.blocks.1.mlp.fc1.weight'\n",
       "            p_pretrained_blocks_1_mlp_fc1_bias: PARAMETER target='pretrained.blocks.1.mlp.fc1.bias'\n",
       "            p_pretrained_blocks_1_mlp_fc2_weight: PARAMETER target='pretrained.blocks.1.mlp.fc2.weight'\n",
       "            p_pretrained_blocks_1_mlp_fc2_bias: PARAMETER target='pretrained.blocks.1.mlp.fc2.bias'\n",
       "            p_pretrained_blocks_1_ls2_gamma: PARAMETER target='pretrained.blocks.1.ls2.gamma'\n",
       "            p_pretrained_blocks_2_norm1_weight: PARAMETER target='pretrained.blocks.2.norm1.weight'\n",
       "            p_pretrained_blocks_2_norm1_bias: PARAMETER target='pretrained.blocks.2.norm1.bias'\n",
       "            p_pretrained_blocks_2_attn_qkv_weight: PARAMETER target='pretrained.blocks.2.attn.qkv.weight'\n",
       "            p_pretrained_blocks_2_attn_qkv_bias: PARAMETER target='pretrained.blocks.2.attn.qkv.bias'\n",
       "            p_pretrained_blocks_2_attn_proj_weight: PARAMETER target='pretrained.blocks.2.attn.proj.weight'\n",
       "            p_pretrained_blocks_2_attn_proj_bias: PARAMETER target='pretrained.blocks.2.attn.proj.bias'\n",
       "            p_pretrained_blocks_2_ls1_gamma: PARAMETER target='pretrained.blocks.2.ls1.gamma'\n",
       "            p_pretrained_blocks_2_norm2_weight: PARAMETER target='pretrained.blocks.2.norm2.weight'\n",
       "            p_pretrained_blocks_2_norm2_bias: PARAMETER target='pretrained.blocks.2.norm2.bias'\n",
       "            p_pretrained_blocks_2_mlp_fc1_weight: PARAMETER target='pretrained.blocks.2.mlp.fc1.weight'\n",
       "            p_pretrained_blocks_2_mlp_fc1_bias: PARAMETER target='pretrained.blocks.2.mlp.fc1.bias'\n",
       "            p_pretrained_blocks_2_mlp_fc2_weight: PARAMETER target='pretrained.blocks.2.mlp.fc2.weight'\n",
       "            p_pretrained_blocks_2_mlp_fc2_bias: PARAMETER target='pretrained.blocks.2.mlp.fc2.bias'\n",
       "            p_pretrained_blocks_2_ls2_gamma: PARAMETER target='pretrained.blocks.2.ls2.gamma'\n",
       "            p_pretrained_blocks_3_norm1_weight: PARAMETER target='pretrained.blocks.3.norm1.weight'\n",
       "            p_pretrained_blocks_3_norm1_bias: PARAMETER target='pretrained.blocks.3.norm1.bias'\n",
       "            p_pretrained_blocks_3_attn_qkv_weight: PARAMETER target='pretrained.blocks.3.attn.qkv.weight'\n",
       "            p_pretrained_blocks_3_attn_qkv_bias: PARAMETER target='pretrained.blocks.3.attn.qkv.bias'\n",
       "            p_pretrained_blocks_3_attn_proj_weight: PARAMETER target='pretrained.blocks.3.attn.proj.weight'\n",
       "            p_pretrained_blocks_3_attn_proj_bias: PARAMETER target='pretrained.blocks.3.attn.proj.bias'\n",
       "            p_pretrained_blocks_3_ls1_gamma: PARAMETER target='pretrained.blocks.3.ls1.gamma'\n",
       "            p_pretrained_blocks_3_norm2_weight: PARAMETER target='pretrained.blocks.3.norm2.weight'\n",
       "            p_pretrained_blocks_3_norm2_bias: PARAMETER target='pretrained.blocks.3.norm2.bias'\n",
       "            p_pretrained_blocks_3_mlp_fc1_weight: PARAMETER target='pretrained.blocks.3.mlp.fc1.weight'\n",
       "            p_pretrained_blocks_3_mlp_fc1_bias: PARAMETER target='pretrained.blocks.3.mlp.fc1.bias'\n",
       "            p_pretrained_blocks_3_mlp_fc2_weight: PARAMETER target='pretrained.blocks.3.mlp.fc2.weight'\n",
       "            p_pretrained_blocks_3_mlp_fc2_bias: PARAMETER target='pretrained.blocks.3.mlp.fc2.bias'\n",
       "            p_pretrained_blocks_3_ls2_gamma: PARAMETER target='pretrained.blocks.3.ls2.gamma'\n",
       "            p_pretrained_blocks_4_norm1_weight: PARAMETER target='pretrained.blocks.4.norm1.weight'\n",
       "            p_pretrained_blocks_4_norm1_bias: PARAMETER target='pretrained.blocks.4.norm1.bias'\n",
       "            p_pretrained_blocks_4_attn_qkv_weight: PARAMETER target='pretrained.blocks.4.attn.qkv.weight'\n",
       "            p_pretrained_blocks_4_attn_qkv_bias: PARAMETER target='pretrained.blocks.4.attn.qkv.bias'\n",
       "            p_pretrained_blocks_4_attn_proj_weight: PARAMETER target='pretrained.blocks.4.attn.proj.weight'\n",
       "            p_pretrained_blocks_4_attn_proj_bias: PARAMETER target='pretrained.blocks.4.attn.proj.bias'\n",
       "            p_pretrained_blocks_4_ls1_gamma: PARAMETER target='pretrained.blocks.4.ls1.gamma'\n",
       "            p_pretrained_blocks_4_norm2_weight: PARAMETER target='pretrained.blocks.4.norm2.weight'\n",
       "            p_pretrained_blocks_4_norm2_bias: PARAMETER target='pretrained.blocks.4.norm2.bias'\n",
       "            p_pretrained_blocks_4_mlp_fc1_weight: PARAMETER target='pretrained.blocks.4.mlp.fc1.weight'\n",
       "            p_pretrained_blocks_4_mlp_fc1_bias: PARAMETER target='pretrained.blocks.4.mlp.fc1.bias'\n",
       "            p_pretrained_blocks_4_mlp_fc2_weight: PARAMETER target='pretrained.blocks.4.mlp.fc2.weight'\n",
       "            p_pretrained_blocks_4_mlp_fc2_bias: PARAMETER target='pretrained.blocks.4.mlp.fc2.bias'\n",
       "            p_pretrained_blocks_4_ls2_gamma: PARAMETER target='pretrained.blocks.4.ls2.gamma'\n",
       "            p_pretrained_blocks_5_norm1_weight: PARAMETER target='pretrained.blocks.5.norm1.weight'\n",
       "            p_pretrained_blocks_5_norm1_bias: PARAMETER target='pretrained.blocks.5.norm1.bias'\n",
       "            p_pretrained_blocks_5_attn_qkv_weight: PARAMETER target='pretrained.blocks.5.attn.qkv.weight'\n",
       "            p_pretrained_blocks_5_attn_qkv_bias: PARAMETER target='pretrained.blocks.5.attn.qkv.bias'\n",
       "            p_pretrained_blocks_5_attn_proj_weight: PARAMETER target='pretrained.blocks.5.attn.proj.weight'\n",
       "            p_pretrained_blocks_5_attn_proj_bias: PARAMETER target='pretrained.blocks.5.attn.proj.bias'\n",
       "            p_pretrained_blocks_5_ls1_gamma: PARAMETER target='pretrained.blocks.5.ls1.gamma'\n",
       "            p_pretrained_blocks_5_norm2_weight: PARAMETER target='pretrained.blocks.5.norm2.weight'\n",
       "            p_pretrained_blocks_5_norm2_bias: PARAMETER target='pretrained.blocks.5.norm2.bias'\n",
       "            p_pretrained_blocks_5_mlp_fc1_weight: PARAMETER target='pretrained.blocks.5.mlp.fc1.weight'\n",
       "            p_pretrained_blocks_5_mlp_fc1_bias: PARAMETER target='pretrained.blocks.5.mlp.fc1.bias'\n",
       "            p_pretrained_blocks_5_mlp_fc2_weight: PARAMETER target='pretrained.blocks.5.mlp.fc2.weight'\n",
       "            p_pretrained_blocks_5_mlp_fc2_bias: PARAMETER target='pretrained.blocks.5.mlp.fc2.bias'\n",
       "            p_pretrained_blocks_5_ls2_gamma: PARAMETER target='pretrained.blocks.5.ls2.gamma'\n",
       "            p_pretrained_blocks_6_norm1_weight: PARAMETER target='pretrained.blocks.6.norm1.weight'\n",
       "            p_pretrained_blocks_6_norm1_bias: PARAMETER target='pretrained.blocks.6.norm1.bias'\n",
       "            p_pretrained_blocks_6_attn_qkv_weight: PARAMETER target='pretrained.blocks.6.attn.qkv.weight'\n",
       "            p_pretrained_blocks_6_attn_qkv_bias: PARAMETER target='pretrained.blocks.6.attn.qkv.bias'\n",
       "            p_pretrained_blocks_6_attn_proj_weight: PARAMETER target='pretrained.blocks.6.attn.proj.weight'\n",
       "            p_pretrained_blocks_6_attn_proj_bias: PARAMETER target='pretrained.blocks.6.attn.proj.bias'\n",
       "            p_pretrained_blocks_6_ls1_gamma: PARAMETER target='pretrained.blocks.6.ls1.gamma'\n",
       "            p_pretrained_blocks_6_norm2_weight: PARAMETER target='pretrained.blocks.6.norm2.weight'\n",
       "            p_pretrained_blocks_6_norm2_bias: PARAMETER target='pretrained.blocks.6.norm2.bias'\n",
       "            p_pretrained_blocks_6_mlp_fc1_weight: PARAMETER target='pretrained.blocks.6.mlp.fc1.weight'\n",
       "            p_pretrained_blocks_6_mlp_fc1_bias: PARAMETER target='pretrained.blocks.6.mlp.fc1.bias'\n",
       "            p_pretrained_blocks_6_mlp_fc2_weight: PARAMETER target='pretrained.blocks.6.mlp.fc2.weight'\n",
       "            p_pretrained_blocks_6_mlp_fc2_bias: PARAMETER target='pretrained.blocks.6.mlp.fc2.bias'\n",
       "            p_pretrained_blocks_6_ls2_gamma: PARAMETER target='pretrained.blocks.6.ls2.gamma'\n",
       "            p_pretrained_blocks_7_norm1_weight: PARAMETER target='pretrained.blocks.7.norm1.weight'\n",
       "            p_pretrained_blocks_7_norm1_bias: PARAMETER target='pretrained.blocks.7.norm1.bias'\n",
       "            p_pretrained_blocks_7_attn_qkv_weight: PARAMETER target='pretrained.blocks.7.attn.qkv.weight'\n",
       "            p_pretrained_blocks_7_attn_qkv_bias: PARAMETER target='pretrained.blocks.7.attn.qkv.bias'\n",
       "            p_pretrained_blocks_7_attn_proj_weight: PARAMETER target='pretrained.blocks.7.attn.proj.weight'\n",
       "            p_pretrained_blocks_7_attn_proj_bias: PARAMETER target='pretrained.blocks.7.attn.proj.bias'\n",
       "            p_pretrained_blocks_7_ls1_gamma: PARAMETER target='pretrained.blocks.7.ls1.gamma'\n",
       "            p_pretrained_blocks_7_norm2_weight: PARAMETER target='pretrained.blocks.7.norm2.weight'\n",
       "            p_pretrained_blocks_7_norm2_bias: PARAMETER target='pretrained.blocks.7.norm2.bias'\n",
       "            p_pretrained_blocks_7_mlp_fc1_weight: PARAMETER target='pretrained.blocks.7.mlp.fc1.weight'\n",
       "            p_pretrained_blocks_7_mlp_fc1_bias: PARAMETER target='pretrained.blocks.7.mlp.fc1.bias'\n",
       "            p_pretrained_blocks_7_mlp_fc2_weight: PARAMETER target='pretrained.blocks.7.mlp.fc2.weight'\n",
       "            p_pretrained_blocks_7_mlp_fc2_bias: PARAMETER target='pretrained.blocks.7.mlp.fc2.bias'\n",
       "            p_pretrained_blocks_7_ls2_gamma: PARAMETER target='pretrained.blocks.7.ls2.gamma'\n",
       "            p_pretrained_blocks_8_norm1_weight: PARAMETER target='pretrained.blocks.8.norm1.weight'\n",
       "            p_pretrained_blocks_8_norm1_bias: PARAMETER target='pretrained.blocks.8.norm1.bias'\n",
       "            p_pretrained_blocks_8_attn_qkv_weight: PARAMETER target='pretrained.blocks.8.attn.qkv.weight'\n",
       "            p_pretrained_blocks_8_attn_qkv_bias: PARAMETER target='pretrained.blocks.8.attn.qkv.bias'\n",
       "            p_pretrained_blocks_8_attn_proj_weight: PARAMETER target='pretrained.blocks.8.attn.proj.weight'\n",
       "            p_pretrained_blocks_8_attn_proj_bias: PARAMETER target='pretrained.blocks.8.attn.proj.bias'\n",
       "            p_pretrained_blocks_8_ls1_gamma: PARAMETER target='pretrained.blocks.8.ls1.gamma'\n",
       "            p_pretrained_blocks_8_norm2_weight: PARAMETER target='pretrained.blocks.8.norm2.weight'\n",
       "            p_pretrained_blocks_8_norm2_bias: PARAMETER target='pretrained.blocks.8.norm2.bias'\n",
       "            p_pretrained_blocks_8_mlp_fc1_weight: PARAMETER target='pretrained.blocks.8.mlp.fc1.weight'\n",
       "            p_pretrained_blocks_8_mlp_fc1_bias: PARAMETER target='pretrained.blocks.8.mlp.fc1.bias'\n",
       "            p_pretrained_blocks_8_mlp_fc2_weight: PARAMETER target='pretrained.blocks.8.mlp.fc2.weight'\n",
       "            p_pretrained_blocks_8_mlp_fc2_bias: PARAMETER target='pretrained.blocks.8.mlp.fc2.bias'\n",
       "            p_pretrained_blocks_8_ls2_gamma: PARAMETER target='pretrained.blocks.8.ls2.gamma'\n",
       "            p_pretrained_blocks_9_norm1_weight: PARAMETER target='pretrained.blocks.9.norm1.weight'\n",
       "            p_pretrained_blocks_9_norm1_bias: PARAMETER target='pretrained.blocks.9.norm1.bias'\n",
       "            p_pretrained_blocks_9_attn_qkv_weight: PARAMETER target='pretrained.blocks.9.attn.qkv.weight'\n",
       "            p_pretrained_blocks_9_attn_qkv_bias: PARAMETER target='pretrained.blocks.9.attn.qkv.bias'\n",
       "            p_pretrained_blocks_9_attn_proj_weight: PARAMETER target='pretrained.blocks.9.attn.proj.weight'\n",
       "            p_pretrained_blocks_9_attn_proj_bias: PARAMETER target='pretrained.blocks.9.attn.proj.bias'\n",
       "            p_pretrained_blocks_9_ls1_gamma: PARAMETER target='pretrained.blocks.9.ls1.gamma'\n",
       "            p_pretrained_blocks_9_norm2_weight: PARAMETER target='pretrained.blocks.9.norm2.weight'\n",
       "            p_pretrained_blocks_9_norm2_bias: PARAMETER target='pretrained.blocks.9.norm2.bias'\n",
       "            p_pretrained_blocks_9_mlp_fc1_weight: PARAMETER target='pretrained.blocks.9.mlp.fc1.weight'\n",
       "            p_pretrained_blocks_9_mlp_fc1_bias: PARAMETER target='pretrained.blocks.9.mlp.fc1.bias'\n",
       "            p_pretrained_blocks_9_mlp_fc2_weight: PARAMETER target='pretrained.blocks.9.mlp.fc2.weight'\n",
       "            p_pretrained_blocks_9_mlp_fc2_bias: PARAMETER target='pretrained.blocks.9.mlp.fc2.bias'\n",
       "            p_pretrained_blocks_9_ls2_gamma: PARAMETER target='pretrained.blocks.9.ls2.gamma'\n",
       "            p_pretrained_blocks_10_norm1_weight: PARAMETER target='pretrained.blocks.10.norm1.weight'\n",
       "            p_pretrained_blocks_10_norm1_bias: PARAMETER target='pretrained.blocks.10.norm1.bias'\n",
       "            p_pretrained_blocks_10_attn_qkv_weight: PARAMETER target='pretrained.blocks.10.attn.qkv.weight'\n",
       "            p_pretrained_blocks_10_attn_qkv_bias: PARAMETER target='pretrained.blocks.10.attn.qkv.bias'\n",
       "            p_pretrained_blocks_10_attn_proj_weight: PARAMETER target='pretrained.blocks.10.attn.proj.weight'\n",
       "            p_pretrained_blocks_10_attn_proj_bias: PARAMETER target='pretrained.blocks.10.attn.proj.bias'\n",
       "            p_pretrained_blocks_10_ls1_gamma: PARAMETER target='pretrained.blocks.10.ls1.gamma'\n",
       "            p_pretrained_blocks_10_norm2_weight: PARAMETER target='pretrained.blocks.10.norm2.weight'\n",
       "            p_pretrained_blocks_10_norm2_bias: PARAMETER target='pretrained.blocks.10.norm2.bias'\n",
       "            p_pretrained_blocks_10_mlp_fc1_weight: PARAMETER target='pretrained.blocks.10.mlp.fc1.weight'\n",
       "            p_pretrained_blocks_10_mlp_fc1_bias: PARAMETER target='pretrained.blocks.10.mlp.fc1.bias'\n",
       "            p_pretrained_blocks_10_mlp_fc2_weight: PARAMETER target='pretrained.blocks.10.mlp.fc2.weight'\n",
       "            p_pretrained_blocks_10_mlp_fc2_bias: PARAMETER target='pretrained.blocks.10.mlp.fc2.bias'\n",
       "            p_pretrained_blocks_10_ls2_gamma: PARAMETER target='pretrained.blocks.10.ls2.gamma'\n",
       "            p_pretrained_blocks_11_norm1_weight: PARAMETER target='pretrained.blocks.11.norm1.weight'\n",
       "            p_pretrained_blocks_11_norm1_bias: PARAMETER target='pretrained.blocks.11.norm1.bias'\n",
       "            p_pretrained_blocks_11_attn_qkv_weight: PARAMETER target='pretrained.blocks.11.attn.qkv.weight'\n",
       "            p_pretrained_blocks_11_attn_qkv_bias: PARAMETER target='pretrained.blocks.11.attn.qkv.bias'\n",
       "            p_pretrained_blocks_11_attn_proj_weight: PARAMETER target='pretrained.blocks.11.attn.proj.weight'\n",
       "            p_pretrained_blocks_11_attn_proj_bias: PARAMETER target='pretrained.blocks.11.attn.proj.bias'\n",
       "            p_pretrained_blocks_11_ls1_gamma: PARAMETER target='pretrained.blocks.11.ls1.gamma'\n",
       "            p_pretrained_blocks_11_norm2_weight: PARAMETER target='pretrained.blocks.11.norm2.weight'\n",
       "            p_pretrained_blocks_11_norm2_bias: PARAMETER target='pretrained.blocks.11.norm2.bias'\n",
       "            p_pretrained_blocks_11_mlp_fc1_weight: PARAMETER target='pretrained.blocks.11.mlp.fc1.weight'\n",
       "            p_pretrained_blocks_11_mlp_fc1_bias: PARAMETER target='pretrained.blocks.11.mlp.fc1.bias'\n",
       "            p_pretrained_blocks_11_mlp_fc2_weight: PARAMETER target='pretrained.blocks.11.mlp.fc2.weight'\n",
       "            p_pretrained_blocks_11_mlp_fc2_bias: PARAMETER target='pretrained.blocks.11.mlp.fc2.bias'\n",
       "            p_pretrained_blocks_11_ls2_gamma: PARAMETER target='pretrained.blocks.11.ls2.gamma'\n",
       "            p_pretrained_norm_weight: PARAMETER target='pretrained.norm.weight'\n",
       "            p_pretrained_norm_bias: PARAMETER target='pretrained.norm.bias'\n",
       "            p_depth_head_projects_0_weight: PARAMETER target='depth_head.projects.0.weight'\n",
       "            p_depth_head_projects_0_bias: PARAMETER target='depth_head.projects.0.bias'\n",
       "            p_depth_head_projects_1_weight: PARAMETER target='depth_head.projects.1.weight'\n",
       "            p_depth_head_projects_1_bias: PARAMETER target='depth_head.projects.1.bias'\n",
       "            p_depth_head_projects_2_weight: PARAMETER target='depth_head.projects.2.weight'\n",
       "            p_depth_head_projects_2_bias: PARAMETER target='depth_head.projects.2.bias'\n",
       "            p_depth_head_projects_3_weight: PARAMETER target='depth_head.projects.3.weight'\n",
       "            p_depth_head_projects_3_bias: PARAMETER target='depth_head.projects.3.bias'\n",
       "            p_depth_head_resize_layers_0_weight: PARAMETER target='depth_head.resize_layers.0.weight'\n",
       "            p_depth_head_resize_layers_0_bias: PARAMETER target='depth_head.resize_layers.0.bias'\n",
       "            p_depth_head_resize_layers_1_weight: PARAMETER target='depth_head.resize_layers.1.weight'\n",
       "            p_depth_head_resize_layers_1_bias: PARAMETER target='depth_head.resize_layers.1.bias'\n",
       "            p_depth_head_resize_layers_3_weight: PARAMETER target='depth_head.resize_layers.3.weight'\n",
       "            p_depth_head_resize_layers_3_bias: PARAMETER target='depth_head.resize_layers.3.bias'\n",
       "            p_depth_head_scratch_layer1_rn_weight: PARAMETER target='depth_head.scratch.layer1_rn.weight'\n",
       "            p_depth_head_scratch_layer2_rn_weight: PARAMETER target='depth_head.scratch.layer2_rn.weight'\n",
       "            p_depth_head_scratch_layer3_rn_weight: PARAMETER target='depth_head.scratch.layer3_rn.weight'\n",
       "            p_depth_head_scratch_layer4_rn_weight: PARAMETER target='depth_head.scratch.layer4_rn.weight'\n",
       "            p_depth_head_scratch_refinenet1_out_conv_weight: PARAMETER target='depth_head.scratch.refinenet1.out_conv.weight'\n",
       "            p_depth_head_scratch_refinenet1_out_conv_bias: PARAMETER target='depth_head.scratch.refinenet1.out_conv.bias'\n",
       "            p_depth_head_scratch_refinenet1_resconfunit1_conv1_weight: PARAMETER target='depth_head.scratch.refinenet1.resConfUnit1.conv1.weight'\n",
       "            p_depth_head_scratch_refinenet1_resconfunit1_conv1_bias: PARAMETER target='depth_head.scratch.refinenet1.resConfUnit1.conv1.bias'\n",
       "            p_depth_head_scratch_refinenet1_resconfunit1_conv2_weight: PARAMETER target='depth_head.scratch.refinenet1.resConfUnit1.conv2.weight'\n",
       "            p_depth_head_scratch_refinenet1_resconfunit1_conv2_bias: PARAMETER target='depth_head.scratch.refinenet1.resConfUnit1.conv2.bias'\n",
       "            p_depth_head_scratch_refinenet1_resconfunit2_conv1_weight: PARAMETER target='depth_head.scratch.refinenet1.resConfUnit2.conv1.weight'\n",
       "            p_depth_head_scratch_refinenet1_resconfunit2_conv1_bias: PARAMETER target='depth_head.scratch.refinenet1.resConfUnit2.conv1.bias'\n",
       "            p_depth_head_scratch_refinenet1_resconfunit2_conv2_weight: PARAMETER target='depth_head.scratch.refinenet1.resConfUnit2.conv2.weight'\n",
       "            p_depth_head_scratch_refinenet1_resconfunit2_conv2_bias: PARAMETER target='depth_head.scratch.refinenet1.resConfUnit2.conv2.bias'\n",
       "            p_depth_head_scratch_refinenet2_out_conv_weight: PARAMETER target='depth_head.scratch.refinenet2.out_conv.weight'\n",
       "            p_depth_head_scratch_refinenet2_out_conv_bias: PARAMETER target='depth_head.scratch.refinenet2.out_conv.bias'\n",
       "            p_depth_head_scratch_refinenet2_resconfunit1_conv1_weight: PARAMETER target='depth_head.scratch.refinenet2.resConfUnit1.conv1.weight'\n",
       "            p_depth_head_scratch_refinenet2_resconfunit1_conv1_bias: PARAMETER target='depth_head.scratch.refinenet2.resConfUnit1.conv1.bias'\n",
       "            p_depth_head_scratch_refinenet2_resconfunit1_conv2_weight: PARAMETER target='depth_head.scratch.refinenet2.resConfUnit1.conv2.weight'\n",
       "            p_depth_head_scratch_refinenet2_resconfunit1_conv2_bias: PARAMETER target='depth_head.scratch.refinenet2.resConfUnit1.conv2.bias'\n",
       "            p_depth_head_scratch_refinenet2_resconfunit2_conv1_weight: PARAMETER target='depth_head.scratch.refinenet2.resConfUnit2.conv1.weight'\n",
       "            p_depth_head_scratch_refinenet2_resconfunit2_conv1_bias: PARAMETER target='depth_head.scratch.refinenet2.resConfUnit2.conv1.bias'\n",
       "            p_depth_head_scratch_refinenet2_resconfunit2_conv2_weight: PARAMETER target='depth_head.scratch.refinenet2.resConfUnit2.conv2.weight'\n",
       "            p_depth_head_scratch_refinenet2_resconfunit2_conv2_bias: PARAMETER target='depth_head.scratch.refinenet2.resConfUnit2.conv2.bias'\n",
       "            p_depth_head_scratch_refinenet3_out_conv_weight: PARAMETER target='depth_head.scratch.refinenet3.out_conv.weight'\n",
       "            p_depth_head_scratch_refinenet3_out_conv_bias: PARAMETER target='depth_head.scratch.refinenet3.out_conv.bias'\n",
       "            p_depth_head_scratch_refinenet3_resconfunit1_conv1_weight: PARAMETER target='depth_head.scratch.refinenet3.resConfUnit1.conv1.weight'\n",
       "            p_depth_head_scratch_refinenet3_resconfunit1_conv1_bias: PARAMETER target='depth_head.scratch.refinenet3.resConfUnit1.conv1.bias'\n",
       "            p_depth_head_scratch_refinenet3_resconfunit1_conv2_weight: PARAMETER target='depth_head.scratch.refinenet3.resConfUnit1.conv2.weight'\n",
       "            p_depth_head_scratch_refinenet3_resconfunit1_conv2_bias: PARAMETER target='depth_head.scratch.refinenet3.resConfUnit1.conv2.bias'\n",
       "            p_depth_head_scratch_refinenet3_resconfunit2_conv1_weight: PARAMETER target='depth_head.scratch.refinenet3.resConfUnit2.conv1.weight'\n",
       "            p_depth_head_scratch_refinenet3_resconfunit2_conv1_bias: PARAMETER target='depth_head.scratch.refinenet3.resConfUnit2.conv1.bias'\n",
       "            p_depth_head_scratch_refinenet3_resconfunit2_conv2_weight: PARAMETER target='depth_head.scratch.refinenet3.resConfUnit2.conv2.weight'\n",
       "            p_depth_head_scratch_refinenet3_resconfunit2_conv2_bias: PARAMETER target='depth_head.scratch.refinenet3.resConfUnit2.conv2.bias'\n",
       "            p_depth_head_scratch_refinenet4_out_conv_weight: PARAMETER target='depth_head.scratch.refinenet4.out_conv.weight'\n",
       "            p_depth_head_scratch_refinenet4_out_conv_bias: PARAMETER target='depth_head.scratch.refinenet4.out_conv.bias'\n",
       "            p_depth_head_scratch_refinenet4_resconfunit1_conv1_weight: PARAMETER target='depth_head.scratch.refinenet4.resConfUnit1.conv1.weight'\n",
       "            p_depth_head_scratch_refinenet4_resconfunit1_conv1_bias: PARAMETER target='depth_head.scratch.refinenet4.resConfUnit1.conv1.bias'\n",
       "            p_depth_head_scratch_refinenet4_resconfunit1_conv2_weight: PARAMETER target='depth_head.scratch.refinenet4.resConfUnit1.conv2.weight'\n",
       "            p_depth_head_scratch_refinenet4_resconfunit1_conv2_bias: PARAMETER target='depth_head.scratch.refinenet4.resConfUnit1.conv2.bias'\n",
       "            p_depth_head_scratch_refinenet4_resconfunit2_conv1_weight: PARAMETER target='depth_head.scratch.refinenet4.resConfUnit2.conv1.weight'\n",
       "            p_depth_head_scratch_refinenet4_resconfunit2_conv1_bias: PARAMETER target='depth_head.scratch.refinenet4.resConfUnit2.conv1.bias'\n",
       "            p_depth_head_scratch_refinenet4_resconfunit2_conv2_weight: PARAMETER target='depth_head.scratch.refinenet4.resConfUnit2.conv2.weight'\n",
       "            p_depth_head_scratch_refinenet4_resconfunit2_conv2_bias: PARAMETER target='depth_head.scratch.refinenet4.resConfUnit2.conv2.bias'\n",
       "            p_depth_head_scratch_output_conv1_weight: PARAMETER target='depth_head.scratch.output_conv1.weight'\n",
       "            p_depth_head_scratch_output_conv1_bias: PARAMETER target='depth_head.scratch.output_conv1.bias'\n",
       "            p_depth_head_scratch_output_conv2_0_weight: PARAMETER target='depth_head.scratch.output_conv2.0.weight'\n",
       "            p_depth_head_scratch_output_conv2_0_bias: PARAMETER target='depth_head.scratch.output_conv2.0.bias'\n",
       "            p_depth_head_scratch_output_conv2_2_weight: PARAMETER target='depth_head.scratch.output_conv2.2.weight'\n",
       "            p_depth_head_scratch_output_conv2_2_bias: PARAMETER target='depth_head.scratch.output_conv2.2.bias'\n",
       "            x: USER_INPUT\n",
       "    \n",
       "            # outputs\n",
       "            squeeze: USER_OUTPUT\n",
       "    \n",
       "        Range constraints: {}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export to ONNX\n",
    "dummy_input = torch.randn(1, 3, resize_shortest, resize_shortest).to(device)\n",
    "torch.onnx.export(model, dummy_input, \"depth_anything_v2.onnx\", opset_version=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f05abc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame_bgr = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert BGR -> RGB\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    frame_pil = Image.fromarray(frame_rgb)\n",
    "    \n",
    "    # Resize keeping aspect ratio\n",
    "    h, w = frame_pil.height, frame_pil.width\n",
    "    scale = resize_shortest / min(h, w)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    \n",
    "    # Round to nearest multiple of 14\n",
    "    new_w = (new_w // 14) * 14\n",
    "    new_h = (new_h // 14) * 14\n",
    "    \n",
    "    frame_resized = frame_pil.resize((new_w, new_h))\n",
    "    \n",
    "    # To tensor\n",
    "    frame_np = np.array(frame_resized).astype(np.float32) / 255.0\n",
    "    input_tensor = torch.from_numpy(frame_np).permute(2,0,1).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        depth_output = model(input_tensor)\n",
    "    \n",
    "    # Postprocess depth map\n",
    "    depth_map = depth_output.squeeze().cpu().numpy()\n",
    "    depth_vis = cv2.resize(depth_map, (frame_width, frame_height))  # resize back to original\n",
    "    depth_vis = cv2.normalize(depth_vis, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    depth_colored = cv2.applyColorMap(depth_vis, cv2.COLORMAP_PLASMA)\n",
    "    \n",
    "    # Optional: overlay original video\n",
    "    overlay = cv2.addWeighted(frame_bgr, 0.6, depth_colored, 0.4, 0)\n",
    "    \n",
    "    # Write frame to output video\n",
    "    out.write(overlay)\n",
    "\n",
    "# --- Release resources ---\n",
    "cap.release()\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
